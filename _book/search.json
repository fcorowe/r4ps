[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Population Science",
    "section": "",
    "text": "This is the website for “Population Science”. This is a course designed and delivered by Dr. Francisco Rowe, Dr. Carmen Cabrera-Arnau and Dr. Elisabetta Pietrostefani from the Geographic Data Science Lab at the University of Liverpool, United Kingdom. You will learn applied tools and cutting-edge analytical approaches to use digital footprint data to explore and understand human population trends and patterns, including supervised and unsupervised machine learning approaches, network analysis and causal inference methods.\nThe website is free to use and is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International. A compilation of this web course is hosted as a GitHub repository that you can access:"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Population Science",
    "section": "Contact",
    "text": "Contact\n\nFrancisco Rowe - f.rowe-gonzalez [at] liverpool.ac.uk Senior Lecturer in Quantitative Human Geography Office 507, Roxby Building, University of Liverpool, Liverpool, L69 7ZT, United Kingdom.\n\n\nCarmen Cabrera-Arnau - c.cabrera-arnau [at] liverpool.ac.uk Lecturer in Geographic Data Science Office 1/016, Roxby Building, University of Liverpool, Liverpool, L69 7ZT, United Kingdom.\n\n\nElisabetta Pietrostefani - e.pietrostefani [at] liverpool.ac.uk Lecturer in Geographic Data Science Office 6xx, Roxby Building, University of Liverpool, Liverpool, L69 7ZT, United Kingdom."
  },
  {
    "objectID": "intro.html#aims",
    "href": "intro.html#aims",
    "title": "1  Overview",
    "section": "1.1 Aims",
    "text": "1.1 Aims\nThis module aims to:\n\nprovide an introduction to fundamental theories of population science;\nintroduce students to novel data and approaches to understanding population dynamics and societal change; and,\nequip students with skills and experience to conduct population science using computational, data science approaches."
  },
  {
    "objectID": "intro.html#learning-outcomes",
    "href": "intro.html#learning-outcomes",
    "title": "1  Overview",
    "section": "1.2 Learning Outcomes",
    "text": "1.2 Learning Outcomes\nBy the end of the module, students should be able to:\n\ngain an appreciation of relevant demographic theory to help interpret patterns of popu- lation change;\ndevelop an understanding of the types of demographic and social science methods that are essential for interpreting and analysing digital footprint data in the context of population dynamics;\ndevelop the ability to apply different methods to understand population dynamics and societal change;\ngain an appreciation of how population science approaches can produce relevant evidence to inform policy debates;\ndevelop critical awareness of modern demographic analysis and ethical considerations in the use of digital footprint data."
  },
  {
    "objectID": "intro.html#feedback",
    "href": "intro.html#feedback",
    "title": "1  Overview",
    "section": "1.3 Feedback",
    "text": "1.3 Feedback\nFormal assessment of two computational essays. Written assignment-specific feedback will be provided within three working weeks of the submission deadline. Comments will offer an understanding of the mark awarded and identify areas which can be considered for improvement in future assignments.\nVerbal face-to-face feedback. Immediate face-to-face feedback will be provided during computer, discussion and clinic sessions in interaction with staff. This will take place in all live sessions during the semester.\nOnline forum. Asynchronous written feedback will be provided via an online forum. Students are encouraged to contribute by asking and answering questions relating to the module content. Staff will monitor the forum Monday to Friday 9am-5pm, but it will be open to students to make contributions at all times. Response time will vary depending on the complexity of the question and staff availability."
  },
  {
    "objectID": "intro.html#computational-environment",
    "href": "intro.html#computational-environment",
    "title": "1  Overview",
    "section": "1.4 Computational Environment",
    "text": "1.4 Computational Environment\nTo reproduce the code in the book, you need the following software packages:\n\nR-4.2.2\nRStudio 2022.12.0-353\nQuarto 1.2.280\nthe list of libraries in the next section\n\nTo check your version of:\n\nR and libraries run sessionInfo()\nRStudio click help on the menu bar and then About\nQuarto check the version file in the quarto folder on your computer.\n\nTo install and update:\n\nR, download the appropriate version from The Comprehensive R Archive Network (CRAN)\nRStudio, download the appropriate version from Posit\nQuarto, download the appropriate version from the Quarto website\n\n\n1.4.1 List of libraries\nThe list of libraries used in this book is provided below:\n\ntidyverse\nviridis\nviridisLite\nggthemes\npatchwork\nshowtext\nRColorBrewer\nlubridate\ntmap\nsjPlot\nsf\nsp\nkableExtra\nggcorrplot\nplotrix\ncluster\nfactoextra\n\nYou need to ensure you have installed the list of libraries used in this book, running the following code:\n\nlist.of.packages.cran <- c( “tidyverse”, “viridis”, “viridisLite”, “ggthemes”, “patchwork”, “showtext”, “RColorBrewer”, “lubridate”, “tmap”, “sjPlot”, “sf”, “sp”, “kableExtra”, “ggcorrplot”, “plotrix”, “cluster”, “factoextra”)\n\n\nnew.packages.cran <- list.of.packages.cran[!(list.of.packages.cran %in% installed.packages()[,“Package”])] if(length(new.packages.cran)) install.packages(new.packages.cran)\n\n\nfor(i in 1:length(list.of.packages.cran)) { library(list.of.packages.cran[i], character.only = T) }"
  },
  {
    "objectID": "intro.html#assessment",
    "href": "intro.html#assessment",
    "title": "1  Overview",
    "section": "1.5 Assessment",
    "text": "1.5 Assessment\nThe final module mark is composed of the two computational essays. Together they are designed to cover the materials introduced in the entirety of content covered during the semester. A computational essay is an essay whose narrative is supported by code and computational results that are included in the essay itself. Each teaching week, you will be required to address a set of questions relating to the module content covered in that week, and to use the material that you will produce for this purpose to build your computational essay.\nAssignment 1 (50%) refers to the set of questions at the end of Chapter 2, Chapter 3, Chapter 4 and Chapter 5. You are required to use your responses to build your computational essay. Each chapter provides more specific guidance of the tasks and discussion that you are required to consider in your assignment.\nAssignment 2 (50%) refers to the set of questions at the end of Chapter 6, Chapter 7, Chapter 8, Chapter 9 and Chapter 10. You are required to use your responses to build your computational essay. Each chapter provides more specific guidance of the tasks and discussion that you are required to consider in your assignment.\n\n1.5.1 Format Requirements\nBoth assignments will have the same requirements:\n\nMaximum word count: 2,000 words, excluding figures and references.\nUp to three maps, plot or figures (a figure may include more than one map and/or plot and will only count as one but needs to be integrated in the figure)\nUp to two tables.\n\nAssignments need to be prepared in “Quarto Document” format (i.e. qmd extension) and then converted into a self-contained HTML file that will then be submitted via Turnitin. The document should only display content that will be assessed. Intermediate steps do not need to be displayed. Messages resulting from loading packages, attaching data frames, or similar messages do not need to be included as output code. Useful resources to customise your R notebook can be found on Quarto’s website.\nTwo Quarto Document templates will be available via the module Canvas site.\nSubmission is electronic only via Turnitin on Canvas.\n\n1.5.1.1 Marking criteria\nThe Standard Environmental Sciences School marking criteria apply, with a stronger emphasis on evidencing the use of regression models, critical analysis of results and presentation standards. In addition to these general criteria, the code and outputs (i.e. tables, maps and plots) contained within the notebook submitted for assessment will be assessed according to the extent of documentation and evidence of expertise in changing and extending the code options illustrated in each chapter. Specifically, the following criteria will be applied:\n\n0-15: no documentation and use of default options.\n16-39: little documentation and use of default options.\n40-49: some documentation, and use of default options.\n50-59: extensive documentation, and edit of some of the options provided in the notebook (e.g. change north arrow location).\n60-69: extensive well organised and easy to read documentation, and evidence of understanding of options provided in the code (e.g. tweaking existing options).\n70-79: all above, plus clear evidence of code design skills (e.g. customising graphics, combining plots (or tables) into a single output, adding clear axis labels and variable names on graphic outputs, etc.).\n80-100: all as above, plus code containing novel contributions that extend/improve the functionality the code was provided with (e.g. comparative model assessments, novel methods to perform the task, etc.).\n\n\n\n\n\nRowe, Francisco. 2021. “Big Data and Human Geography.” http://dx.doi.org/10.31235/osf.io/phz3e."
  },
  {
    "objectID": "intro-population-science.html#introduction",
    "href": "intro-population-science.html#introduction",
    "title": "2  Introducing Population Science",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nPopulation science sits at the intersection between population studies and data science. As the general field of population studies, population science seeks to quantitatively understand human populations, including the three key demographic processes of population change, namely fertility, mortality and migration. It seeks to understand the size, structure, temporal changes and spatial distribution of populations, and the drivers and impacts that underpin their variations and regularities. It considers the ways in which structural social, economic, political and environmental factors shape population trends. What is unique about population science is that it seeks to leverage on the ongoing digital revolution characterised by technological advances in computer processing, digitalised information storage capacity and digital connectivity (Hilbert and López 2011).\nThe digital revolution ushered in the 1990s has unleashed a data revolution. Technological advances in computational power, storage and digital network platforms have enabled the emergence of “Big Data” or “digital footprint data”. These technological developments have enabled the production, processing, analysis and storage of large volumes of digital data. Analysing 1986-2007 data, Hilbert and López (2011) estimated that the world has already passed the point at which more data were being collected than could be physically stored. They estimated that the global general-purpose computing capacity grew at an annual rate of 58 percent between 1986 and 2007, exceeding that of global storage capacity (23 percent). We can now digitally captured and generated data that previously could not easily be recorded and stored.\nThe unprecedented amount of information that we can now capture through digital technology offers unique opportunities to advance our understanding of micro human behaviour (e.g. individual-level decision making, preferences and choices) and macro population processes (e.g. structural population processes and trends). Digital footprint data offer a continuous flow of information to capture human population dynamics at unprecedentedly fine spatial and temporal resolution in real or near real-time comprising entire social systems. We can capture and study micro individual behaviours such as online time use, purchasing behaviour, visitation patterns and public opinion from data sources, such as mobile phones, social media and retail website platforms. These behaviours can also be aggregated to shed light into macro structural processes and trends, such as urban mobility, consumer demand, transport usage, population ageing and decline. Fundamentally digital footprint data thus have the potential to become a key pillar informing and supporting decision making. They can inform business to increase sales revenue, football clubs to improve team performance, and governments to tackle major societal issues, such as the COVID-19 pandemic and global warming, influencing policy, practice and governance structures.\nYet, the use of digital footprint data also poses major conceptual, methodological and ethical challenges (Rowe 2021). It is these challenges that motivated this module. Digital footprint data are a by-product of an administrative process or service, and it is not purposely collected for research. Turning raw digital footprint data into actionable, usable information thus requires a unique combination of technical computational expertise and subject-specific knowledge. Traditionally university programmes have tended to focus on providing technical training, such as statistics or on specific knowledge subjects. But they are rarely found as a single coherent package. This module aims to fills this gap by offering training in the use of digital footprint data, and sophisticated methodological approaches (including machine learning, artificial intelligence, network science and statistical methods) to tackle important population issues, such as population segmenting, decline and mobility. Access to digital footprint data are highly variable; hence, we do not focus on this here. However, we encourage users of this book to read a report put together by the Joint Research Centre (2022) identifying and discussing key data sources focusing population processes.\nThe name of this module Population Science reflects the inclusive and interdisciplinary perspective we hope to capture. The data revolution has led to the emergence of a range of sub-disciplines, seeking to leverage on the use of digital footprint data to study human behaviour and population processes. These emerging sub-disciplines have tended to focus on discipline-specific issues such as digital demography (Kashyap et al. 2022), or particular methodological approaches, such as the use of networks principles in computational social sciences (Lazer et al. 2009). Population science seeks to integrate these perspectives and provide a fertile framework for critique, collaboration and co-creation across these emerging areas of scholarship in the study of human population. And, of course, take a spatial perspective adopting geographic data science approaches (Singleton and Arribas-Bel 2019).\nSpecifically, this chapter aims to discuss key opportunities and challenges of digital footprint data to analyse human population dynamics. We place a particular focus on the challenges relating to privacy, bias and privacy issues. The chapter starts by defining digital footprint data before discussing the key opportunities offered by these data and the challenges they pose."
  },
  {
    "objectID": "intro-population-science.html#defining-digital-footprint-data",
    "href": "intro-population-science.html#defining-digital-footprint-data",
    "title": "2  Introducing Population Science",
    "section": "2.2 Defining digital footprint data",
    "text": "2.2 Defining digital footprint data\nWe define digital footprint data as:\n\nthe data recorded by digital technology resulting from the interactions of people among themselves or with their social and physical environment, and they can take the form of images, video, text and numbers.\n\nData footprint data are distinctive features in their volume, velocity, variety, exhaustiveness, resolution, relational nature and flexibility (Kitchin 2014). They can take different forms. Traditional data used to be mostly numeric. Digital footprint data has facilitated the collection, storage and analysis of text (e.g. Twitter posts), image (e.g. Instagram photos) and video (e.g. CCTV footage) data.\nMultiple digital systems contribute to the storage and generation of digital footprint data. Kitchin (2014) identified three broad systems directed, automated and volunteered systems. Directed systems comprise digital administrative systems operated by a human recording data on places or people e.g. immigration control, biometric scanning and health records. Automated systems involve digital systems which automatically and autonomously record and process data with little human intervention e.g. mobile phone applications, electronic smartcard ticketing, energy smart meter and traffic sensors. Volunteered systems involve digital spaces in which humans contribute data through interactions on social media platforms (e.g. Twitter and Facebook) or crowdsourcing (e.g. OpenStreetMap and Wikipedia).\n\n\n\nDigital footprint systems"
  },
  {
    "objectID": "intro-population-science.html#opportunities-of-digital-footprint-data",
    "href": "intro-population-science.html#opportunities-of-digital-footprint-data",
    "title": "2  Introducing Population Science",
    "section": "2.3 Opportunities of digital footprint data",
    "text": "2.3 Opportunities of digital footprint data\nDigital footprint data offer unique opportunities for the analysis of human population patterns. As Rowe (2021) argues, digital footprint data offer three key promises in relation to traditional data sources, such as surveys and censuses. They generally provide greater spatio-temporal granularity, wider coverage and timeliness.\nDigital footprint data offer high geographic and temporal granularity. Most digital footprint data are time-stamped and geographically referenced with high precision. Digital technology, such as mobile phone and geographical positioning systems enables the generation of a continuous steams of time-stamped location data. Such information thus provides an opportunity to trace and enhance our understanding human populations over highly granular spatial scales and time intervals, going beyond the static representation afforded by most traditional data sources. Spatial human interactions, and how people use and are influenced by their environment, can be analysed in a temporally dynamic way.\nDigital footprint data provide extensive coverage. Contrasting to traditional random sampling, digital footprint data promise information on universal or near-universal population or geographical systems. Social media platforms, such as Twitter generate data to capture the entire universe of Twitter users. Satellite technology produces imagery snapshots to composite a representation of the Earth. Electronic smartcard ticketing systems produce information to capture the population of users in the system. Because the information is typically consistently collected and storage, the coverage of digital footprint data offer the potential to study human behaviour of entire systems at a global scale based on harmonised definitions, which is rarely possible using traditional data sources.\nDigital footprint data are generated in real-time. Unlike traditional systems of data collection and release, digital footprint data can be streamed continuously in real- or near real-time. Commercial transactions are generally recorded on bank ledgers as bank card payments occur at retail shops. Individual mobile phone’s location are captured as applications ping cellular antennas. Such information offer an opportunity to monitor and response to rapidly evolving situations, such as the COVID-19 pandemic (Green, Pollock, and Rowe 2021), natural disasters (Rowe 2022) and conflicts (Rowe, Neville, and González-Leonardo 2022).\nWe also loudly and clearly argue that while digital footprint data should be seen as a key asset to support government and business decision making processes, they should not be considered at the expenses of traditional data sources. Digital footprint data and traditional data sources should be used to complement each another. As indicated earlier, digital footprint data are the by-product of administrative processes or services. They were not designed with the aim of doing research. They require considerable work of data re-engineering to re-purpose them and turn them into an analysis-ready data product that can be used for further analysis (Arribas-Bel et al. 2021). Yet, as we will discuss below significant challenges remain. As the saying goes “all data are dirty, but some data are useful”. This quote used in the data science community to convey the idea that data are often imperfect, but they can still be used to gain valuable insights. Our message is that digital footprint data and traditional data sources should be triangulated to leverage on their strengths and mitigate their weaknesses."
  },
  {
    "objectID": "intro-population-science.html#challenges-of-digital-footprint-data",
    "href": "intro-population-science.html#challenges-of-digital-footprint-data",
    "title": "2  Introducing Population Science",
    "section": "2.4 Challenges of digital footprint data",
    "text": "2.4 Challenges of digital footprint data\nDigital footprint data also impose key conceptual, methodological and ethical challenges. In this section, we provide a brief explanation of challenges in these areas, focusing particularly on issues around biases, privacy, ethics and new methods. We focus on these issues because they are of practical importance and probably of most interest to the readers of this book. Excellent discussions have been written and, if you are interested in learning more about the challenges relating to digital footprint data, we recommend Kitchin (2014), Cesare et al. (2018), Lazer et al. (2020) and Rowe (2021).\n\n2.4.1 Conceptual challenges\nConceptually, the emergence of digital footprint data has led to the rethinking and questioning of existing theoretical social science approaches (Franklin 2022). On the one hand, digital footprint data provide an opportunity to explore existing theories or hypotheses through different lens and test the consistency of existing beliefs. For example, economics theories discuss the existence of temporal and spatial equilibrium. Resulting hypotheses are generally tested through mathematical theoretical models or empirical analyses relying on temporally static data. The existence of equilibrium has thus remained hard to assess. Digital footprint data provide an opportunity to empirically test temporal and spatial equilibrium ideas based on suitable temporally dynamic data. They can enable the testing of cause and impact hypotheses, rather than only focusing on static associations.\nOn the other hand, digital footprint data sparked new questions. Digital footprint data provide data on previously unmeasured activities. Data now capture activities that were previously difficult to quantify, such as personal communications, social networks, search and information gathering, and location data. These data offer an opportunity to develop new questions expanding existing theories by looking inside the “black box” of households, organisations and markets. They may also open the door to developing entirely new questions such as the role of digital technology in shaping human behaviour, and the role of artificial intelligence on productivity and financial markets.\n\n\n2.4.2 Methodological challenges\nMethodologically, the need for a wide and new set of digital skills and expertise to handle, store and analyse large volumes of data is a key challenge. As indicated earlier, digital footprint data are not created for research purposes. They need to be reengineered for research. Large streams of digital footprint data cannot be stored on local memory. They can rarely be read as a single unit on a local computer and may involve performing the same task numerous times in regular basis, requiring therefore large storage, computational capacity and computer science expertise. The manipulation and storage of digital footprint data often require technical expertise in data management systems, such as SQL, Google Cloud Storage and Amazon S3, as well as in efficient computing involving expertise in distributed computing systems and parallelisation frameworks. The analysis and modelling of digital footprint data may entail competencies in the application of machine learning and artificial intelligence. While these competencies generally form part of a computer science programme, they are rarely taught in an integrated framework focusing on addressing societal or business challenges relating to human populations - where the key focus is their application.\nAn additional methodological challenge is the presence of biases in digital footprint data. Digital footprint data are representative of a specific segment of the population but little is known which segments and how their representation varies across data sets and digital technology. Digital footprint data may comprise multiple sources of biases. They may reflect differences in the use of a digital device (e.g. mobile phone) and/or a piece of digital technology (e.g. a mobile phone application) Schlosser et al. (2021) . They may also reflect differences in frequency in the use of digital technology (e.g. number of times an individual uses a mobile phone application) - and this frequency may in turn reflect differences in algorithmic decisions embedded in digital platforms, such as suggesting content based on prior interactions to increase engagement with a given mobile phone application. Some work has been done on assessing biases as well as developing approaches to mitigate their influence Ribeiro, Benevenuto, and Zagheni (2020).\n\n\n2.4.3 Ethical challenges\nPrivacy represents a major ethical challenge. Digital footprint data are highly sensitive, and hence, anonymisation and disclosure control are required. Individual records must be anonymised so they are not identifiable. The high degree of granularity and personal information of these records may and have been used in ethically questionable ways; for example, Cambridge Analytica used information of Facebook users to segment the population and target politically motivated content (Cadwalladr and Graham-Harrison 2018). Anonymising information, however, imposes a key challenge as there is a trade-off between accuracy and privacy (Petti and Flaxman 2020). Anonymisation may reduce the usability of data. The greater the degree of privacy, the lower is the degree of accuracy of the resulting data and vice versa. Identifying the optimal point balancing the privacy-accuracy trade-off is the key challenge. If doing incorrectly, we could end up drawing inferences that do not reflect the actual population processes displayed in the data, or have artificially been encoded in the data through noise or reshuffling. The application of data differential privacy to the US census provides a recent good example of this challenge. An emblematic case is New York’s Liberty Island which has no resident population, but official US census reported 48 residents which was the result of adding statistical noise to the data, in order to enhance privacy."
  },
  {
    "objectID": "intro-population-science.html#conclusion",
    "href": "intro-population-science.html#conclusion",
    "title": "2  Introducing Population Science",
    "section": "2.5 Conclusion",
    "text": "2.5 Conclusion\nDigital footprint data present unique oppotunites to enhance our understanding of population processes and support individual, business and government decisions to improve targeted processes and outcomes. Businesses have used digital footprint data to segment their consumer populations and improve their targeting of marketing content, products and ultimately increase sales and revenue (Dolega, Rowe, and Branagan 2021). Governments and health care institutions, particularly during the COVID-19 have leverage digital footprint data to monitor the spread of the pandemic and develop appropriate mitigation responses (Green, Pollock, and Rowe 2021). However, the use of digital footprint data poses major conceptual, methodological and ethical challenges - which need to be overcome to unleash their full potential. The aim of this book is to address of the key methodological challenges. In particular, this book seeks to provide applied training on the practical application of commonly used machine learning and artificial intelligence approaches to leverage on digital footprint data in the understanding of human behaviour and population processes.\n\n\n\n\nArribas-Bel, Dani, Mark Green, Francisco Rowe, and Alex Singleton. 2021. “Open Data Products-A Framework for Creating Valuable Analysis Ready Data.” Journal of Geographical Systems 23 (4): 497–514. https://doi.org/10.1007/s10109-021-00363-5.\n\n\nCadwalladr, Carole, and Emma Graham-Harrison. 2018. “Revealed: 50 Million Facebook Profiles Harvested for Cambridge Analytica in Major Data Breach.” The Guardian 17 (1): 22.\n\n\nCesare, Nina, Hedwig Lee, Tyler McCormick, Emma Spiro, and Emilio Zagheni. 2018. “Promises and Pitfalls of Using Digital Traces for Demographic Research.” Demography 55 (5): 1979–99. https://doi.org/10.1007/s13524-018-0715-2.\n\n\nDolega, Les, Francisco Rowe, and Emma Branagan. 2021. “Going Digital? The Impact of Social Media Marketing on Retail Website Traffic, Orders and Sales.” Journal of Retailing and Consumer Services 60 (May): 102501. https://doi.org/10.1016/j.jretconser.2021.102501.\n\n\nFranklin, Rachel. 2022. “Quantitative Methods II: Big Theory.” Progress in Human Geography 47 (1): 178–86. https://doi.org/10.1177/03091325221137334.\n\n\nGreen, Mark, Frances Darlington Pollock, and Francisco Rowe. 2021. “New Forms of Data and New Forms of Opportunities to Monitor and Tackle a Pandemic.” In, 423–29. Springer International Publishing. https://doi.org/10.1007/978-3-030-70179-6_56.\n\n\nHilbert, Martin, and Priscila López. 2011. “The World’s Technological Capacity to Store, Communicate, and Compute Information.” Science 332 (6025): 60–65. https://doi.org/10.1126/science.1200970.\n\n\nJoint Research Centre. 2022. Data innovation in demography, migration and human mobility. LU: European Commission. Publications Office. https://doi.org/10.2760/027157.\n\n\nKashyap, Ridhi, R. Gordon Rinderknecht, Aliakbar Akbaritabar, Diego Alburez-Gutierrez, Sofia Gil-Clavel, André Grow, Jisu Kim, et al. 2022. “Digital and Computational Demography.” http://dx.doi.org/10.31235/osf.io/7bvpt.\n\n\nKitchin, Rob. 2014. “Big Data, New Epistemologies and Paradigm Shifts.” Big Data & Society 1 (1): 205395171452848. https://doi.org/10.1177/2053951714528481.\n\n\nLazer, David, Alex Pentland, Lada Adamic, Sinan Aral, Albert-László Barabási, Devon Brewer, Nicholas Christakis, et al. 2009. “Computational Social Science.” Science 323 (5915): 721–23. https://doi.org/10.1126/science.1167742.\n\n\nLazer, David, Alex Pentland, Duncan J. Watts, Sinan Aral, Susan Athey, Noshir Contractor, Deen Freelon, et al. 2020. “Computational Social Science: Obstacles and Opportunities.” Science 369 (6507): 1060–62. https://doi.org/10.1126/science.aaz8170.\n\n\nLiang, Hai, and King-wa Fu. 2015. “Testing Propositions Derived from Twitter Studies: Generalization and Replication in Computational Social Science.” Edited by Zi-Ke Zhang. PLOS ONE 10 (8): e0134270. https://doi.org/10.1371/journal.pone.0134270.\n\n\nPetti, Samantha, and Abraham Flaxman. 2020. “Differential Privacy in the 2020 US Census: What Will It Do? Quantifying the Accuracy/Privacy Tradeoff.” Gates Open Research 3 (April): 1722. https://doi.org/10.12688/gatesopenres.13089.2.\n\n\nRibeiro, Filipe N., Fabrício Benevenuto, and Emilio Zagheni. 2020. “How Biased Is the Population of Facebook Users? Comparing the Demographics of Facebook Users with Census Data to Generate Correction Factors.” 12th ACM Conference on Web Science, July. https://doi.org/10.1145/3394231.3397923.\n\n\nRowe, Francisco. 2021. “Big Data and Human Geography.” http://dx.doi.org/10.31235/osf.io/phz3e.\n\n\n———. 2022. “Using Digital Footprint Data to Monitor Human Mobility and Support Rapid Humanitarian Responses.” Regional Studies, Regional Science 9 (1): 665–68. https://doi.org/10.1080/21681376.2022.2135458.\n\n\nRowe, Francisco, Ruth Neville, and Miguel González-Leonardo. 2022. “Sensing Population Displacement from Ukraine Using Facebook Data: Potential Impacts and Settlement Areas.” http://dx.doi.org/10.31219/osf.io/7n6wm.\n\n\nSchlosser, Frank, Vedran Sekara, Dirk Brockmann, and Manuel Garcia-Herranz. 2021. “Biases in Human Mobility Data Impact Epidemic Modeling.” https://doi.org/10.48550/ARXIV.2112.12521.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2019. “Geographic Data Science.” Geographical Analysis 53 (1): 61–75. https://doi.org/10.1111/gean.12194.\n\n\nZagheni, Emilio, and Ingmar Weber. 2015. “Demographic Research with Non-Representative Internet Data.” Edited by Nikolaos Askitas and Professor Professor Klaus F. Zimmermann. International Journal of Manpower 36 (1): 13–25. https://doi.org/10.1108/ijm-12-2014-0261."
  },
  {
    "objectID": "geodemographics.html#sec-sec31",
    "href": "geodemographics.html#sec-sec31",
    "title": "3  Geodemographics",
    "section": "3.1 Dependencies",
    "text": "3.1 Dependencies\nThis chapter uses the libraries below. Ensure they are installed on your machine, then execute the following code chunk to load them:\n\n#Support for simple features, a standardised way to encode spatial vector data\nlibrary(sf)\n#Data manipulation\nlibrary(dplyr)\n#A system for creating graphics\nlibrary(ggplot2)\n#Easy viisualisation of a correlation matrix using ggplot2\nlibrary(ggcorrplot)\n#Color maps designed to improve graph readability\nlibrary(viridis)\n#Alternative way of plotting, useful for radial plots\nlibrary(plotrix)\n#Methods for cluster analysis\nlibrary(cluster)\n#Thematic maps can be generated with great flexibility\nlibrary(tmap)\n#Provides some easy-to-use functions to extract and visualize the output of multivariate data analyses\nlibrary(factoextra)\n\n\n #Obtain the working directory, where we will save the data directory\ngetwd()\n\n[1] \"/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202223/r4ps\""
  },
  {
    "objectID": "geodemographics.html#sec-sec32",
    "href": "geodemographics.html#sec-sec32",
    "title": "3  Geodemographics",
    "section": "3.2 Data",
    "text": "3.2 Data\n\n3.2.1 Demographic data for Greater London Authority\nIn this Chapter we will be looking at data provided by London Datastore, a website created by the Greater London Authority (GLA) to distribute openly and freely London’s data. In particular, we have prepared the file lsoa-data-clean.csv based on the LSOA Atlas, which contains demographic and related data for each Lower Layer Super Output Area (LSOA) in Greater London.\nLSOAs are geographic hierarchies designed to improve the reporting of small area statistics in England and Wales. LSOAs are built from groups of contiguous Output Areas (OAs) and have been automatically generated to be as consistent as possible in population size, with a minimum population of 1,000. For this reason, their spatial extent varies depending on how densely populated a region is. The average population of an LSOA in London in 2010 was 1,722.\n\n\n3.2.2 Import the data\nIn the code chunk below we load the dataset described above, lsoa-data-clean.csv as a data frame and call it df_LSOA. We will be generating some maps to show the geographical distribution of our data and results. To do this, we need the data that defines the geographical boundaries of the LSOAs. This data can be found in the form of a shapefile here. We have also stored this shapefile, called LSOA_2011_Lodnon_gen_MHW.shp, in the data folder of this workbook so you can import it directly as a data frame with simple features using the st_read() function from the sf package. For more information on the sf package, check the documentation.\n\n# Import LSOA demographic data for GLA\n# The raw data can be obtained from link below, but it has been cleaned by Carmen Cabrera-Arnau for this chapter\n# https://data.london.gov.uk/dataset/lsoa-atlas\ndf_LSOA <- read.csv(\"./data/geodemographics/lsoa-data-clean.csv\")\n\n# Import LSOA boundaries for GLA\nst_LSOA <- st_read(\"./data/geodemographics/LSOA_2011_London_gen_MHW/LSOA_2011_London_gen_MHW.shp\")\n\nReading layer `LSOA_2011_London_gen_MHW' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202223/r4ps/data/geodemographics/LSOA_2011_London_gen_MHW/LSOA_2011_London_gen_MHW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4835 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid"
  },
  {
    "objectID": "geodemographics.html#sec-sec34",
    "href": "geodemographics.html#sec-sec34",
    "title": "3  Geodemographics",
    "section": "3.3 Preparing the data for GDC",
    "text": "3.3 Preparing the data for GDC\n\n3.3.1 Choice of geographic units\nNormally, GDCs involve the analysis of aggregated demographic data into geographic units. Very small geographic units of data aggregation can provide more detailed results, but if the counts are too low, this could lead to re-identification issues.\nAs mentioned above, the data for this chapter is aggregated into LSOAs. The size of the LSOAs is small enough to produce detailed results and is also a convenient choice, since it is broadly used in the UK Census and other official data-reporting exercises.\nWe can visualise the LSOAs within GLA simply by plotting the geometry column of sf_LSOA, which can be selected with the function st_geometry().\n\nplot(st_geometry(st_LSOA), border=adjustcolor(\"gray20\", alpha.f=0.4), lwd=0.6)\n\n\n\n\n\n\n3.3.2 Variables of interest\nAny classification task must be based on certain criteria that determines how elements are grouped into classes. For GDC, these criteria are demographic characteristics of the population located in the geographic units under study. In this case, we have prepared the file lsoa-data-clean.csv to contain some interesting demographic data corresponding to each LSOA. The data frame df_LSOA contains this data and we can visualise its first few lines by using the function head():\n\nhead(df_LSOA[,1:4])\n\n  Lower.Super.Output.Area    LSOA11NM MidYrPop MidYrPop0to15\n1               E01000907 Camden 001A     1431         20.68\n2               E01000908 Camden 001B     1564         18.16\n3               E01000909 Camden 001C     1602         14.86\n4               E01000912 Camden 001D     1589         16.17\n5               E01000913 Camden 001E     1695         17.23\n6               E01000893 Camden 002A     1563         17.08\n\n\nAs we can see, each row contains information about an LSOA and each column (starting from the third column) represents a demographic characteristic of the LSOA and the people living there. With the function names(), we can get the names of the columns in df_LSOA\n\nnames(df_LSOA[,1:8])\n\n[1] \"Lower.Super.Output.Area\" \"LSOA11NM\"               \n[3] \"MidYrPop\"                \"MidYrPop0to15\"          \n[5] \"MidYrPop16to29\"          \"MidYrPop30to44\"         \n[7] \"MidYrPop45to64\"          \"MidYrPop65\"             \n\n\nThe data frame df_LSOA contains many variables. As we can see above, they have summarised names. For a short description of what these names mean, we can load the file called Dictionary-lsoa-data-clean.csv:\n\ndf_dictionary <- read.csv(\"./data/geodemographics/Dictionary-lsoa-data-clean.csv\")\n\nhead(df_dictionary)\n\n           Label                                   Description\n1       LSOA11NM                                  Name of LSOA\n2       MidYrPop   Mid-year Population Estimates;All Ages;2011\n3  MidYrPop0to15  Mid-year Population Estimates;Aged 0-15;2011\n4 MidYrPop16to29 Mid-year Population Estimates;Aged 16-29;2011\n5 MidYrPop30to44 Mid-year Population Estimates;Aged 30-44;2011\n6 MidYrPop45to64 Mid-year Population Estimates;Aged 45-64;2011\n\n\nFor the purposes of this chapter, we will focus on just a few of these variables since this will make the results easier to interpret. In particular, we will look at variables related to ethnicity, country of birth, employment status and qualifications. Let us select the fields of interest:\n\ndf_LSOA <- df_LSOA[, c(\"LSOA11NM\", \"White\", \"MixedMulti\", \"Asian\", \"BlackAfricanCaribbean\", \"Ethnic_Other\", \"BAME\", \"BirthUK\", \"BirthNotUK\", \"FullTimeStudent\", \"EmployRate\", \"UnemployRate\", \"Q_None\", \"Q_L1\", \"Q_L2\", \"Q_Apprenticeship\", \"Q_L3\", \"Q_L4OrAbove\", \"Q_Other\", \"Q_Sch_FullTimeSt18\" )]\n\nWe can explore the summary statistics for each of the selected fields with the summary() function applied on the field of interest. For example, to obtain the summary statistics for the percentage of people belonging to the ethnic group ‘Black/African/Caribbean/Black British’, we can run the code below:\n\nsummary(df_LSOA$BlackAfricanCaribbean)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.10    4.30    9.50   13.05   18.90   63.70 \n\n\nThis tells us that the mean or average percentage of people from this ethnic group in LSOAs within GLA is 13.05%. It also tells us that 63.70% of the population are Black/African/Caribbean/Black British in the LSOA with the maximum proportion of people belonging to this ethnic group.\nTo visualise the whole distribution of the variable ‘Percentage of Black/African/Caribbean/Black British’, we can plot a histogram:\n\nhist(df_LSOA$BlackAfricanCaribbean, breaks=50, xlab=\"% Black/African/Caribbean/Black British\", ylab='Number of LSOAs', main=NULL)\n\n\n\n\nThe histogram reveals that many LSOAs have a low proportion of Black/African/Caribbean/Black British people, but there are a few with more than 50% of their population belonging to this ethnic group.\nNow the question is whether the LSOAs with similar proportions of Black/African/Caribbean/Black British are also spatially close. To find out, we need to map the data. We can do this by joining the data frame df_LSOA with the data frame st_LSOA which contains the geographic boundaries of the LSOAs:\n\njoin_LSOA <- st_LSOA %>% left_join(df_LSOA, by='LSOA11NM')\n\nIf we plot the joined data frames using the tmap library functionalities, we can observe that, indeed there are specific regions within GLA with a high proportion of Black/African/Caribbean/Black British people.\n\nlegend_title = expression(\"% Black, African, Caribbean or Black British\")\nmap_ethnic = tm_shape(join_LSOA) +\n  tm_fill(col = \"BlackAfricanCaribbean\", title = legend_title, text.size = 10, palette = viridis(256), style = \"cont\") + # add fill\n  tm_layout(legend.position = c(0.71, 0.02), legend.title.size=0.7, inner.margins=c(0.05, 0.05, 0.05, 0.14)) +\n  tm_borders(col = \"white\", lwd = .01)  + # add borders\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 1) + # add compass\n  tm_scale_bar(breaks = c(0,1,2,3), text.size = 0.8, position =  c(\"left\", \"bottom\")) # add scale bar\nmap_ethnic"
  },
  {
    "objectID": "geodemographics.html#sec-sec3",
    "href": "geodemographics.html#sec-sec3",
    "title": "3  Geodemographics",
    "section": "3.4 Standardisation",
    "text": "3.4 Standardisation\n\n3.4.1 Across geographic units\nAlthough LSOAs have been designed to have similar population sizes, the population figures fluctuate. And of course, if the population of a place is bigger or smaller, this can affect the figures corresponding to demographic characteristics (e.g. presumably, the larger the total population, the higher the number of people who are unemployed).\nTo counter the effect of variable population sizes across geographic units, we always need to standardise the data so it is given as a proportion or a percentage. This has already done in the dataset lsoa-data-clean.csv, however, if you were to create your own dataset, you need to take this into account. To compute the right percentages, it is important to consider the right denominator. For example, if we are computing the percentage of people over the age of 65 in a given geographic unit, we can divide the number of people over 65 by the total population in that geographic unit, then multiply by 100. However, if we are computing the percentage of single-person households, we need to divide the number of single-person households by the total number of households (and not by the total population), then multiply by 100.\n\n\n3.4.2 Variable standardisation\nData outliers are often present when analysing data from the real-world. These values are generally extreme and difficult to treat statistically. In GDC, they could end up dominating the classification process. To avoid this, we need to standardise the input variables as well, so that they all contribute equally to the classification process.\nThere are different methods for variable standardisation, but here we will achieve this by computing the Z-scores for each variable, i.e. for variable \\(X\\), Z-score = \\(\\dfrac{X-mean(X)}{std(X)}\\) where \\(std()\\) refers to standard deviation. In R, obtaining the Z-score of a variable is very simple with the function scale(). Since we want to obtain the Z-scores of all the variables under consideration, we can loop over the columns corresponding to the variables that we want to standardise:\n\n# creates a new data frame\ndf_std <- df_LSOA\n# extracts column names from df_std\ncolnames_df_std <- colnames(df_std)\n# loops columns from position 1 : the last column\nfor(i in 2: ncol (df_std)){\ndf_std[, colnames_df_std[i]] <- scale(as.numeric(df_std[, colnames_df_std[i]]))\n}"
  },
  {
    "objectID": "geodemographics.html#sec-sec35",
    "href": "geodemographics.html#sec-sec35",
    "title": "3  Geodemographics",
    "section": "3.5 Checking for variable association",
    "text": "3.5 Checking for variable association\nBefore diving into the clustering process, it is necessary to check for variable associations. Two variables that are strongly associated could be conveying essentially the same information. Consequently, excessive weight could be attributed to the phenomenon they refer to in the clustering process. There are different techniques to check for variable association, but here we focus on the Pearson’s correlation matrix.\nEach row and column in a Pearson’s correlation matrix represents a variable. Each entry in the matrix represents the level of correlation between the variables represented by the corresponding row and column. In R, a Pearson’s correlation matrix can be created very easily with the corr() function, where the method parameter is set to “pearson”. As a general rule, two variables with correlation coefficient greater than 0.8 or smaller than -0.8 are considered to be highly correlated. If this is the case, we might want to discard one of the two variables since the information they convey is redundant. However, in some cases, it might be reasonable to keep both variables if we can argue that they both have a similar but unique meaning.\nThe correlation coefficients by themselves are not enough to conclude whether two variables are correlated. Each correlation coefficient must be computed in combination with its p-value. For this reason, we also apply the cor_pmat() function below, which outputs a matrix of p-values corresponding to each correlation coefficient. Here, we set the confidence level at 0.95, therefore, p-values smaller than 0.05 are considered to be statistically significant. In the correlation matrix plot, we add crosses to indicate which correlation coefficients are not significant (i.e. those above 0.05). Those crosses indicate that there is not enough statistical evidence to reject the claim that the variables in the corresponding row and column are uncorrelated.\n\n# Matrix of Pearson correlation coefficients\ncorr_mat <- cor(df_std[,c(colnames_df_std[2: ncol(df_std)])], method = \"pearson\")\n# Matrix of p-values\ncorr_pmat <- cor_pmat(df_std[,c(colnames_df_std[2: ncol(df_std)])], method = \"pearson\", conf.level = 0.95)\n# Barring the no significant coefficient\nggcorrplot(corr_mat, tl.cex=7, hc.order = TRUE, outline.color = \"white\", p.mat = corr_pmat, colors = c(viridis(3)), lab=TRUE, lab_size=1.6)\n\n\n\n\nAmong the statistically significant values in the correlation matrix, we can see that BAME and White have a correlation of -1. So do BirthUK and BirthNotUK. We will therefore remove BAME and BrithUK from our dataset. We also see that Q_L4OrAbove has a strong negative correlation with Q_None and Q_L1. We will therefore remove two of these variables, for example Q_None and Q_L1.\n\n#  Remove BAME, BirthUK, Q_None, Q_L1\ndata <- subset(df_std, select = -c(BAME, BirthUK, Q_None, Q_L1))\n\nWe can now perform a join of the resulting dataset with the variable st_LSOA, which stores the geographical units for the LSOAs. We perform this step so that we can later map the results\n\njoin_data <- st_LSOA %>% left_join(data, by='LSOA11NM')\n\nAnd once again, we can check the Pearson correlation matrix of the resulting dataset. Obviously, now that we have removed some variables that were strongly correlated to others, the values of the correlation coefficients are not as high as before.\n\n#Obtain column names from data\ncolnames_data <- colnames(data)\n# Matrix of Pearson correlation coefficients\ncorr_mat_data <- cor(data[,c(colnames_data[2: ncol(data)])], method = \"pearson\")\n# Matrix of p-values\ncorr_pmat_data <- cor_pmat(data[,c(colnames_data[2: ncol(data)])], method = \"pearson\")\n# Barring the no significant coefficient\nggcorrplot(corr_mat_data, tl.cex=7, hc.order = TRUE, outline.color = \"white\", p.mat = corr_pmat_data, colors = c(viridis(3)), lab=TRUE, lab_size=1.6)"
  },
  {
    "objectID": "geodemographics.html#sec-sec36",
    "href": "geodemographics.html#sec-sec36",
    "title": "3  Geodemographics",
    "section": "3.6 The clustering process",
    "text": "3.6 The clustering process\n\n3.6.1 K-means\nK-means clustering is a way of grouping similar items together. To illustrate the method, imagine you have a bag filled with vegetables, and you want to separate them into smaller bags based on their color, size and flavour. K-means would do this for you by first randomly selecting a number k of vegetables (you provide k, e.g. k=4), and then grouping all the other vegetables based on which of the k vegetables selected initially they are closest to in color, size and flavour. This process is repeated a few times until the vegetables are grouped as best as possible. The end result is k smaller bags, each containing veg of similar color, size and flavour. This is similar to how k-means groups similar items in a data set into clusters.\nMore technically, k-means clustering is actually an algorithm of unsupervised learning (we will learn more about this in Chapter 10) that partitions a set of points into k clusters, where k is a user-specified number. The algorithm iteratively assigns each point to the closest cluster, based on the mean of the points in the cluster, until no point can be moved to a different cluster to decrease the sum of squared distances between points and their assigned cluster mean. The result is a partitioning of the points into k clusters, where the points within a cluster are as similar as possible to each other, and as dissimilar as possible from points in other clusters.\nIn R, k-means can be easily applied by using the function k-means(), where some of the required arguments are: the dataset, the number of clusters (which is called centers), the number of random sets to choose (nstart) or the maximum number of iterations allowed. For example, for a 4-cluster classification, we would run the following line of code:\n\nKm <- kmeans(data[,c(colnames_data[2: ncol(data)])], centers=4, nstart=20, iter.max = 1000)\n\n\n\n3.6.2 Number of clusters\nAs mentioned above, the number of clusters k is a parameter of the algorithm that has to be specified by the user. Ultimately, there is no right or wrong answer to the question ‘what is the optimum number of clusters?’. Deciding the value of k in the k-means algorithm can be a somewhat subjective process where in most cases, common sense is the most useful approach. For example, you can ask yourself if the obtained groups are meaningful and easy to interpret or if, on the other hand, there are too few or too many clusters, making the results unclear.\nHowever, there are some techniques and guidelines to help us decide what the right number of clusters is. Here we explore the silhouette score method.\nThe silhouette score of a data point (in this case an LSOA and its demographic data) is a measure of how similar this data point is to the data points in its own cluster compared to the data points in other clusters. The silhouette score ranges from -1 to 1, with a higher value indicating that the data point is well matched to its own cluster and poorly matched to neighbouring clusters. A score close to 1 means that the data point is distinctly separate from other clusters, whereas a score close to -1 means the data point may have been assigned to the wrong cluster. Given a number of clusters k obtained with k-means, we can compute the average silhouette score over all the data points. Then, we can plot the average silhouette score against k. The optimal value of k will be the one with the highest k score.\nYou can run the code below to compute the average silhouette score corresponding to different values of k ranging from 2 to 8. The optimum number of clusters is given by the value of k at which the average silhouette is maximised.\n\nsilhouette_score <- function(k){\n  km <- kmeans(data[,c(colnames_data[2: ncol(data)])], centers = k, nstart=5, iter.max = 1000)\n  ss <- silhouette(km$cluster, dist(data[,c(colnames_data[2: ncol(data)])]))\n  mean(ss[, 3])\n}\nk <- 2:8\navg_sil <- sapply(k, silhouette_score)\nplot(k, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)\n\n\n\n\nFrom the figure, we can see that the optimum k is 5, so we will take 5 as the number of clusters. Note, this number might be different when you run the programme since the clustering algorithm involves some random steps.\n\nKm <- kmeans(data[,c(colnames_data[2: ncol(data)])], centers=5, nstart=20, iter.max = 1000)\n\n\n\n3.6.3 Other clustering methods\nThere are several other clustering methods apart from k-means. Each method has its own advantages and disadvantages, and the choice of method will ultimately depend on the specific data and clustering problem. We will not explore these methods in detail, but below we include some of their names and a brief description. If you want to learn about them, you can refer to the book “Pattern Recognition and Machine Learning” by Christopher Bishop (Bishop 2006).\n\nFuzzy C-means: a variation of k-means where a data point can belong to multiple clusters with different membership levels.\nHierarchical clustering: this method forms a tree-based representation of the data, where each leaf node represents a single data point and the branches represent clusters. A popular version of this method is agglomerative hierarchical clustering, where individual data points start as their own clusters, and are merged together in a bottom-up fashion based on similarity.\nDBSCAN: a density-based clustering method that groups together nearby points and marks as outliers those points that are far away from any cluster.\nGaussian Mixture Model (GMM): GMMs are probabilistic models that assume each cluster is generated from a Gaussian distribution. They can handle clusters of different shapes, sizes, and orientations."
  },
  {
    "objectID": "geodemographics.html#sec-sec37",
    "href": "geodemographics.html#sec-sec37",
    "title": "3  Geodemographics",
    "section": "3.7 GDC results",
    "text": "3.7 GDC results\n\n3.7.1 Mapping the clusters\nOur LSOAs are now grouped into 5 clusters according to the similarity in their demographic characteristics. We can create a final data set based on join_data which includes the cluster where each geographical unit belongs to:\n\njoin_data_cluster <- join_data\njoin_data_cluster$cluster <- Km$cluster\n\nFinally, we can plot the results of the clustering process on a map using functions from the tmap library:\n\nmap_cluster = tm_shape(join_data_cluster) +\n  tm_fill(col = \"cluster\", title = \"cluster\", palette = viridis(256), style = \"cont\") + # add fill\n  tm_borders(col = \"white\", lwd = .01)  + # add borders\n  tm_layout(legend.position = c(0.88, 0.02)) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 1) + # add compass\n  tm_scale_bar(breaks = c(0,1,2,3), text.size = 0.5, position =  c(\"left\", \"bottom\")) # add scale bar\nmap_cluster\n\n\n\n\nNote: sometimes, the number of items in a cluster may be very small. In that case, you may want to merge to cluster to make the number of items in each group more homogeneous or perhaps change k in the k-means algorithm.\n\n\n3.7.2 Cluster interpretation\nThe map above not only displays the clusters where each LSOA belongs, but it also shows that there is a tendency for LSOAs belonging to the same cluster to be geographically close. This indicates that people with similar demographic characteristics live close to each other. However, we still need to understand what each cluster represents.\nThe so-called cluster centers (kmCenters) are the data points that, within each cluster, provide a clear indication of the average characteristics the cluster where they belong based, of course, on the variables used in the classification. The data used in the clustering process was Z-score standardized, so the values of each variable corresponding to the cluster centers are still presented as Z-scores. Zero indicates the mean for each variable across all the data points in the sample, and values above or below zero indicate the number of standard deviations from the average. This makes it easy to understand the unique characteristics of each cluster relative to the entire sample. To visualise the characteristics and meaning of the clusters centers and their corresponding clusters, we use radial plots. Below we produce a radial plot for cluster 1. Can you see which variables are higher or lower than average in this cluster? If you want to visualise the radial plot for other clusters, you will need to change the number inside the brackets of KmCenters[1,].\n\n# creates a radial plot for cluster 1\n# the boxed.radial (False) prevents white boxes forming under labels\n# radlab rotates the labels\nKmCenters <- as.matrix(Km$centers)\nKmCenters <- as.data.frame(KmCenters)\nradial.plot(KmCenters[1,], labels = colnames(KmCenters),\nboxed.radial = FALSE, show.grid = TRUE,\nline.col = \"blue\", radlab = TRUE, rp.type=\"p\", label.prop=0.9, mar=c(3,3,3,3))\n\n\n\n\n\n\n3.7.3 Testing\nWe will evalueate the fit of the k-means model with 5 clusters by creating an x-y plot of the of the first two principal components of each data point. Each point is coloured according to the cluster where it belongs. Remember that the aim of principal component analysis is to create the minimum number of new variables based on a combination of the original variables that can explain the variability in the data. The first principal component is the new variable that captures the most variability.\nIn the plot below, we can see the first and second principal components in the x and y axes respectively, with the axis label indicating the amount of variability that these components are able to explain. To create the plot, we use the fviz_cluster() function from the factoextra library.\n\nfviz_cluster(Km, data = data[,c(colnames_data[2: ncol(data)])], geom = \"point\", ellipse = F, pointsize = 0.5,\nggtheme = theme_classic())\n\n\n\n\nThere are obvious clusters in the plot, but some points are in the overlapping regions of two or more clusters, making it unclear to what cluster they should really belong. This does not mean that our classification is wrong, instead, it is a result of the fact that the plot is only representing two of the principal components, and there are other variables that are not captured in this 2-dimensional representation."
  },
  {
    "objectID": "geodemographics.html#questions",
    "href": "geodemographics.html#questions",
    "title": "3  Geodemographics",
    "section": "3.8 Questions",
    "text": "3.8 Questions\nFor this set of questions, we will be using the same datasets that we used for Chapter 3, the London LSOA dataset and the shapefile for the LSOA boundaries:\n\ndf_LSOA <- read.csv(\"./data/geodemographics/lsoa-data-clean.csv\")\n\n# Import LSOA boundaries for GLA\nst_LSOA <- st_read(\"./data/geodemographics/LSOA_2011_London_gen_MHW/LSOA_2011_London_gen_MHW.shp\")\n\nReading layer `LSOA_2011_London_gen_MHW' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202223/r4ps/data/geodemographics/LSOA_2011_London_gen_MHW/LSOA_2011_London_gen_MHW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4835 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n\nThis time, we will focus on demographic variables related to ethnicity, country of birth, housing ownership status, income and age group. Let us select these demographic variables for the questions below.\n\ndf_LSOA <- df_LSOA[, c(\"LSOA11NM\", \"White\", \"MixedMulti\", \"Asian\", \"BlackAfricanCaribbean\", \"Ethnic_Other\", \"BAME\", \"BirthUK\", \"BirthNotUK\", \"OwnedOutright\", \"OwnedMortgage\", \"SocialRent\", \"PrivateRent\", \"Hh_MeanIncome\", \"MidYrPop0to15\", \"MidYrPop16to29\", \"MidYrPop30to44\", \"MidYrPop45to64\", \"MidYrPop65\")]\n\nPrepare your data for a geodemographic classification (GDC). To do this, start by standardising the selected variables. Then, check for variable association using a correlation matrix. Discard any variables if necessary. Join the resulting dataset with the LSOA boundary data. Now you should be ready to group the data into clusters using the k-means algorithm. Based on the average silhouette score method, select the number of clusters for a GDC with k-means. Every time you apply the kmeans() function, you should set nstart=20 and iter.max=1000.\n\nEssay questions:\n\nDescribe how you prepared your data for the GDC. There is no need to include figures, but you should briefly explain how you reached certain decisions. For example, did you discard any variables due to their strong association with other variables in the dataset? How did you pick the number of clusters for your GDC?\nMap the resulting clusters and generate a radial plot for one of the clusters. You should create just one figure with as many subplots as needed.\nDescribe what you observe and comment on your results. Do you observe any interesting patterns? Do the results of this GDC agree with what you would expect? Justify your answer.\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer."
  },
  {
    "objectID": "sequence-analysis.html#dependencies",
    "href": "sequence-analysis.html#dependencies",
    "title": "4  Sequence Analysis",
    "section": "4.1 Dependencies",
    "text": "4.1 Dependencies\nWe use the libraries below. Note that to use the theme_tufte2() used for ggplot() objects in this chapter, you need to call the file data-viz-themes.R in the repository.\n\n# data manipulation\nlibrary(tidyverse)\n# spatial data manipulation\nlibrary(stars)\nlibrary(sf)\n# download world pop data\nlibrary(wpgpDownloadR) # you may need to install this package running `install.packages(\"devtools\")` `devtools::install_github(\"wpgp/wpgpDownloadR\")'\n# data visualisation\nlibrary(viridis)\nlibrary(RColorBrewer)\nlibrary(patchwork)\nlibrary(ggseqplot) # may need to install by running `devtools::install_github(\"maraab23/ggseqplot\")`\n# sequence analysis\nlibrary(TraMineR)\n# cluster analysis\nlibrary(cluster)\n\nKey packages to this chapter are TraMineR,stars and ggseqplot. TraMineR is the go-to package in social sciences for exploring, analysing and rendering sequences based on categorical data. stars is designed to handle spatio-temporal data in the form of dense arrays, with space and time as dimensions. stars provides classes and methods for reading, manipulating, plotting and writing data cubes. It is a very powerful package. It interacts nicely with sf and is suggested to be superior to raster and terra, which are also known for their capacity to work with multilayer rasters. stars is suggested to deal with more complex data types and be faster than raster and terra. ggseqplot provides functionality to visualise categorical sequence data based on ggplot capabilities. This differs from TraMineR which is based on the base function plot. We prefer ggseqplot for the wide usage of ggplot as a data visualisation tool in R."
  },
  {
    "objectID": "sequence-analysis.html#data",
    "href": "sequence-analysis.html#data",
    "title": "4  Sequence Analysis",
    "section": "4.2 Data",
    "text": "4.2 Data\nThe key aim of this chapter is to define representative trajectories of population decline using sequence analysis and WorldPop data. We use WorldPop data for the period extending from 2000 to 2020. WorldPop offers open access gridded population estimates at a high spatial resolution for all countries in the world. WoldPop produces these gridded datasets using a top-down (i.e. dissagregating administrative area counts into smaller grid cells) or bottom-up (i.e. interpolating data from counts from sample locations into grid cells) approach. You can learn about about these approaches and the data available from WorldPop.\nWorldPop population data are available in various formats: * Two spatial resolutions: 100m and 1km; * Constrained and unconstrained counts to built settlement areas; * Adjusted or unadjusted to United Nations’ (UN) national population counts; * Two formats i.e. tiff and csv formats.\nWe use annual 1km gridded, UN adjusted, unconstrained population count data for Ukraine during 2000-2021 in tiff format. We use tiff formats to illustrate the manipulation of raster data. Such skills will come handy if you ever decide to work with satellite imagery or image data in general.\nBefore calling the data, let’s see how we can use wpgpDownloadR package. Let’s browse the data catalogue.\n\nwpgpListCountries() %>% \n  head()\n\nWarning in readLines(con, n = 1): incomplete final line found on\n'/var/folders/9z/ql42lpgn22x_c5353k3ycqfr0000gn/T//Rtmpmg0W6N/wpgpDatasets.md5'\n\n\n  ISO ISO3            Country\n1 643  RUS             Russia\n2 360  IDN          Indonesia\n3 840  USA      United States\n4 850  VIR Virgin_Islands_U_S\n5 304  GRL          Greenland\n6 156  CHN              China\n\n\nBy using the ISO3 country code, let’s look for the available datasets for Ukraine.\n\nwpgpListCountryDatasets(ISO3 = \"UKR\") %>% \n  head()\n\nWarning in readLines(con, n = 1): incomplete final line found on\n'/var/folders/9z/ql42lpgn22x_c5353k3ycqfr0000gn/T//Rtmpmg0W6N/wpgpDatasets.md5'\n\n\n     ISO ISO3 Country Covariate\n232  804  UKR Ukraine  ppp_2000\n481  804  UKR Ukraine  ppp_2001\n730  804  UKR Ukraine  ppp_2002\n979  804  UKR Ukraine  ppp_2003\n1228 804  UKR Ukraine  ppp_2004\n1477 804  UKR Ukraine  ppp_2005\n                                                                                                                                                                 Description\n232  Estimated total number of people per grid-cell 2000 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n481  Estimated total number of people per grid-cell 2001 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n730  Estimated total number of people per grid-cell 2002 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n979  Estimated total number of people per grid-cell 2003 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n1228 Estimated total number of people per grid-cell 2004 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n1477 Estimated total number of people per grid-cell 2005 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n\n\nThe wpgpDownloadR package includes 100m resolution data. To keep things efficient, we use 1km gridded population counts from the WorldPop data page. Obtain population data for Ukraine 2000-2020. We start by reading the set of tiff files using the read_stars function from the star package.\n\n# create a list of file names\nfile_list <- fs::dir_ls(\"./data/sequence-analysis/raster\")\nfile_list\n\n./data/sequence-analysis/raster/ukr_ppp_2000_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2001_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2002_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2003_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2004_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2005_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2006_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2007_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2008_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2009_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2010_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2011_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2012_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2013_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2014_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2015_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2016_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2017_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2018_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2019_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2020_1km_Aggregated_UNadj.tif\n\n# read a list of raster data\npop_raster <- read_stars(file_list, quiet = TRUE)\n\nWe map the data for 2000 to get a quick understanding of the data.\n\nplot(pop_raster[1], col = inferno(100))\n\ndownsample set to 2\n\n\n\n\n\nNext we read shapefile of administrative boundaries in the form of polygons. We obtain these data from the GADM website. GADM provides maps and spatial data for individuals countries at the national and sub-national administrative divisions. In this chapter, we will work with these data as they come directly from the website which provides a more realistic and similar context to which you will probably come across in the “real-world”.\n\n# read spatial data frame\nukr_shp <- st_read(\"./data/sequence-analysis/ukr_shp/gadm41_UKR_2.shp\") %>% \n  st_simplify(., # simplify boundaries for efficiency\n              preserveTopology = T,\n              dTolerance = 1000) %>%  # 1km\n  sf::st_make_valid(.) %>% \n  fortify(.) %>%  # turns maps into a data frame so they can more easily be plotted with ggplot2\n  st_transform(., \"EPSG:4326\") # set projection system\n\nReading layer `gadm41_UKR_2' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202223/r4ps/data/sequence-analysis/ukr_shp/gadm41_UKR_2.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 629 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 22.14045 ymin: 44.38597 xmax: 40.21807 ymax: 52.37503\nGeodetic CRS:  WGS 84\n\nukr_shp\n\nSimple feature collection with 629 features and 13 fields (with 1 geometry empty)\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 22.14519 ymin: 44.38681 xmax: 40.21807 ymax: 52.375\nGeodetic CRS:  WGS 84\nFirst 10 features:\n       GID_2 GID_0 COUNTRY   GID_1   NAME_1 NL_NAME_1            NAME_2\n1          ?   UKR Ukraine       ?        ?         ?                 ?\n2  UKR.1.1_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська        Cherkas'ka\n3  UKR.1.2_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська       Cherkas'kyi\n4  UKR.1.3_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська   Chornobaivs'kyi\n5  UKR.1.4_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська     Chyhyryns'kyi\n6  UKR.1.5_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська       Drabivs'kyi\n7  UKR.1.6_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська Horodyshchens'kyi\n8  UKR.1.7_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська       Kamians'kyi\n9  UKR.1.8_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська         Kanivs'ka\n10 UKR.1.9_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська        Kanivs'kyi\n         VARNAME_2 NL_NAME_2      TYPE_2                     ENGTYPE_2 CC_2\n1                ?        NA           ?                            NA   NA\n2               NA        NA Mis'ka Rada City of Regional Significance   NA\n3               NA        NA       Raion                      District   NA\n4  Chornobayivskyi        NA       Raion                      District   NA\n5               NA        NA       Raion                      District   NA\n6               NA        NA       Raion                      District   NA\n7  Gorodyschenskyi        NA       Raion                      District   NA\n8               NA        NA       Raion                      District   NA\n9               NA        NA       Misto                          City   NA\n10              NA        NA       Raion                      District   NA\n     HASC_2                       geometry\n1         ? POLYGON ((30.59574 50.40547...\n2  UA.CK.CM POLYGON ((32.1715 49.43881,...\n3  UA.CK.CR POLYGON ((32.03393 49.49881...\n4  UA.CK.CB POLYGON ((32.17991 49.44486...\n5  UA.CK.CY POLYGON ((32.26144 49.20893...\n6  UA.CK.DR POLYGON ((32.41852 49.83724...\n7  UA.CK.HO POLYGON ((31.56959 49.42509...\n8  UA.CK.KN POLYGON ((32.19797 49.20946...\n9  UA.CK.KM MULTIPOLYGON (((31.4459 49....\n10 UA.CK.KR POLYGON ((31.5851 49.62482,...\n\n\nLet’s have a quick look at the resolution of the administrative areas we will be working. The areas below represent areas at the administrative area level 2 in the spatial data frame ukr_shp.\n\nplot(ukr_shp$geometry)\n\n\n\n\nWe ensure that the pop_raster object is in the same projection system as ukr_shp. So we can make both objects to work together.\n\npop_raster <- st_transform(pop_raster, st_crs(ukr_shp))                      \n\n\n4.2.1 Data wrangling\nFor our application, we want to work with administrative areas for three reasons. First, public policy and planning decisions are often made based on administrative areas. These are the areas local governments have jurisdiction, represent and can exert power. Second, migration is a key component of population change and hence directly determines population decline. At a small area, residential mobility may also impact patterns of population potentially adding more complexity and variability to the process. Third, WorldPop data are modelled population estimates with potentially high levels of uncertainty or errors in certain locations. Our aim is to mitigate the potential impacts of these errors.\nWe therefore recommend working with aggregated data. We aggregate the 1km gridded population data to administrative areas in Ukraine. We use system.time to time the duration of the proccess of aggregation which could take some time depending on your local computational environment.\n\nsystem.time({\n\npopbyarea_df = aggregate(x = pop_raster, \n                                   by = ukr_shp, \n                                   FUN = sum, \n                                   na.rm = TRUE) \n})\n\n   user  system elapsed \n 63.794  11.082  75.280 \n\n\nSub-national population\nThe chunk code above returns a list of raster data. We want to create a spatial data frame containing population counts for individual sub-national areas and years. We achieve this by running the following code:\n\n# create a function to bind the population data frame to the shapefile\nadd_population <- function(x) mutate(ukr_shp, \n                      population = x)\n\n# obtain sub-national population counts\nukr_eshp <- lapply(popbyarea_df, add_population)\n\n# create a dataframe with sub-national populations\nselect_pop <- function(x) dplyr::select(x, GID_2, NAME_2, population)\npopulation_df <- lapply(ukr_eshp, select_pop) %>% \n  do.call(rbind, .)\npopulation_df$year <- rep(seq(2000, 2020, by=1), times = 1)\nrownames(population_df) <- rep(seq(1, nrow(population_df), by=1), times = 1)\n\n# sub-national spatial data frame\npopulation_df \n\nSimple feature collection with 13209 features and 4 fields (with 21 geometries empty)\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 22.14519 ymin: 44.38681 xmax: 40.21807 ymax: 52.375\nGeodetic CRS:  WGS 84\nFirst 10 features:\n       GID_2            NAME_2 population                       geometry year\n1          ?                 ?  301849.00 POLYGON ((30.59574 50.40547... 2000\n2  UKR.1.1_1        Cherkas'ka  280917.39 POLYGON ((32.1715 49.43881,... 2001\n3  UKR.1.2_1       Cherkas'kyi   89116.78 POLYGON ((32.03393 49.49881... 2002\n4  UKR.1.3_1   Chornobaivs'kyi   50096.24 POLYGON ((32.17991 49.44486... 2003\n5  UKR.1.4_1     Chyhyryns'kyi   36646.73 POLYGON ((32.26144 49.20893... 2004\n6  UKR.1.5_1       Drabivs'kyi   42467.86 POLYGON ((32.41852 49.83724... 2005\n7  UKR.1.6_1 Horodyshchens'kyi   49886.59 POLYGON ((31.56959 49.42509... 2006\n8  UKR.1.7_1       Kamians'kyi   35587.28 POLYGON ((32.19797 49.20946... 2007\n9  UKR.1.8_1         Kanivs'ka   14406.93 MULTIPOLYGON (((31.4459 49.... 2008\n10 UKR.1.9_1        Kanivs'kyi   37495.04 POLYGON ((31.5851 49.62482,... 2009\n\n\nNational population\nWe also create a data frame providing population counts at the national level.\n\n# obtain national population counts\npopulation_count <- map_dbl(ukr_eshp, ~.x %>% \n          pull(population) %>% \n          sum(na.rm = TRUE)\n        ) %>% \n  as.data.frame()\n\n# change labels\ncolnames(population_count) <-  c(\"population\")\nrownames(population_count) <- rep(seq(2000, 2020, by=1), times = 1)\npopulation_count$year <- rep(seq(2000, 2020, by=1), times = 1)\n\n# national annual population counts\npopulation_count\n\n     population year\n2000   47955683 2000\n2001   47520197 2001\n2002   47094225 2002\n2003   46700872 2003\n2004   46330322 2004\n2005   46011048 2005\n2006   45734099 2006\n2007   45502336 2007\n2008   45286748 2008\n2009   45090608 2009\n2010   44923112 2010\n2011   44744969 2011\n2012   44593427 2012\n2013   44424702 2013\n2014   44250993 2014\n2015   44068072 2015\n2016   43856852 2016\n2017   43622605 2017\n2018   43391259 2018\n2019   43140679 2019\n2020   42880388 2020\n\n\n\n\n4.2.2 Exploratory data analysis\nNow we are ready to start analysing the data. Before building complexity on our analysis, conducting some exploratory data analysis to understand the data is generally a good starting point, particularly given the multi-layer nature of the data at hand - capturing space, time and population levels.\nNational patterns\nWe first analyse national population trends. We want to know to what extent the population of Ukraine has declined over time over the last 20 years. An effective way to do this is to compute summary statistics and visualise the data. Below we look at year-to-year changes in population levels and as a percentage change. By using patchwork, we combine two plots into a single figure.\n\n# visualise national population trends\npop_level_p <- ggplot(population_count, \n       aes(x = year, y = population/1000000 )) +\n  geom_line(size = 1) +\n  theme_tufte2() +\n  ylim(0, 48) + \n  labs(y = \"Population \\n(million)\",\n       x = \"Year\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# visualise percentage change in population\npop_percent_p <- population_count %>% \n  mutate(\n    pct_change = ( ( population - 47955683) / 47955683) * 100\n  ) %>% \nggplot(aes(x = year, y = pct_change )) +\n  geom_line(size = 1) +\n  theme_tufte2() + \n  labs(y = \"Population \\npercentage change (%)\",\n       x = \"Year\") \n\npop_level_p | pop_percent_p\n\n\n\n\nSub-national\nPopulation losses are likely vary across the country. From previous research we know that rural and less well connected areas tend to lose population through the internal migration of young individuals as they move for work and job opportunities (Rowe, Corcoran, and Bell 2016). We also know that they tend to move to large, densely populated cities where these opportunities are concentrated and because they also offer a wide variety of amenities and activities. Cities tend to work as accelarators enabling fast career development and occupational progression (Fielding 1992). Though, we have also seen the shrinkage of populations in cities, particularly in eastern European countries (Turok and Mykhnenko 2007).\nTo examine the patterns of sun-national population losses, we compute two summary measures: (1) annual percentage change in population; and, (2) overall percentage change in population between 2000 and 2021. We start by looking the overall percentage change as it is easier to visualise. To this end, we categorise our measure of overall percentage change into seven different classes. Based on previous work by González-Leonardo, Newsham, and Rowe (2023), we classify changes into high decline (\\(\\leq\\) -3), decline (> -3 and \\(\\leq\\) -1.5), moderate decline (> -1.5 and \\(\\leq\\) -0.3), stable (< -0.3 and < 0.3), moderate growth (\\(\\geq\\) 0.3 and < 1.5), growth (\\(\\geq\\) 1.5 and < 3) and high growth (\\(\\geq\\) 3). Let’s first create the measures of population change.\n\n# compute population change metrics\npopulation_df <- population_df %>% \n  dplyr::group_by(GID_2) %>% \n  arrange(-year, .by_group = TRUE ) %>% \n  mutate(\n  pct_change = ( population / lead(population) - 1) * 100, # rate of population change\n  pct_change_2000_21 = ( population[year == \"2020\"] / population[year == \"2000\"] - 1) * 100, # overall rate of change\n  ave_pct_change_2000_21 = mean(pct_change, na.rm = TRUE)\n) %>% \n  ungroup()\n\nLet’s map the overall percentage change in population between 2000 and 2020. We see a wide spread pattern of population decline across Ukraine. We observe a large spatial cluster of population decline around the capital city of Kiev and moderate population decline across most of the country. Administrative areas containing large cities seem to record overall population growth between 2000 and 2020, potentially absorbing population movements from the rest of the country. What else do you think may be driving population growth in cities? And in contrast, what do you think is contributing to population decline in other parts of Ukraine?\n\n# set colours\ncols <- c(\"#7f3b08\", \"#b35806\", \"#e08214\", \"#faf0e6\", \"#8073ac\", \"#542788\", \"#2d004b\")\n# reverse order\ncols <- rev(cols)\n\npopulation_df %>% dplyr::filter( year == 2020) %>%\n  drop_na(pct_change_2000_21) %>% \n    mutate(\n    ove_pop_class = case_when( pct_change_2000_21 <= -3 ~ 'high_decline',\n                           pct_change_2000_21 <= -1.5 & pct_change_2000_21 > -3 ~ 'decline',\n                           pct_change_2000_21 <= -.3 & pct_change_2000_21 > -1.5 ~ 'moderate_decline',\n                           pct_change_2000_21 > -0.3 & pct_change_2000_21 < 0.3 ~ 'stable',\n                           pct_change_2000_21 >= 0.3 & pct_change_2000_21 < 1.5 ~ 'moderate_growth',\n                           pct_change_2000_21 >= 1.5 & pct_change_2000_21 < 3 ~ 'growth',\n                           pct_change_2000_21 >= 3 ~ 'high_growth'),\n    ove_pop_class = factor(ove_pop_class, \n         levels = c(\"high_decline\", \"decline\", \"moderate_decline\", \"stable\", \"moderate_growth\", \"growth\", \"high_growth\") )\n    ) %>% \n  ggplot(aes(fill = ove_pop_class)) +\n  geom_sf(col = \"white\", size = .1) +\n  scale_fill_manual(values = cols,\n                    name = \"Population change\") +\n  theme_map_tufte() \n\n\n\n\nNow that we have understanding of population changes over the whole 2000-2020 period. Let’s try to understand how different places arrive to different outcomes. A way to do this is to look at the evolution of population changes. Different trajectories of population change could underpin the outcomes of population change that we observe today. Current outcomes could be the result of a consistent pattern of population decline over the last 20 years. They could be the result of acceleration in population loss after a major natural or war event, or they could reflect a gradual process of erosion. We visualise way to get an understanding of this is to analyse annual percentage population changes across individual areas. We use a Hovmöller Plot as illustrated by Rowe and Arribas-Bel (2022) for the analysis of spatio-temporal data.\n\npopulation_df %>% dplyr::filter( ave_pct_change_2000_21 < 0) %>% \n  tail(., 40*21) %>% \n  ggplot(data = ., \n           mapping = aes(x= year, y= reorder(NAME_2, pct_change), fill= pct_change)) +\n  geom_tile() +\n  scale_fill_viridis(name=\"Population\", option =\"plasma\", begin = .2, end = .8, direction = 1) +\n  theme_tufte2() +\n  labs(title= paste(\" \"), x=\"Year\", y=\"Area\") +\n  theme(text = element_text(size=14)) + \n  theme(axis.text.y = element_text(size=8))\n\n\n\n\nThe Hovmöller Plot shows that most of the selected areas tend to experience moderate annual percentage changes in population between -5 and 5. Though, it also reveals sudden large losses in specific years. Thus, while Hovmöller Plots provide some understanding of the annual population changes, they are limited in that we can only analyse a handful of areas at the time and it is therefore difficult to identify systematic representative patterns. Here we have selected 40 areas of a total of 629. Displaying the total number of areas in a Hovmöller Plot will not produce readable results. Even if that was the case, it would be difficult to identify systematic patterns. As we will seek to persuade you below, sequence analysis provides a very novel way to define representative trajectories in the data, identify systematic patterns and extract distinctive features characterising those trajectories."
  },
  {
    "objectID": "sequence-analysis.html#application",
    "href": "sequence-analysis.html#application",
    "title": "4  Sequence Analysis",
    "section": "4.3 Application",
    "text": "4.3 Application\nNext, we focus on the application of sequence analysis to identify representative trajectories of population decline at the sub-national level between 2000 and 2020 in Ukraine. Intuitively, sequence analysis can be seen as a four-stage process. First, it requires the definition of longitudinal categorical outcome. Second, it measures the dissimilarity of individual sequences via a process known as optimal matching (OM). Third, it uses these dissimilarity measures to define a typology of representative trajectories using unsupervised machine learning clustering techniques. Fourth, trajectories can be visualised and their distinctive features can be measured. Below we describe the implementation of each stage to identify representative trajectories of population decline.\n\n4.3.1 Defining outcome process\nSequence analysis requires longitudinal categorical data as an input. We therefore classify our population count data into distinct categorical categories, henceforth referred to as states of population change. We compute the annual percentage rate of population change for individual areas and use these rates to measure the extent and pace of population change. The annual rate of population change is computed as follows:\n\\[\n{p(t1) - p(t0) \\over p(t0)}*100\n\\]\nwhere: \\(p(t0)\\) is the population at year t0 and \\(p(t1)\\) is the population at t + 1.\nAs previously, we differentiate areas of high decline, decline, moderate decline, stable, moderate growth, growth and high growth. For the analysis, we focus on areas recording population losses between 2000 and 2020. The histogram shows the magnitude and distribution of population decline over this period. We observe that most occurrences of decline are moderate around zero, while very few exceed 5%.\n\n# select areas reporting losses between 2000 and 2020\npopulation_loss_df <- population_df %>% \n  dplyr::filter( pct_change_2000_21 < 0)\n# plot distribution of percentage change \npopulation_loss_df %>% \n  dplyr::filter(pct_change  < 0) %>% \n  ggplot(data =  ) +\n  geom_density(alpha=0.8, colour=\"black\", fill=\"lightblue\", aes(x = pct_change)) +\n  theme_tufte2()\n\n\n\n\nNext we classify the annual percentage of population change into our seven states.\n\n# remove 2000 as it has no observations of population change\npopulation_loss_df <- population_loss_df %>% \n  dplyr::filter( year != 2000)\n# clasify data\npopulation_loss_df <- population_loss_df %>%\n  mutate(\n    pop_class = case_when( pct_change <= -3 ~ 'high_decline',\n                           pct_change <= -1.5 & pct_change > -3 ~ 'decline',\n                           pct_change <= -.3 & pct_change > -1.5 ~ 'moderate_decline',\n                           pct_change > -0.3 & pct_change < 0.3 ~ 'stable',\n                           pct_change >= 0.3 & pct_change < 1.5 ~ 'moderate_growth',\n                           pct_change >= 1.5 & pct_change < 3 ~ 'growth',\n                           pct_change >= 3 ~ 'high_growth')\n)\n\n\n\n4.3.2 Optimal matching\nWe measure the extent of dissimilarity between individual sequence of population decline. To this end, we used a sequence analysis technique, OM, which computes distances between sequences as a function of the number of transformations required to make sequences identical. Two sets of operations are generally used: (1) insertion/deletion (known as indel) and (2) substitution operations. Both of these operations represent the cost of transforming one sequence into another. These costs are challenging to define and below we discuss what is generally used in empirical work. Intuitively, the idea of OM is to estimate the cost of transforming one sequence into another so that the greater the cost to make two sequences identical, the greater the dissimilarity and vice versa.\nIndel operations involve the addition or removal of an element within the sequence and substitution operations are the replacement of one element for another. Each of these operations is assigned a cost, and the distance between two sequences is defined as the minimum cost to transform one sequence to another (Abbott and Tsay 2000). By default, indel costs are set to 1. To illustrate indel operations, let’s consider an example of sequences of annual population change for three areas during 2000 and 2003. The sequences are identical, except for 2003. In this case, indel operations involve the cost of transforming the status stable in the sequence for area 1 to high decline in the sequence for area 2, and thus this operation would return a cost is 2. Why 2? It is 2 because you would need to delete stable and add high decline. Now, let’s try the cost of transforming the status stable in the sequence for area 1 to the status in the sequence for area 3 using indel operations. What is the cost? The answer is 1 because we only need to delete stable to make it identical.\n\n\n\nArea\n2000\n2001\n2002\n2003\n\n\n\n\n1\ndecline\ndecline\ndecline\nstable\n\n\n2\ndecline\ndecline\ndecline\nhigh decline\n\n\n3\ndecline\ndecline\ndecline\n-\n\n\n\nSubstitution operations or costs represent transition costs; that is, the cost for substituting each state with another. Substitution costs are defined in one of two ways (Salmela-Aro et al. 2011). One approach is the theory-driven approach. In such approach, substitution costs are grounded in theory suggesting that, for example, transforming state 1 to state 2 should have a greater cost than transforming state 1 to state 3, or performing the opposite operation i.e. transforming state 2 to state 1. An example could be that it is more financially costly to transition from full-time employment to full-time education than transition from full-time education to full-time employment.\nA second approach and most commonly used in empirical work is a data-driven approach. In this approach, substitution costs are empirically derived from transition rates between states. The cost of substitution is inversely related to the frequency of observed transitions within the data. This means that infrequent transitions between states have a higher substitution cost. For example, as we will see, transitions from the state of decline to moderate decline are rarer than decline to growth in Ukraine. The transition rate between state \\(i\\) and state \\(j\\) is the probability of observing state \\(j\\) at time \\(t1\\) given that the state \\(i\\) is observed at time \\(t\\) for \\(i \\neq j\\). The substitution cost between states \\(i\\) and \\(j\\) is computed as:\n\\[\n2 - {p(i | j) - p(j | i)}\n\\] where \\(p(i | j)\\) is the transition rate between state \\(i\\) and \\(j\\).\nTo implement OM, we first need to rearrange the structure of our data from long to wide format. You can now see now that individual rows represent areas (column 1) and columns from 2 to 21 represent years.\n\nsee Rowe and Arribas-Bel (2022) for a description on different spatio-temporal data structures and their manipulation using tidyverse principles.\n\n\n# transform from long to wide format\nwide_population_loss_df <- population_loss_df %>% \n  as_tibble() %>% \n  group_by(GID_2) %>% \n  arrange(year, .by_group = TRUE ) %>%\n  ungroup() %>% \n  tidyr::pivot_wider(\n  id_cols =  GID_2,\n  names_from = \"year\",\n  values_from = \"pop_class\"\n)\n  \nwide_population_loss_df\n\n# A tibble: 522 × 21\n   GID_2   `2001` `2002` `2003` `2004` `2005` `2006` `2007` `2008` `2009` `2010`\n   <chr>   <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr> \n 1 UKR.1.… moder… high_… moder… moder… stable moder… moder… moder… moder… stable\n 2 UKR.1.… moder… moder… moder… moder… moder… growth growth growth moder… growth\n 3 UKR.1.… moder… moder… moder… moder… moder… moder… moder… moder… moder… moder…\n 4 UKR.1.… moder… moder… moder… moder… moder… moder… moder… moder… moder… moder…\n 5 UKR.1.… moder… moder… moder… moder… moder… moder… moder… moder… moder… growth\n 6 UKR.1.… moder… moder… moder… moder… moder… moder… moder… moder… moder… moder…\n 7 UKR.1.… moder… moder… moder… moder… moder… moder… moder… moder… moder… moder…\n 8 UKR.1.… moder… moder… moder… moder… moder… moder… moder… moder… moder… moder…\n 9 UKR.1.… moder… moder… moder… moder… moder… stable moder… moder… moder… moder…\n10 UKR.1.… growth growth growth growth growth growth growth growth growth growth\n# … with 512 more rows, and 10 more variables: `2011` <chr>, `2012` <chr>,\n#   `2013` <chr>, `2014` <chr>, `2015` <chr>, `2016` <chr>, `2017` <chr>,\n#   `2018` <chr>, `2019` <chr>, `2020` <chr>\n\n\nOnce the data frame has been reshaped into a wide format, we define the data as a state sequence object using the R package TraMineR. Key here is to appropriately define the labels and an appropriate palette of colours. Depending on the patterns you are seeking to capture a diverging, sequential or qualitative colour palette may be more appropriate. For this chapter, we use a diverging colour palette as we want to effectively represent areas experiencing diverging patterns of population decline or growth.\n\nNote: various types of sequence data representation exist in TraMineR. These representations vary in the way they capture states or events. Chapter 4 in Gabadinho et al. (2009) describes the various representations that TraMineR can handle. In any case, the state sequence representation used in this chapter is the most commonly used and internal format used by TraMineR. Hence we focus on it.\n\n\n# alphabet\nseq.alphab <- c(\"high_growth\", \"growth\", \"moderate_growth\", \"stable\", \"moderate_decline\", \"decline\", \"high_decline\")\n# labels\nseq.lab <- c(\"High growth\", \"Growth\", \"Moderate growth\", \"Stable\", \"Moderate decline\", \"Decline\", \"High decline\")\n# define state sequence object\nseq.cl <- seqdef(wide_population_loss_df, \n                 2:21, \n                 alphabet = seq.alphab,\n                 labels = seq.lab,\n                 cnames = c(\"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"),\n                 cpal =c(\"1\" = \"#7f3b08\", \n                         \"2\" = \"#b35806\",\n                         \"3\" = \"#e08214\",\n                         \"4\" = \"#faf0e6\",\n                         \"5\" = \"#8073ac\",\n                         \"6\" = \"#542788\",\n                         \"7\" = \"#2d004b\"))\n\n [>] 7 distinct states appear in the data: \n\n\n     1 = decline\n\n\n     2 = growth\n\n\n     3 = high_decline\n\n\n     4 = high_growth\n\n\n     5 = moderate_decline\n\n\n     6 = moderate_growth\n\n\n     7 = stable\n\n\n [>] state coding:\n\n\n       [alphabet]       [label]          [long label] \n\n\n     1  high_growth      high_growth      High growth\n\n\n     2  growth           growth           Growth\n\n\n     3  moderate_growth  moderate_growth  Moderate growth\n\n\n     4  stable           stable           Stable\n\n\n     5  moderate_decline moderate_decline Moderate decline\n\n\n     6  decline          decline          Decline\n\n\n     7  high_decline     high_decline     High decline\n\n\n [>] 522 sequences in the data set\n\n\n [>] min/max sequence length: 20/20\n\n\nUsing the sequence data object, we create a state distribution plot to get an understanding of the data. The plot shows the distribution of areas across status of population change in individual years. The overall picture emerging from the plots is that while areas in the sample recorded overall population decline between 2000 and 2020, the predominant year-to-year pattern was moderate growth. This suggests that areas may have experienced one or two years of severe population loss resulting in an overall pattern of decline.\n\nseqplot(seq.cl, \n        title=\"State distribution plot\", \n        type = \"d\",\n        with.legend = \"right\",\n        border = NA)\n\n [!!] In .main() : title is deprecated, use main instead.\n\n\n\n\n\nWe now move on to compute the substitution costs for our population states. From the equation above, you may have realised that transition costs vary between 0 and 2, with the former indicating zero cost. The latter indicates the maximum cost of converting one state into another. The option TRATE in method states to derive costs from observed transition rates. That is the data-driven approach discussed above. From the matrix below, you can see that it is more costly to convert a status “high decline” to “decline” than “high decline” to “growth”. This is an unexpected result. Intuitively, we could have expected the opposite. We would naturally expect areas to continue their trajectory population of decline, rather than a shift to growth. Though patterns of temporary decline have been identified as a predominant trajectory of population decline across Europe, particularly in Germany (Newsham and Rowe 2022a).\n\nNote we are considering a fixed measure of transition rates. That means that we are using the whole dataset to compute an average transition rate between states. That assumes that the rate of change between states does not change over time. Yet, there may be good reasons to believe they do as areas move across different states. In empirical work, time varying transition rates are more often considered. That means we use temporal slices of the data to compute transition rates; for example, using data from 2001, 2002 and so on. In this way, we end up with potentially different transition rates for every year.\n\n\n# Calculate transition rates\nsubs_costs <- seqsubm(seq.cl, \n                      method = \"TRATE\",\n                      #time.varying = TRUE\n                      )\n\n [>] creating substitution-cost matrix using transition rates ...\n\n\n [>] computing transition probabilities for states high_growth/growth/moderate_growth/stable/moderate_decline/decline/high_decline ...\n\nsubs_costs\n\n                 high_growth   growth moderate_growth   stable moderate_decline\nhigh_growth         0.000000 1.847966        1.870644 1.855502         1.837147\ngrowth              1.847966 0.000000        1.696672 1.925177         1.841392\nmoderate_growth     1.870644 1.696672        0.000000 1.556781         1.566771\nstable              1.855502 1.925177        1.556781 0.000000         1.689740\nmoderate_decline    1.837147 1.841392        1.566771 1.689740         0.000000\ndecline             1.790673 1.879887        1.800011 1.639568         1.821239\nhigh_decline        1.702113 1.672678        1.293439 1.943686         1.988414\n                  decline high_decline\nhigh_growth      1.790673     1.702113\ngrowth           1.879887     1.672678\nmoderate_growth  1.800011     1.293439\nstable           1.639568     1.943686\nmoderate_decline 1.821239     1.988414\ndecline          0.000000     1.980392\nhigh_decline     1.980392     0.000000\n\n\nTo understand better the idea of substitution costs, we can have direct look at transition rates underpinning these costs. Transition rates can be computed via seqtrate . By definition, transition rates vary between 0 and 1, with zero indicating no probability of a transition occurring. One indicates a 100% probability of a transition taking place. Thus, for example, the matrix below tell us that there is a 66% probability of observing a transition from high decline to growth in our sample. Examining transition rates could provide very valuable information about the process in analysis.\n\nseq.trate <- seqtrate(seq.cl)\n\n [>] computing transition probabilities for states high_growth/growth/moderate_growth/stable/moderate_decline/decline/high_decline ...\n\nround(seq.trate, 2)\n\n                      [-> high_growth] [-> growth] [-> moderate_growth]\n[high_growth ->]                  0.16        0.14                 0.13\n[growth ->]                       0.01        0.64                 0.22\n[moderate_growth ->]              0.00        0.09                 0.78\n[stable ->]                       0.01        0.05                 0.38\n[moderate_decline ->]             0.05        0.14                 0.42\n[decline ->]                      0.14        0.12                 0.20\n[high_decline ->]                 0.05        0.23                 0.66\n                      [-> stable] [-> moderate_decline] [-> decline]\n[high_growth ->]             0.14                  0.11         0.07\n[growth ->]                  0.03                  0.02         0.00\n[moderate_growth ->]         0.07                  0.02         0.00\n[stable ->]                  0.46                  0.07         0.01\n[moderate_decline ->]        0.24                  0.12         0.02\n[decline ->]                 0.35                  0.16         0.02\n[high_decline ->]            0.04                  0.01         0.00\n                      [-> high_decline]\n[high_growth ->]                   0.25\n[growth ->]                        0.09\n[moderate_growth ->]               0.04\n[stable ->]                        0.02\n[moderate_decline ->]              0.00\n[decline ->]                       0.02\n[high_decline ->]                  0.01\n\n\nNow we focus on the probably most important component of sequence analysis; that is the calculation of dissimilarity. Recall our aim is to identify representative trajectories. To this end, we need a way to measure how similar or different sequences are - which is known as OM. Above, we described that we can use indel and substitution operations to measure the dissimilarity or costs between individual sequences. The code chunk implements OM based on indel and substitution operations. The algorithm takes an individual sequence and compares it with all of the sequences in the dataset, and identifies the sequence with the minimum cost i.e. the most similar sequence. The result of this computing intensive process is a distance matrix encoding the similarity or dissimilarity between individual sequences.\n\nFor indel, auto sets the indel as max(sm)/2 when sm is a matrix. For more details, run ?seqdist on your console\n\n\n# Calculate a distance matrix\nseq.om <- seqdist(seq.cl,\n                  method = \"OM\", # specify the method\n                  indel = \"auto\", # specify indel costs\n                  sm = subs_costs) # specify substitution costs\n\n [>] 522 sequences with 7 distinct states\n\n\n [>] checking 'sm' (size and triangle inequality)\n\n\n [>] 441 distinct  sequences \n\n\n [>] min/max sequence lengths: 20/20\n\n\n [>] computing distances using the OM metric\n\n\n [>] elapsed time: 0.093 secs\n\n\nAs highlighted above, if you would like to apply varying substitution costs, you can do this directly here by using the option method = DHD .\n\n\n4.3.3 Clustering\nThe resulting distance matrix from OM seq.om indicates the degree of similarity between individual sequences. To identify representative trajectories, we then need to a way to group together similar sequences to produce a typology, in this case of population decline trajectories. Unsupervised cluster analysis is generally used for this task. Trusting you have built an understanding of cluster analysis from the previous chapter, we will not provide an elaborate description here. If you would like to know more about cluster analysis, we recommend the introductory book by Kaufman and Rousseeuw (2009). We use a clustering method called k-meloids . This methods is known to be more robust to noise and outliers than the conventional k-means procedure (Backman, Lopez, and Rowe 2020). This is because the medoid algorithm clusters the data by minimising a sum of pair-wise dissimilarities (Kaufman and Rousseeuw 2009), rather than a sum of squared Euclidean distances. We run cluster analyses at different numbers of k starting from 2 to 20.\n\n# run PAMs\nfor (k in 2:20)\n  pam_sol <- pam(seq.om, k)\n\nWe then seek to determine the optimal number of clusters k. We use silhouette scores, but as we noted Chapter 3, the optimal number of clusters is better determined by the user given the context and use case. It is an art. There is no wrong or right answer. As can be seen from the results below from the average silhouette score, two clusters is suggested as the optimal solution. However, we could argue that we gain very little from such coarse partition of the data. We suggest to take this as guidance and a starting point to look to identify an appropriate data partition. We suggest to visualise different solution and gain an understanding of what data get split and decide on whether the resulting patterns contribute to the understanding of the process at hand.\n\n# compute average silhouette scores for all 20 cluster solutions\nasw <- numeric(20)\nfor (k in 2:20)\n  asw[k] <- pam(seq.om, k) $ silinfo $ avg.width\n  k.best <- which.max(asw)\n  cat(\"silhouette-optimal number of clusters:\", k.best, \"\\n\")\n\nsilhouette-optimal number of clusters: 2 \n\n  asw\n\n [1] 0.0000000 0.5584480 0.5369175 0.5103584 0.4989714 0.4094865 0.3950388\n [8] 0.4190322 0.4234781 0.4143383 0.4047555 0.3961153 0.3338910 0.3280279\n[15] 0.3118651 0.3095253 0.2909311 0.3099763 0.3101910 0.3087775\n\n\nWe rerun and save the results for a 7k cluster partition. If you inspect the resulting data frame, it provides an identifier for each cluster. Each individual area is attributed to a cluster. Next the question that we seek to answer is what sort of pattern do these clusters capture?\n\n# rerun pam for k=7\npam_optimal <- pam(seq.om, 7)\n\n\n\n4.3.4 Visualising\nTo understand the representative patterns captured in our data partition, we use visualisation. There is a battery of different visualisation tools to extract information and identify distinctive features of the identified trajectories. We start by using individual sequence plots by trajectory type. They provide a visual representation of how individual areas in each trajectory type moves between states. Recall that we are capturing representative trajectories; hence, there is still quite a bit of variability in terms of the patterns encapsulated in each representative trajectory. Back to the individual sequence plots, each line in these plots represents an area. Time is displayed horizontally and colours encode different states - in our case of population change. Numbers on the y-axis display the number of areas in each cluster. The figure immediate below relies on the base plot library, and by default, it is not very visually appealing.\n\n# create individual sequence plots\npar(mar=c(1,1,1,1))\nseqplot(seq.cl, \n        group = pam_optimal$clustering,\n        type = \"I\",\n        border = NA, \n        cex.axis = 1.5, \n        cex.lab = 1.5,\n        sortv = seq.om)\n\n\n\n\nWe therefore switch to the R library ggseqplot which enables visualisation of sequence data based on ggplot functionalities. This package may provide more flexibility if we are more familiar with ggplot.\nThe figure below offers a clear representation of the systematic sequencing of states that each trajectory captures. It provides information on two key features of trajectories: sequencing and size. For example, trajectory 1 seems to capture a chaotic sequencing pattern of transitions between moderate growth, stability and decline and high decline. Trajectory 4 represents patterns of areas experiencing high population growth followed by a year of high decline, resulting in overall population decline in 2000-2020. From these plots, you can also identify which trajectories tend to be more common. In our example, trajectories 2 and 3 seem to involve the largest number of areas: 132 and 143, respectively.\n\nNote: the resulting trajectories also raise various questions about the use of modelled data to analyse population trajectories. The data used in this chapter are estimates combining various sources and dissagregating data into small grids. The current results suggest the existence of various sources of noise and uncertainty. We are working with a sample of areas which reported population decline between 2000 and 2020. Yet, the most common pattern across the years is population growth. A closer examination into the data are required. We have revised our code several times and we could not identify any errors in data processing. While we do not discard this 100%, we still think the data need a careful examination.\n\n\n# create individual sequence plots based on ggplot\nggseqiplot(seq.cl, \n        group = pam_optimal$clustering,\n        sortv = seq.om,\n        facet_ncol = 4) +\n  scale_fill_manual(values = rev(cols)) +\n  scale_color_manual(values = rev(cols)) \n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\nWe can also get a better understanding of the resulting trajectories by analysing state frequency plots. They show the number of occurrences of a given state in individual years. These plots examine the data from a vertical perspective i.e. looking at individual years across areas, rather than at individual areas over time. State frequency plots reveal that predominant states in each year and changes in their prevalence. Focusing on trajectory 1, we observe that moderate growth was the predominant state in 2006-07 but declined the following year.\n\n# create state frequency plots based on ggplot\nggseqdplot(seq.cl, \n        group = pam_optimal$clustering,\n        facet_ncol = 4) +\n  scale_fill_manual(values = cols) +\n  scale_color_manual(values = cols) \n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\nWe can also examine time spent in individual states in each trajectory. Time spent plots report the average time spent in each state. The measure of time depends on the original data used in the analysis. We use years so the y-axis refers to the average number of years that a given status appears in a representative trajectory type. For example, a score over 5 for stable in trajectory 1 indicates that the average number of years that areas in that typology are classified in that category is over 5.\n\n# create time spent plots based on ggplot\nggseqmtplot(seq.cl, \n        group = pam_optimal$clustering,\n        facet_ncol = 4) +\n  scale_fill_manual(values = rev(cols)) +\n  scale_color_manual(values = rev(cols)) +\n  scale_x_discrete(labels=c(\"high_growth\" = \"HG\", \"growth\" = \"G\",\n                              \"moderate_growth\" = \"MG\", \"stable\" = \"S\", \"moderate_decline\" = \"MD\", \"decline\" = \"D\", \"high_decline\" = \"HD\" ))\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\nFinally we analyse entropy index plots. Entropy is a measure of diversity. The greater the score, the greater the entropy or diversity of states. The plot below displays the entropy index computed for individual trajectories each year. Each line represents the entropy index for a trajectory in each year. The top yellow line in 2001 indicates that in 2001 areas following a trajectory 7 type were distributed across a larger number of states than any other trajectory. In other word, it indicates that there was more diversity of states.\n\n# create entropy index plots based on ggplot\nggseqeplot(seq.cl, \n        group = pam_optimal$clustering) +\n  scale_colour_viridis_d()\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale."
  },
  {
    "objectID": "sequence-analysis.html#questions",
    "href": "sequence-analysis.html#questions",
    "title": "4  Sequence Analysis",
    "section": "4.4 Questions",
    "text": "4.4 Questions\nFor the first assignment, we will continue to focus on London as our area of analysis. We will use population count estimates from the Office of National Statistics (ONS). The dataset provides information on area, population numbers and population density at national, regional and smaller sub-national area level, including Unitary Authority, Metropolitan County, Metropolitan District, County, Non-metropolitan District, London Borough, Council Area and Local Government District for the period from 2001 to 2020.\n\npop_df <- read_csv(\"./data/sequence-analysis/population_uk/population-uk-2011_20.csv\")\npop_df\n\n# A tibble: 420 × 44\n   Code    Name  Geogr…¹ Area …² Estim…³ 2020 …⁴ Estim…⁵ 2019 …⁶ Estim…⁷ 2018 …⁸\n   <chr>   <chr> <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 K02000… UNIT… Country  2.43e5  6.71e7   276.   6.68e7   275.   6.64e7   274. \n 2 K03000… GREA… Country  2.29e5  6.52e7   285.   6.49e7   283.   6.46e7   282. \n 3 K04000… ENGL… Country  1.51e5  5.97e7   395.   5.94e7   394.   5.91e7   391. \n 4 E92000… ENGL… Country  1.30e5  5.66e7   434.   5.63e7   432.   5.60e7   430. \n 5 E12000… NORT… Region   8.58e3  2.68e6   312.   2.67e6   311.   2.66e6   310. \n 6 E06000… Coun… Unitar…  2.23e3  5.33e5   240.   5.30e5   238.   5.27e5   237. \n 7 E06000… Darl… Unitar…  1.97e2  1.07e5   544.   1.07e5   541.   1.07e5   540. \n 8 E06000… Hart… Unitar…  9.37e1  9.38e4  1001.   9.37e4   999.   9.32e4   995. \n 9 E06000… Midd… Unitar…  5.39e1  1.41e5  2622.   1.41e5  2616.   1.41e5  2608. \n10 E06000… Nort… Unitar…  5.02e3  3.24e5    64.5  3.22e5    64.2  3.20e5    63.8\n# … with 410 more rows, 34 more variables:\n#   `Estimated Population mid-2017` <dbl>, `2017 people per sq. km` <dbl>,\n#   `Estimated Population mid-2016` <dbl>, `2016 people per sq. km` <dbl>,\n#   `Estimated Population mid-2015` <dbl>, `2015 people per sq. km` <dbl>,\n#   `Estimated Population mid-2014` <dbl>, `2014 people per sq. km` <dbl>,\n#   `Estimated Population mid-2013` <dbl>, `2013 people per sq. km` <dbl>,\n#   `Estimated Population mid-2012` <dbl>, `2012 people per sq. km` <dbl>, …\n\n\nFor the assignment, you should only work with smaller sub-national areas. Filter out country and regional area. You should address the following questions:\n\nUse sequence analysis to identify representative trajectories of population change and discuss the type of trajectories identified in London Boroughs.\nUse individual sequence plot to identify distinctive features in the resulting trajectories.\n\nFor the analysis, aim to focus on the area of London so you can link your narrative to the rest of analyses you will be conducting.\nEnsure you justify the number of optimal clusters you will use in your analysis and provide a brief description of the trajectories identified. Describe how they are unique.\n\n\n\n\nAbbott, Andrew, and Angela Tsay. 2000. “Sequence Analysis and Optimal Matching Methods in Sociology: Review and Prospect.” Sociological Methods & Research 29 (1): 3–33.\n\n\nBackman, Mikaela, Esteban Lopez, and Francisco Rowe. 2020. “The Occupational Trajectories and Outcomes of Forced Migrants in Sweden. Entrepreneurship, Employment or Persistent Inactivity?” Small Business Economics 56 (3): 963–83. https://doi.org/10.1007/s11187-019-00312-z.\n\n\nFielding, A. J. 1992. “Migration and Social Mobility: South East England as an Escalator Region.” Regional Studies 26 (1): 1–15. https://doi.org/10.1080/00343409212331346741.\n\n\nGabadinho, Alexis, Gilbert Ritschard, Nicolas S. Müller, and Matthias Studer. 2011. “Analyzing and Visualizing State Sequences inRwithTraMineR.” Journal of Statistical Software 40 (4). https://doi.org/10.18637/jss.v040.i04.\n\n\nGabadinho, Alexis, Gilbert Ritschard, Matthias Studer, and Nicolas S Müller. 2009. “Mining Sequence Data in r with the TraMineR Package: A User’s Guide.” Geneva: Department of Econometrics and Laboratory of Demography, University of Geneva.\n\n\nGonzález-Leonardo, Miguel, Niall Newsham, and Francisco Rowe. 2023. “Understanding Population Decline Trajectories in Spain Using Sequence Analysis.” Geographical Analysis, January. https://doi.org/10.1111/gean.12357.\n\n\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in Data: An Introduction to Cluster Analysis. John Wiley & Sons.\n\n\nNewsham, Niall, and Francisco Rowe. 2022a. “Understanding the Trajectories of Population Decline Across Rural and Urban Europe: A Sequence Analysis.” https://doi.org/10.48550/ARXIV.2203.09798.\n\n\n———. 2022b. “Understanding Trajectories of Population Decline Across Rural and Urban Europe: A Sequence Analysis.” Population, Space and Place, December. https://doi.org/10.1002/psp.2630.\n\n\nPatias, Nikos, Francisco Rowe, and Dani Arribas-Bel. 2021. “Trajectories of Neighbourhood Inequality in Britain: Unpacking Inter-Regional Socioeconomic Imbalances, 1971-2011.” The Geographical Journal 188 (2): 150–65. https://doi.org/10.1111/geoj.12420.\n\n\nPatias, Nikos, Francisco Rowe, Stefano Cavazzi, and Dani Arribas-Bel. 2021. “Sustainable Urban Development Indicators in Great Britain from 2001 to 2016.” Landscape and Urban Planning 214 (October): 104148. https://doi.org/10.1016/j.landurbplan.2021.104148.\n\n\nRowe, Francisco, and Dani Arribas-Bel. 2022. “Spatial Modelling for Data Scientists.” Open Science Framework, August. https://doi.org/10.17605/OSF.IO/8F6XR.\n\n\nRowe, Francisco, Jonathan Corcoran, and Martin Bell. 2016. “The Returns to Migration and Human Capital Accumulation Pathways: Non-Metropolitan Youth in the School-to-Work Transition.” The Annals of Regional Science 59 (3): 819–45. https://doi.org/10.1007/s00168-016-0771-8.\n\n\nSalmela-Aro, Katariina, Noona Kiuru, Jari-Erik Nurmi, and Mervi Eerola. 2011. “Mapping Pathways to Adulthood Among Finnish University Students: Sequences, Patterns, Variations in Family- and Work-Related Roles.” Advances in Life Course Research 16 (1): 25–41. https://doi.org/10.1016/j.alcr.2011.01.003.\n\n\nTatem, Andrew J. 2017. “WorldPop, Open Data for Spatial Demography.” Scientific Data 4 (1). https://doi.org/10.1038/sdata.2017.4.\n\n\nTurok, Ivan, and Vlad Mykhnenko. 2007. “The Trajectories of European Cities, 19602005.” Cities 24 (3): 165–82. https://doi.org/10.1016/j.cities.2007.01.007."
  },
  {
    "objectID": "network.html#sec-sec_dependencies",
    "href": "network.html#sec-sec_dependencies",
    "title": "5  Network Analysis",
    "section": "5.1 Dependencies",
    "text": "5.1 Dependencies\nIn this session we need some basic R packages before importing the data.\n\nlibrary(magrittr)\nlibrary(dplyr)\n\n# An R package for network manipulation and analysis\nlibrary(igraph)"
  },
  {
    "objectID": "network.html#sec-sec_data",
    "href": "network.html#sec-sec_data",
    "title": "5  Network Analysis",
    "section": "5.2 Data",
    "text": "5.2 Data\n\n5.2.1 The US Census dataset\nDescribe here the dataset used for this session. It has been cleaned beforehand by myself.\nEach row corresponds to a origin-destination pair, the number or rows gives the total number of reported migratory movements.\n\n\n5.2.2 Import the data\nBefore we start any analysis with the data, ensure to set the path to the directory where we are working. Please replace in the following line the path to the folder where you have placed the data file.\n\ndf <- read.csv(\"./data/networks/metro_to_metro_2015_2019_US_migration.csv\")\n\n#Ensure the MSA code is imported as a character and not as a number\ndf$MSA_Current_Code <- as.character(df$MSA_Current_Code) \n\n#Include an additional column with the full name of the MSA in the format: Name, State\ndf$MSA_Previous_Name_State <- paste0(df$MSA_Previous_Name, ', ', df$MSA_Previous_State)\ndf$MSA_Current_Name_State <- paste0(df$MSA_Current_Name, ', ', df$MSA_Current_State)\n\n#Examine the first few rows of the dataset\nhead(df) \n\n  MSA_Current_Code MSA_Current_Name MSA_Current_State\n1            10180          Abilene                TX\n2            10180          Abilene                TX\n3            10180          Abilene                TX\n4            10180          Abilene                TX\n5            10180          Abilene                TX\n6            10180          Abilene                TX\n  MSA_Current_Population_1_Year_and_Over_Estimate\n1                                         168,306\n2                                         168,306\n3                                         168,306\n4                                         168,306\n5                                         168,306\n6                                         168,306\n  MSA_Current_Population_1_Year_and_Over_MOE MSA_Previous_Code\n1                                        300             10740\n2                                        300             11100\n3                                        300             12060\n4                                        300             12220\n5                                        300             12420\n6                                        300             12580\n                 MSA_Previous_Name MSA_Previous_State\n1                      Albuquerque                 NM\n2                         Amarillo                 TX\n3 Atlanta-Sandy Springs-Alpharetta                 GA\n4                   Auburn-Opelika                 AL\n5     Austin-Round Rock-Georgetown                 TX\n6        Baltimore-Columbia-Towson                 MD\n  MSA_Previous_Population_1_Year_and_Over_Estimate\n1                                          902,213\n2                                          262,574\n3                                        5,753,503\n4                                          153,728\n5                                        2,045,336\n6                                        2,766,530\n  MSA_Previous_Population_1_Year_and_Over_MOE\n1                                       2,916\n2                                       1,866\n3                                       9,607\n4                                       1,654\n5                                       5,648\n6                                       6,154\n  Movers_Metro_to_Metro_Flow_Estimate Movers_Metro_to_Metro_Flow_MOE\n1                                  41                             47\n2                                 244                             98\n3                                 118                             95\n4                                  30                             40\n5                                 289                             85\n6                                   8                             15\n               MSA_Previous_Name_State MSA_Current_Name_State\n1                      Albuquerque, NM            Abilene, TX\n2                         Amarillo, TX            Abilene, TX\n3 Atlanta-Sandy Springs-Alpharetta, GA            Abilene, TX\n4                   Auburn-Opelika, AL            Abilene, TX\n5     Austin-Round Rock-Georgetown, TX            Abilene, TX\n6        Baltimore-Columbia-Towson, MD            Abilene, TX\n\n\nWe can obtain the total number of reported migratory movements with the following command:\n\nnrow(df)\n\n[1] 52930"
  },
  {
    "objectID": "network.html#sec-sec_create",
    "href": "network.html#sec-sec_create",
    "title": "5  Network Analysis",
    "section": "5.3 Creating networks",
    "text": "5.3 Creating networks\nBefore we start to analyse the data introduced in Section 5.2, let us first take a step back to consider the main object of study of this Chapter: the so-called networks. In the most general sense, a network (also known as a graph) is a structure formed by a set of objects which may have some connections between them. The objects are represented by nodes (a.k.a. vertices) and the connections between these objects are represented by edges (a.k.a. links). Networks are used as a tool to conceptualise many real-life contexts, such as the friendships between the members of a year group at school, the direct airline connections between cities in a continent or the presence of hyperlinks between a set of websites. In this session, we will use networks to model the migratory flows between US cities.\n\n5.3.1 Starting from the basics\nIn order to create, manipulate and analyse networks in R, we will use the igraph package, which we imported in Section 5.2. We start by creating a very simple network with the code below. The network contains five nodes and five edges and it is undirected, so the edges do not have orientations. The nodes and edges could represent, respectively, a set of cities and the presence of migration flows between these cities in two consecutive years.\n\ng1 <- graph( edges=c(1,2, 1,4, 2,3, 2,4, 4,5), n=5, directed=F ) # Creates an undirected network with 5 nodes and 5 edges\n# The number of nodes is given by argument n\n# In this case, the node labels or IDs are represented by numbers 1 to 5\n# The edges are specified as a list of pairs of nodes\nplot(g1) # A simple plot of the network allows us to visualise it\n\n\n\n\nIf the connections between the nodes of a network are non-reciprocal, the network is called directed. For example, this could correspond to a situation where there are people moving from city 1 to city 2, but nobody moving from city 2 to city 1. Note that in the code below we have not only added directions to the edges, but we have also added a few additional parameters to the plot function in order to customise the diagram.\n\ng2 <- graph( edges=c(1,2, 1,4, 2,3, 4,1, 4,2, 4,5), n=7, directed=T ) # Creates a directed network with 7 nodes and 6 edges \n#note that we now have edge 1,4 and edge 4,1 and that 2 of the nodes are isolated\nplot(g2, vertex.frame.color=\"red\",  vertex.label.color=\"black\",\nvertex.label.cex=0.9, vertex.label.dist=2.3, edge.curved=0.3, edge.arrow.size=.5, edge.color = \"blue\", vertex.color=\"yellow\", vertex.size=15) # A simple plot of the network with a few extra features\n\n\n\n\nThe network can also be defined as a list containing pairs of named nodes. Then, it is not necessary to specify the number of nodes but the isolated nodes have to be included. The following code generates a network which is equivalent to the one above.\n\ng3 <- graph( c(\"City 1\",\"City 2\", \"City 2\",\"City 3\", \"City 1\",\"City 4\",  \"City 4\",\"City 1\",  \"City 4\",\"City 2\", \"City 4\",\"City 5\"), isolates=c(\"City 6\", \"City 7\") ) \nplot(g3, vertex.frame.color=\"red\",  vertex.label.color=\"black\",\nvertex.label.cex=0.9, vertex.label.dist=2.3, edge.curved=0.3, edge.arrow.size=.5, edge.color = \"blue\", vertex.color=\"yellow\", vertex.size=15) \n\n\n\n\n\n\n5.3.2 Adding attributes\nIn R, we can add attributes to the nodes, edges and the network. To add attributes to the nodes, we first need to access them via the following command:\n\nV(g3)\n\n+ 7/7 vertices, named, from f2819d0:\n[1] City 1 City 2 City 3 City 4 City 5 City 6 City 7\n\n\nThe node attribute name is automatically generated from the node labels that we manually assigned before.\n\nV(g3)$name\n\n[1] \"City 1\" \"City 2\" \"City 3\" \"City 4\" \"City 5\" \"City 6\" \"City 7\"\n\n\nBut other node attributes could be added. For example, the current population of the cities represented by the nodes:\n\nV(g3)$population <- c(134000, 92000, 549000, 1786000, 74000, 8000, 21000)\n\nSimilarly, we can access the edges:\n\nE(g3)\n\n+ 6/6 edges from f2819d0 (vertex names):\n[1] City 1->City 2 City 2->City 3 City 1->City 4 City 4->City 1 City 4->City 2\n[6] City 4->City 5\n\n\nand add edge attributes, such as the number of people moving from an origin to a destination city in two consecutive years. We call this attribute the weight of the edge, since if there is a lot of people going from one city to another, the connection between these cities has more importance or “weight” in the network.\n\n{E(g3)$weight <- c(2000, 3000, 5000, 1000, 1000, 4000)}\n\nWe can examine the adjacency matrix of the network, which represents the presence of edges between different pairs of nodes. In this case, each row corresponds to an origin city and each column to a destination:\n\ng3[] #The adjacency matrix of network g3\n\n7 x 7 sparse Matrix of class \"dgCMatrix\"\n       City 1 City 2 City 3 City 4 City 5 City 6 City 7\nCity 1      .   2000      .   5000      .      .      .\nCity 2      .      .   3000      .      .      .      .\nCity 3      .      .      .      .      .      .      .\nCity 4   1000   1000      .      .   4000      .      .\nCity 5      .      .      .      .      .      .      .\nCity 6      .      .      .      .      .      .      .\nCity 7      .      .      .      .      .      .      .\n\n\nWe can also look at the existing node and edge attributes.\n\nvertex_attr(g3) #Node attributes of g3. Use edge_attr() to access the edge attributes\n\n$name\n[1] \"City 1\" \"City 2\" \"City 3\" \"City 4\" \"City 5\" \"City 6\" \"City 7\"\n\n$population\n[1]  134000   92000  549000 1786000   74000    8000   21000\n\n\nFinally, it is possible to add network attributes\n\ng3$title <- \"Network of migration between cities\""
  },
  {
    "objectID": "network.html#sec-sec_reading",
    "href": "network.html#sec-sec_reading",
    "title": "5  Network Analysis",
    "section": "5.4 Reading networks from data files",
    "text": "5.4 Reading networks from data files\n\n5.4.1 Preparing the data to create an igraph object\nAt the beginning of the chapter, we defined a data frame called df based on some imported data from the US Census about migratory movements between different US cities, or more precisely, between US Metropolitan Statistical Areas. This is a large data frame containing 52,930 rows, but how can we turn this data frame into a network similar to the ones that we generated in Section 5.3. The igraph function graph_from_data_frame() can do this for us. To find out more about this function, we can run the following command:\n\nhelp(\"graph_from_data_frame\")\n\nAs we can see, the input data for graph_from_data_frame() needs to be in a certain format which is different from our migration data frame. In particular, the function requires three arguments: 1) d, which is a data frame containing an edge list in the first two columns and any additional columns are considered as edge attributes; 2) vertices, which is either NULL or a data frame with vertex metadata (i.e. vertex attributes); and 3) directed, which is a boolean argument indicating whether the network is directed or not. Our next task is therefore to obtain 1) and 2) from the migration data frame called df.\nLet us start with argument 1). Each row in df will correspond to an edge in the migration network since it contains information about a pair of origin and destination cities for two consecutive years. The names of the origin and destination cities are given by the columns in df called MSA_Previous_Name and MSA_Current_Name. In addition, the column called Movers_Metro_to_Metro_Flow_Estimate gives the number of people moving between the origin and the destination cities, so this will be the weight attribute of each edge in the migration network. Hence, we can define a data frame of edges which we will call df_edges that conforms with the format required by the argument 1) as follows:\n\n#The pipe operator used below and denoted by %>% is a feature of the magrittr package, it takes the output of one function and passes it into another function as an argument\n\n# Creates the df_edges data frame with data from df and renames the columns as \"origin\", \"destination\" and \"weight\"\ndf_edges <- data.frame(df$MSA_Previous_Name_State, df$MSA_Current_Name_State, df$Movers_Metro_to_Metro_Flow_Estimate) %>%\n  rename(origin = df.MSA_Previous_Name_State, destination = df.MSA_Current_Name_State, weight = df.Movers_Metro_to_Metro_Flow_Estimate) \n\n#Ensure that the weight attribute is stored as a number and not as character \ndf_edges$weight <- as.numeric(gsub(\",\",\"\",df_edges$weight)) \n\nFor argument 2) we can define a data frame of nodes which we will call df_nodes, where each row will correspond to a unique node or city. To obtain all the unique cities from df, we can firstly obtain a data frame of unique origin cities, then a data frame of unique destinations, and finally, apply the full_join() function to these two data frames to obtain their union, which will be df_nodes. The name of the unique cities in df_nodes is in the column called label, the other columns can be seen as the nodes metadata.\n\ndf_unique_origins <- df %>% \n  distinct(MSA_Previous_Name_State) %>%\n  rename(name = MSA_Previous_Name_State) \n\ndf_unique_destinations <- df %>%\n  distinct(MSA_Current_Name_State) %>%\n  rename(name = MSA_Current_Name_State)\n\ndf_nodes <- full_join(df_unique_origins, df_unique_destinations, by = \"name\")\n\nFinally, a directed migration network can be obtained with the following line of code. It should contain 386 nodes and 52,930 edges. You can test this yourself with the functions that you learnt in Section 5.3.\n\ng_US <- graph_from_data_frame(d = df_edges,\n                                       vertices = df_nodes,\n                                       directed = TRUE)\n\nIf we try to plot the network g3 containing the migratory movements between all the US cities with the plot() function as we did before, we obtain a result which is rather undesirable…\n\nplot(g_US)\n\n\n\n\n\n\n5.4.2 Filtering the data to create a subgraph\nWe will dedicate the entirety of next section to explore tools that can help us improve the visualisation of networks, since it is one of the most important aspects of network analysis. To facilitate the visualisation in the examples shown in Section 5.5, we will work with a subset of the full network called g_US. A way to create a subnetwork is to filter the original data frame. In particular, we will filter df to only include cities from a state, in this case, Minnesota. To filter, we use the grepl() function, which stands for grep logical. Both grep() and grepl() allow us to check whether a pattern is present in a character string or vector of a character string. While the grep() function returns a vector of indices of the element if a pattern exists in that vector, the grepl() function returns TRUE if the given pattern is present in the vector. Otherwise, it returns FALSE. In this case, we are filtering the dataset so that only the rows where the field MSA_Current_State is WA, which is the official abbreviation for Washington state.\n\ndf_sub <- df %>% filter(grepl('WA', MSA_Current_State)) %>% filter(grepl('WA', MSA_Previous_State)) #Filter the original data frame\n\nThen, we can prepare the data as we did before to create gUS. But, instead of basing the network on df, we will generate it from df_sub.\n\ndf_sub_edges <- data.frame(df_sub$MSA_Previous_Name, df_sub$MSA_Current_Name, df_sub$Movers_Metro_to_Metro_Flow_Estimate) %>%\n  rename(origin = df_sub.MSA_Previous_Name, destination = df_sub.MSA_Current_Name, weight = df_sub.Movers_Metro_to_Metro_Flow_Estimate)\n\n#Split long names into several lines for visualisation purposes\ndf_sub_edges$origin <- gsub(\"-\", \"-\\n\", df_sub_edges$origin)\ndf_sub_edges$destination <- gsub(\"-\", \"-\\n\", df_sub_edges$destination)\n\ndf_sub_edges$weight <- as.numeric(gsub(\",\",\"\",df_sub_edges$weight)) \n\ndf_sub_unique_origins <- df_sub %>% \n  distinct(MSA_Previous_Name) %>%\n  rename(name = MSA_Previous_Name) \n\ndf_sub_unique_destinations <- df_sub %>%\n  distinct(MSA_Current_Name) %>%\n  rename(name = MSA_Current_Name)\n\ndf_sub_nodes <- full_join(df_sub_unique_origins, df_sub_unique_destinations, by = \"name\")\ndf_sub_nodes$name <- gsub(\"-\", \"-\\n\", df_sub_nodes$name)\n\ng_sub <- graph_from_data_frame(d = df_sub_edges,\n                                       vertices = df_sub_nodes,\n                                       directed = TRUE)"
  },
  {
    "objectID": "network.html#sec-sec_visualise",
    "href": "network.html#sec-sec_visualise",
    "title": "5  Network Analysis",
    "section": "5.5 Network visualisation",
    "text": "5.5 Network visualisation\n\n5.5.1 Visualisation with igraph\nLet us start by generating the most basic visualisation of g_sub.\n\nplot(g_sub)\n\n\n\n\nThis plot can be improved by changing adding a few additional arguments to the plot() function. For example, by just changing the color and size of the labels, the color and size of the nodes and the arrow size of the edges, we can already see some improvements.\n\nplot(g_sub, vertex.size=10, edge.arrow.size=.2, edge.curved=0.1,\nvertex.color=\"gold\", vertex.frame.color=\"black\",\nvertex.label=V(g_sub)$name, vertex.label.color=\"black\",\nvertex.label.cex=.65)\n\n\n\n\nBut there are few more things we can do not only to improve the look of the diagram, but also to include more information about the network. For example, we can set the size of the nodes so that it reflects the total number of people that the corresponding cities receive. We can do this by adding a new node attribute, inflow, which is obtained as the sum of the rows of the adjacency matrix of g_sub.\n\nV(g_sub)$inflow <- rowSums(as.matrix(g_sub[]))\n\nBelow we set the node size based on the inflow attribute. Note the formula 0.4*(V(gsub)$inflow)^0.4, where the power of 0.4 is chosen to scale the size of the nodes in such a way that the largest ones do not get excessively large and the smallest ones do not get excessively small. We also set the edge width based on its weight, which is the total number of people migrating from the origin and destination cities that it connects.\n\n# Set node size based on inflow of migrants:\nV(g_sub)$size <- 0.4*(V(g_sub)$inflow)^0.4\n# Set edge width based on weight:\nE(g_sub)$width <- E(g_sub)$weight/1200\n\nRun the code below to discover how the aspect of the network has significantly improved with the modifications that we have introduced above.\n\nplot(g_sub, vertex.size=V(g_sub)$size, edge.arrow.size=.15, edge.arrow.width=.2, edge.curved=0.1, edge.width=E(g_sub)$width, edge.color =\"gray80\",\nvertex.color=\"gold\", vertex.frame.color=\"gray90\",\nvertex.label=V(g_sub)$name, vertex.label.color=\"black\",\nvertex.label.cex=.65)\n\n\n\n\n\n\n5.5.2 Visualisation of spatial networks\nFirstly, we will import geographical data for the metropolitan and micropolitan statistical areas in the whole of the US, using the sf package. Here, we are only interested in the metropolitan areas so we will filter the data frame cbsa_us to keep only the metropolitan areas, i.e. those entries with value M1 for the column named LSAD.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(stringr)\n\n#Import core-based statistical areas https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.2020.html#list-tab-YXS5CUH5MBYOZ7MJLN\ncbsa_us <- st_read(\"./data/networks/cb_2020_us_cbsa_500k/cb_2020_us_cbsa_500k.shp\")\n\nReading layer `cb_2020_us_cbsa_500k' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202223/r4ps/data/networks/cb_2020_us_cbsa_500k/cb_2020_us_cbsa_500k.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 939 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -178.3347 ymin: 17.88328 xmax: -65.56427 ymax: 65.45352\nGeodetic CRS:  NAD83\n\nmsa_us <- cbsa_us %>% filter(grepl('M1', LSAD)) #Filter the original data frame to obtain only metro areas\n\nWe will now find the centroid of each MSA polygon and add columns to msa_us for the longitude and latitude of each centroid.\n\n#Add longitude and latitude corresponding to centroid of each MSA polygon\nmsa_us$lon_centroid <- st_coordinates(st_centroid(msa_us$geometry))[,\"X\"]\nmsa_us$lat_centroid <- st_coordinates(st_centroid(msa_us$geometry))[,\"Y\"]\n\nSince we are focusing on Washington state, let us filter msa_us so that it only includes data from Washington. This requires some data manipulation via the library stringr:\n\nmsa_us$NAME_ONLY <- gsub(\",.*$\", \"\", msa_us$NAME) #Create a new column with the name of the state taken from the last two characters of entries in column NAME\n\n#Long names of MSAs are split into lines for visualisation purposes\nmsa_us$NAME_ONLY <- gsub(\"-\", \"-\\n\", msa_us$NAME_ONLY) \nmsa_us$STATE <- substr(msa_us$NAME, nchar(msa_us$NAME)-1, nchar(msa_us$NAME)) #Create a new column with the name of the state taken from the last two characters of entries in column NAME\n\nmsa_sub <- msa_us %>% filter(grepl('WA', STATE)) #Filter to keep the metro areas belonging to Washington state only\n\nWe can now plot the polygons for the MSA belonging to Washington state as well as the centroids:\n\nplot(st_geometry(msa_sub))\nplot(st_centroid(msa_sub$geometry), add=TRUE, col=\"red\", cex=0.5, pch=20)\n\n\n\n\nHowever, we still need to link this data to the network data that we obtained before. In order to incorporate the geographic information to the nodes of the migration subnetwork, we can join data from two data frames: msa_sub, which contains the geographic data, and df_sub_nodes, which contains the names of the nodes. To do this, we can use the function left_join() and then, select only the columns of interest. For more information on this magical function, check this link.\n\n#Join the data frame of nodes df_sub_nodes with the geographic information of the centroid of each MSA\ndf_sub_spatial_nodes <- df_sub_nodes %>% left_join(msa_sub, by = c(\"name\" = \"NAME_ONLY\")) %>% select(c(\"name\", \"lon_centroid\", \"lat_centroid\"))\n\n\nlo <- as.matrix(df_sub_spatial_nodes[,2:3])\n\n\nplot(st_geometry(msa_sub), border=adjustcolor(\"gray50\"))\nplot(g_sub, layout=lo, add = TRUE, rescale = FALSE, vertex.size=V(g_sub)$size, edge.arrow.size=.1, edge.arrow.width=1., edge.curved=0.1, edge.width=E(g_sub)$width, edge.color=adjustcolor(\"gray80\", alpha.f = .6), vertex.color=\"gold\", vertex.frame.color=\"gray90\",\nvertex.label=V(g_sub)$name, vertex.label.color=\"black\",\nvertex.label.cex=.45)\n\n\n\n\n\n\n5.5.3 Alternative visualisations\nIn this session we have based our visualisations on igraph, however, there exist a variety of packages that would also allow us to generate nice plots of networks.\nFor example, COMPLETE\nIn addition, migration networks are particularly well-suited to be represented as a chord diagram. If you want to explore this type of visualisation, you can find further information on the official R documentation and also, for example, on this other link link."
  },
  {
    "objectID": "network.html#sec-sec_metrics",
    "href": "network.html#sec-sec_metrics",
    "title": "5  Network Analysis",
    "section": "5.6 Network metrics",
    "text": "5.6 Network metrics\nHere we define some of the most important metrics that help us quantify different characteristics of a network. We will use the migration network for the whole of the US again, g_US. It has more nodes and edges than g_sub and consequently, its behaviour is richer and helps us illustrate better the concepts that we introduce in this section.\n\n5.6.1 Density\nThe network density is defined as the proportion of existing edges out of all the possible edges. In a network with \\(n\\) nodes, the total number of possible edges is $n\\times(n-1)$, i.e. the number of edges if each node was connected to all the other nodes. A density equal to \\(1\\) corresponds to a situation where \\(n\\times(n-1)\\) edges are present. A network with no edges at all would have density equal to \\(0\\). The line of code below tells us that the density of g_sub is approximately 0.33, meaning that about 33% of all the possible edges are present, or in other words, that there are migratory movements between almost a third of every pair of cities.\n\nedge_density(g_US, loops=FALSE)\n\n[1] 0.3283458\n\n\n\n\n5.6.2 Reciprocity\nThe reciprocity in a directed network is the proportion of reciprocated connections between nodes (i.e. number of pairs of nodes with edges in both directions) from all the existing edges.\n\nreciprocity(g_US)\n\n[1] 0.6067259\n\n\nFrom this result, we conclude that about 62% of the pairs of nodes that are connected have edges in both directions.\n\n\n5.6.3 Degree\nThe total degree of a node refers to the number of edges that emerge from or point at that node. The in-degree of a node in a directed network is the number of edges that point at it whereas the out-degree is the number of edges that emerge from it. The degree() functions, allows us to compute the degree of one or more nodes and allows us to specify if we are interested in the total degree, the in-degree or the out-degree.\n\n# Compute degree of the nodes given by v belonging to graph g_US, in this case the in-degree\ndeg <- degree(g_US, v=V(g_US), mode=\"in\")\n\n#Produces histogram of the frequency of nodes with a certain in-degree\nhist(deg, breaks = 30, main=\"Histogram of node in-degree\")\n\n\n\n\nAs we can see in the histogram, many cities receive immigrants from 60-70 different cities. Very few cities receive immigrants from 300 or above cities. We can check which is the city with the maximum in-degree.\n\nV(g_US)$name[degree(g_US, mode=\"in\")==max(degree(g_US, mode=\"in\"))]\n\n[1] \"Phoenix-Mesa-Chandler, AZ\"                   \n[2] \"Washington-Arlington-Alexandria, DC-VA-MD-WV\"\n\n\nWe actually obtain a tie between two: the MSA containing Phoenix in Arizona and the MSA containing Washington DC, which actually spans over four states. Their in-degree is 354 as we can see below.\n\ndegree(g_US, v=c(\"Phoenix-Mesa-Chandler, AZ\"), mode=\"in\")\n\nPhoenix-Mesa-Chandler, AZ \n                      354 \n\ndegree(g_US, v=c(\"Washington-Arlington-Alexandria, DC-VA-MD-WV\"), mode=\"in\")\n\nWashington-Arlington-Alexandria, DC-VA-MD-WV \n                                         354 \n\n\nNote that the fact that these two cities have the largest in-degree does not necessarily mean that they are the ones receiving the largest number of migrants.\n\n\n5.6.4 Distances\nShortest path between nodes, mean distance, diameter ,etc.\n\ndiameter(g_US)\n\n[1] 36\n\n\n\n\n5.6.5 Centrality\n\ncentr_degree(g_US, mode=\"in\", normalized=T)\n\n$res\n  [1] 198 108 330 108 283 275 127 182 116 175 104  66 251 285 187 104 344 198\n [19] 259 123 258 231 141 197 345 331 280 174  66 113 237 159 110  78 148  96\n [37] 230  63 304 261 220 132  44  73  55  76 300 169 166  95 115 333 115 121\n [55] 125 205 297  79 123  66 144  78 291 330 148  67 176 243 237 150 209 240\n [73] 311 354 105  96 282 230 277 134 228 101 279 304 276 222 110 204 316  73\n [91] 113 110 149 336  73 122 245  81 101 128  45 128 354  78 164 138 157   0\n[109]   0   0   0   0   0   0 172  33 163 276 148 132 271 167 104 103 105  26\n[127] 247 128  36 232 286  53 112 200 160  25 113 261  47  20   0  75  70 170\n[145]  61 194 202  74  81 113 170 164  72 105 221  65  92 255  92 271 216 192\n[163]  55 180 196 181  97 178 170 122 157 117  83 151 232 122 288 228 133 163\n[181] 187  70 227 186  71  68 282  64 127 185 222 240 144  54 276 156 274 157\n[199] 207 235 219 127  69  88 128 158 103 156  74 116 190 102 100 163  90 237\n[217] 104 319  93  67  68  77 179 146   0   0  76 136  68  56 131 166 115  69\n[235] 164  88 106  93 217  99  71 140 200  70  82  81 103  94 146 168 155 109\n[253]  94  95  62 113  98 103 152 123  62  98  87 165 187  76  66 117  69  93\n[271]  85 108  92  90 123 116  88 116 133 213  50  69 110 151  61  99 129  67\n[289]  72  74  65 151 161  63  83 154  93 129 117 107 132 115 145 150 155 125\n[307] 134 130 178  42  62 139 130 151 185 161  54  86  99 100  52  70  74 108\n[325]  75 119 140 128 115 121 134 139  43  52  78  47  68  69  90  87 143  80\n[343]  66  83  50 140 159  67  80 106 114  85 117 107  80 123  95  87  93  72\n[361]  86  81  81  89  63  83  93  54  66 124  55 134  60 119  75  77  81 107\n[379]  77  56  78 113  51  37  74 102  80 145 101  62  87  83 100  64  59  68\n[397]  99  67  41  76  51  46\n\n$centralization\n[1] 0.5544472\n\n$theoretical_max\n[1] 161202"
  },
  {
    "objectID": "network.html#sec-sec_communities",
    "href": "network.html#sec-sec_communities",
    "title": "5  Network Analysis",
    "section": "5.7 Communities",
    "text": "5.7 Communities"
  },
  {
    "objectID": "network.html#final-visualisation",
    "href": "network.html#final-visualisation",
    "title": "5  Network Analysis",
    "section": "5.8 Final visualisation",
    "text": "5.8 Final visualisation\n\n# V(g_US)$inflow <- rowSums(as.matrix(g_US[]))\n# # Set node size based on inflow of migrants:\n# V(g_US)$size <- 0.03*(V(g_US)$inflow)^0.1\n# # Set edge width based on weight:\n# E(g_US)$width <- E(g_US)$weight/1200\n# \n# #Join the data frame of nodes df_sub_nodes with the geographic information of the centroid of each MSA\n# df_spatial_nodes <- df_nodes %>% left_join(msa_us, by = c(\"name\" = \"NAME\")) %>% select(c(\"name\", \"lon_centroid\", \"lat_centroid\"))\n# \n# lo <- as.matrix(df_spatial_nodes[,2:3])\n# \n# plot(st_geometry(msa_us), border=adjustcolor(\"gray50\"))\n# plot(g_US, layout=lo, add = TRUE, rescale = FALSE, vertex.size=V(g_US)$size, edge.arrow.size=.1, edge.arrow.width=1., edge.curved=0.1, edge.width=E(g_US)$width, edge.color=adjustcolor(\"gray80\", alpha.f = .1), vertex.color=\"gold\", vertex.frame.color=adjustcolor(\"gray100\", alpha.f = .01),\n# vertex.label=\"\", vertex.label.color=\"black\",\n# vertex.label.cex=.0)"
  },
  {
    "objectID": "sentiment-analysis.html",
    "href": "sentiment-analysis.html",
    "title": "6  Sentiment Analysis",
    "section": "",
    "text": "In progress"
  },
  {
    "objectID": "topic-modelling.html",
    "href": "topic-modelling.html",
    "title": "7  Topic Modelling",
    "section": "",
    "text": "In progress"
  },
  {
    "objectID": "longitudinal-1.html",
    "href": "longitudinal-1.html",
    "title": "8  Modelling Time",
    "section": "",
    "text": "In progress"
  },
  {
    "objectID": "longitudinal-2.html",
    "href": "longitudinal-2.html",
    "title": "9  Assessing Interventions",
    "section": "",
    "text": "In progress"
  },
  {
    "objectID": "machine-learning.html",
    "href": "machine-learning.html",
    "title": "10  Machine Learning",
    "section": "",
    "text": "In progress"
  },
  {
    "objectID": "data-sets.html",
    "href": "data-sets.html",
    "title": "11  Data sets",
    "section": "",
    "text": "In progress"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abbott, Andrew, and Angela Tsay. 2000. “Sequence Analysis and\nOptimal Matching Methods in Sociology: Review and Prospect.”\nSociological Methods & Research 29 (1): 3–33.\n\n\nArribas-Bel, Dani, Mark Green, Francisco Rowe, and Alex Singleton. 2021.\n“Open Data Products-A Framework for Creating Valuable Analysis\nReady Data.” Journal of Geographical Systems 23 (4):\n497–514. https://doi.org/10.1007/s10109-021-00363-5.\n\n\nBackman, Mikaela, Esteban Lopez, and Francisco Rowe. 2020. “The\nOccupational Trajectories and Outcomes of Forced Migrants in Sweden.\nEntrepreneurship, Employment or Persistent Inactivity?” Small\nBusiness Economics 56 (3): 963–83. https://doi.org/10.1007/s11187-019-00312-z.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Information Science and Statistics. New York: Springer.\n\n\nCadwalladr, Carole, and Emma Graham-Harrison. 2018. “Revealed: 50\nMillion Facebook Profiles Harvested for Cambridge Analytica in Major\nData Breach.” The Guardian 17 (1): 22.\n\n\nCesare, Nina, Hedwig Lee, Tyler McCormick, Emma Spiro, and Emilio\nZagheni. 2018. “Promises and Pitfalls of Using Digital Traces for\nDemographic Research.” Demography 55 (5): 1979–99. https://doi.org/10.1007/s13524-018-0715-2.\n\n\nDolega, Les, Francisco Rowe, and Emma Branagan. 2021. “Going\nDigital? The Impact of Social Media Marketing on Retail Website Traffic,\nOrders and Sales.” Journal of Retailing and Consumer\nServices 60 (May): 102501. https://doi.org/10.1016/j.jretconser.2021.102501.\n\n\nFielding, A. J. 1992. “Migration and Social Mobility: South East\nEngland as an Escalator Region.” Regional Studies 26\n(1): 1–15. https://doi.org/10.1080/00343409212331346741.\n\n\nFranklin, Rachel. 2022. “Quantitative Methods II: Big\nTheory.” Progress in Human Geography 47 (1): 178–86. https://doi.org/10.1177/03091325221137334.\n\n\nGabadinho, Alexis, Gilbert Ritschard, Nicolas S. Müller, and Matthias\nStuder. 2011. “Analyzing and Visualizing State Sequences\ninRwithTraMineR.”\nJournal of Statistical Software 40 (4). https://doi.org/10.18637/jss.v040.i04.\n\n\nGabadinho, Alexis, Gilbert Ritschard, Matthias Studer, and Nicolas S\nMüller. 2009. “Mining Sequence Data in r with the TraMineR\nPackage: A User’s Guide.” Geneva: Department of Econometrics\nand Laboratory of Demography, University of Geneva.\n\n\nGonzález-Leonardo, Miguel, Niall Newsham, and Francisco Rowe. 2023.\n“Understanding Population Decline Trajectories in Spain Using\nSequence Analysis.” Geographical Analysis, January. https://doi.org/10.1111/gean.12357.\n\n\nGreen, Mark, Frances Darlington Pollock, and Francisco Rowe. 2021.\n“New Forms of Data and New Forms of Opportunities to Monitor and\nTackle a Pandemic.” In, 423–29. Springer International\nPublishing. https://doi.org/10.1007/978-3-030-70179-6_56.\n\n\nHilbert, Martin, and Priscila López. 2011. “The\nWorld’s Technological Capacity to Store, Communicate, and\nCompute Information.” Science 332 (6025): 60–65. https://doi.org/10.1126/science.1200970.\n\n\nJoint Research Centre. 2022. Data innovation in demography,\nmigration and human mobility. LU: European Commission. Publications\nOffice. https://doi.org/10.2760/027157.\n\n\nKashyap, Ridhi, R. Gordon Rinderknecht, Aliakbar Akbaritabar, Diego\nAlburez-Gutierrez, Sofia Gil-Clavel, André Grow, Jisu Kim, et al. 2022.\n“Digital and Computational Demography.” http://dx.doi.org/10.31235/osf.io/7bvpt.\n\n\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in\nData: An Introduction to Cluster Analysis. John Wiley & Sons.\n\n\nKitchin, Rob. 2014. “Big Data, New Epistemologies and Paradigm\nShifts.” Big Data & Society 1 (1): 205395171452848.\nhttps://doi.org/10.1177/2053951714528481.\n\n\nLazer, David, Alex Pentland, Lada Adamic, Sinan Aral, Albert-László\nBarabási, Devon Brewer, Nicholas Christakis, et al. 2009.\n“Computational Social Science.” Science 323\n(5915): 721–23. https://doi.org/10.1126/science.1167742.\n\n\nLazer, David, Alex Pentland, Duncan J. Watts, Sinan Aral, Susan Athey,\nNoshir Contractor, Deen Freelon, et al. 2020. “Computational\nSocial Science: Obstacles and Opportunities.” Science\n369 (6507): 1060–62. https://doi.org/10.1126/science.aaz8170.\n\n\nLiang, Hai, and King-wa Fu. 2015. “Testing Propositions Derived\nfrom Twitter Studies: Generalization and Replication in Computational\nSocial Science.” Edited by Zi-Ke Zhang. PLOS ONE 10 (8):\ne0134270. https://doi.org/10.1371/journal.pone.0134270.\n\n\nNewsham, Niall, and Francisco Rowe. 2022a. “Understanding the\nTrajectories of Population Decline Across Rural and Urban Europe: A\nSequence Analysis.” https://doi.org/10.48550/ARXIV.2203.09798.\n\n\n———. 2022b. “Understanding Trajectories of Population Decline\nAcross Rural and Urban Europe: A Sequence Analysis.”\nPopulation, Space and Place, December. https://doi.org/10.1002/psp.2630.\n\n\nPatias, Nikos, Francisco Rowe, and Dani Arribas-Bel. 2021.\n“Trajectories of Neighbourhood Inequality in Britain: Unpacking\nInter-Regional Socioeconomic Imbalances,\n1971-2011.” The Geographical Journal 188\n(2): 150–65. https://doi.org/10.1111/geoj.12420.\n\n\nPatias, Nikos, Francisco Rowe, Stefano Cavazzi, and Dani Arribas-Bel.\n2021. “Sustainable Urban Development Indicators in Great Britain\nfrom 2001 to 2016.” Landscape and Urban Planning 214\n(October): 104148. https://doi.org/10.1016/j.landurbplan.2021.104148.\n\n\nPetti, Samantha, and Abraham Flaxman. 2020. “Differential Privacy\nin the 2020 US Census: What Will It Do? Quantifying the Accuracy/Privacy\nTradeoff.” Gates Open Research 3 (April): 1722. https://doi.org/10.12688/gatesopenres.13089.2.\n\n\nRowe, Francisco. 2021. “Big Data and Human Geography.” http://dx.doi.org/10.31235/osf.io/phz3e.\n\n\n———. 2022. “Using Digital Footprint Data to Monitor Human Mobility\nand Support Rapid Humanitarian Responses.” Regional Studies,\nRegional Science 9 (1): 665–68. https://doi.org/10.1080/21681376.2022.2135458.\n\n\nRowe, Francisco, and Dani Arribas-Bel. 2022. “Spatial Modelling\nfor Data Scientists.” Open Science Framework, August. https://doi.org/10.17605/OSF.IO/8F6XR.\n\n\nRowe, Francisco, Jonathan Corcoran, and Martin Bell. 2016. “The\nReturns to Migration and Human Capital Accumulation Pathways:\nNon-Metropolitan Youth in the School-to-Work Transition.” The\nAnnals of Regional Science 59 (3): 819–45. https://doi.org/10.1007/s00168-016-0771-8.\n\n\nRowe, Francisco, Ruth Neville, and Miguel González-Leonardo. 2022.\n“Sensing Population Displacement from Ukraine Using Facebook Data:\nPotential Impacts and Settlement Areas.” http://dx.doi.org/10.31219/osf.io/7n6wm.\n\n\nSalmela-Aro, Katariina, Noona Kiuru, Jari-Erik Nurmi, and Mervi Eerola.\n2011. “Mapping Pathways to Adulthood Among Finnish University\nStudents: Sequences, Patterns, Variations in Family- and Work-Related\nRoles.” Advances in Life Course Research 16 (1): 25–41.\nhttps://doi.org/10.1016/j.alcr.2011.01.003.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2019. “Geographic Data\nScience.” Geographical Analysis 53 (1): 61–75. https://doi.org/10.1111/gean.12194.\n\n\nTatem, Andrew J. 2017. “WorldPop, Open Data for Spatial\nDemography.” Scientific Data 4 (1). https://doi.org/10.1038/sdata.2017.4.\n\n\nTurok, Ivan, and Vlad Mykhnenko. 2007. “The Trajectories of\nEuropean Cities, 19602005.” Cities 24 (3):\n165–82. https://doi.org/10.1016/j.cities.2007.01.007.\n\n\nZagheni, Emilio, and Ingmar Weber. 2015. “Demographic Research\nwith Non-Representative Internet Data.” Edited by Nikolaos\nAskitas and Professor Professor Klaus F. Zimmermann. International\nJournal of Manpower 36 (1): 13–25. https://doi.org/10.1108/ijm-12-2014-0261."
  }
]