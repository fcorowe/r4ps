[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Population Science",
    "section": "",
    "text": "Welcome\nThis is the website for “Population Science”. This is a course designed and delivered by Dr. Carmen Cabrera-Arnau, Prof. Francisco Rowe and Dr. Elisabetta Pietrostefani from the Geographic Data Science Lab at the University of Liverpool, United Kingdom. You will learn applied tools and cutting-edge analytical approaches to use digital footprint data to explore and understand human population trends and patterns, including supervised and unsupervised machine learning approaches, network analysis and causal inference methods.\nThe website is free to use and is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International. A compilation of this web course is hosted as a GitHub repository that you can access:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Population Science",
    "section": "Contact",
    "text": "Contact\n\nModule lead: Dr Carmen Cabrera-Arnau - c.cabrera-arnau [at] liverpool.ac.uk - Lecturer in Geographic Data Science\nProf Francisco Rowe - f.rowe-gonzalez [at] liverpool.ac.uk - Professor in Population Data Science\nDr Elisabetta Pietrostefani - e.pietrostefani [at] liverpool.ac.uk - Lecturer in Geographic Data Science\nFind us in Roxby Building, University of Liverpool, UK",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html#aims",
    "href": "intro.html#aims",
    "title": "1  Overview",
    "section": "",
    "text": "provide an introduction to fundamental theories of population science;\nintroduce students to novel data and approaches to understanding population dynamics and societal change; and,\nequip students with skills and experience to conduct population science using computational, data science approaches.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#learning-outcomes",
    "href": "intro.html#learning-outcomes",
    "title": "1  Overview",
    "section": "1.2 Learning Outcomes",
    "text": "1.2 Learning Outcomes\nBy the end of the module, students should be able to:\n\ngain an appreciation of relevant demographic theory to help interpret patterns of population change;\ndevelop an understanding of the types of demographic and social science methods that are essential for interpreting and analysing digital footprint data in the context of population dynamics;\ndevelop the ability to apply different methods to understand population dynamics and societal change;\ngain an appreciation of how population science approaches can produce relevant evidence to inform policy debates;\ndevelop critical awareness of modern demographic analysis and ethical considerations in the use of digital footprint data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#feedback",
    "href": "intro.html#feedback",
    "title": "1  Overview",
    "section": "1.3 Feedback",
    "text": "1.3 Feedback\nFormal assessment of two computational essays. Written assignment-specific feedback will be provided within three working weeks of the submission deadline. Comments will offer an understanding of the mark awarded and identify areas which can be considered for improvement in future assignments.\nVerbal face-to-face feedback. Immediate face-to-face feedback will be provided during computer, discussion and clinic sessions in interaction with staff. This will take place in all live sessions during the semester.\nOnline forum. Asynchronous written feedback will be provided via an online forum. Students are encouraged to contribute by asking and answering questions relating to the module content. Staff will monitor the forum Monday to Friday 9am-5pm, but it will be open to students to make contributions at all times. Response time will vary depending on the complexity of the question and staff availability.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#computational-environment",
    "href": "intro.html#computational-environment",
    "title": "1  Overview",
    "section": "1.4 Computational Environment",
    "text": "1.4 Computational Environment\nTo reproduce the code in the book, you need the following software packages:\n\nR-4.3.2\nRStudio 2023.12.0-369\nQuarto 1.4.543\nthe list of libraries in the next section\n\nTo check your version of:\n\nR and libraries run sessionInfo()\nRStudio click help on the menu bar and then About\nQuarto check the version file in the quarto folder on your computer.\n\nTo install and update:\n\nR, download the appropriate version from The Comprehensive R Archive Network (CRAN)\nRStudio, download the appropriate version from Posit\nQuarto, download the appropriate version from the Quarto website\n\n\n1.4.1 List of libraries\nThe list of libraries used in this book is provided below:\n\n“tidyverse”,\n“viridis”\n“viridisLite”\n“ggthemes”\n“patchwork”\n“showtext”\n“RColorBrewer”\n“lubridate”\n“tmap”\n“sjPlot”\n“sf”\n“sp”\n“kableExtra”\n“ggcorrplot”\n“plotrix”\n“cluster”\n“factoextra”\n“igraph”\n“stringr”\n“rpart”\n“rpart.plot”\n“ggplot2”\n“Metrics”\n“caret”\n“randomForest”\n“ranger”\n“wpgpDownloadR”\n“devtools”\n“ggseqplot”\n“tidytext”\n“tm”\n“textdata”\n“topicmodels”\n“RedditExtractoR”\n“stm”\n“dygraphs”\n“plotly”\n“ggpmisc”\n“ggformula”\n“ggimage”\n“modelsummary”\n“gtools”\n“webshot”\n“gridExtra”\n“broom”\n“rtweet”\n“dplyr”\n“ggraph”\n“tidygraph”\n“ggspatial”\n\nYou need to ensure you have installed the list of libraries used in this book, running the following code:\n\n# package names\npackages &lt;- c( \"tidyverse\", \"viridis\", \"viridisLite\", \"ggthemes\", \"patchwork\", \"showtext\", \"RColorBrewer\", \"lubridate\", \"tmap\", \"sjPlot\", \"sf\", \"sp\", \"kableExtra\", \"ggcorrplot\", \"plotrix\", \"cluster\", \"factoextra\", \"igraph\", \"stringr\", \"rpart\", \"rpart.plot\", \"ggplot2\", \"Metrics\", \"caret\", \"randomForest\", \"ranger\", \"devtools\", \"vader\", \"wpgpDownloadR\", \"ggseqplot\", \"tidytext\", \"tm\", \"textdata\", \"topicmodels\", \"RedditExtractoR\", \"stm\", \"dygraphs\", \"plotly\", \"ggpmisc\", \"ggformula\", \"ggimage\", \"modelsummary\", \"gtools\", \"gridExtra\", \"broom\", \"rtweet\", \"webshot\", \"ggraph\", \"tidygraph\", \"ggspatial\")\n\n# install packages not yet installed\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n# packages loading\ninvisible(lapply(packages, library, character.only = TRUE))",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#assessment",
    "href": "intro.html#assessment",
    "title": "1  Overview",
    "section": "1.5 Assessment",
    "text": "1.5 Assessment\nThe final module mark is composed of the two computational essays. Together they are designed to cover the materials introduced in the entirety of content covered during the semester. A computational essay is an essay whose narrative is supported by code and computational results that are included in the essay itself. Each teaching week, you will be required to address a set of questions relating to the module content covered in that week, and to use the material that you will produce for this purpose to build your computational essay.\nAssignment 1 (50%) refers to the set of questions at the end of Chapter 2, Chapter 3, Chapter 4 and Chapter 5. You are required to use your responses to build your computational essay. Each chapter provides more specific guidance of the tasks and discussion that you are required to consider in your assignment.\nAssignment 2 (50%) refers to the set of questions at the end of Chapter 6, Chapter 7, Chapter 8, Chapter 9 and Chapter 10. You are required to use your responses to build your computational essay. Each chapter provides more specific guidance of the tasks and discussion that you are required to consider in your assignment.\n\n1.5.1 Format Requirements\nBoth assignments will have the same requirements:\n\nMaximum word count: 2,000 words, excluding figures and references.\nUp to three maps, plot or figures (a figure may include more than one map and/or plot and will only count as one but needs to be integrated in the figure)\nUp to two tables.\n\nAssignments need to be prepared in “Quarto Document” format (i.e. qmd extension) and then converted into a self-contained HTML file that will then be submitted via Turnitin. The document should only display content that will be assessed. Intermediate steps do not need to be displayed. Messages resulting from loading packages, attaching data frames, or similar messages do not need to be included as output code. Useful resources to customise your R notebook can be found on Quarto’s website.\nTwo Quarto Document templates will be available via the module Canvas site.\nSubmission is electronic only via Turnitin on Canvas.\n\n1.5.1.1 Marking criteria\nThe Standard Environmental Sciences School marking criteria apply, with a stronger emphasis on evidencing the use of regression models, critical analysis of results and presentation standards. In addition to these general criteria, the code and outputs (i.e. tables, maps and plots) contained within the notebook submitted for assessment will be assessed according to the extent of documentation and evidence of expertise in changing and extending the code options illustrated in each chapter. Specifically, the following criteria will be applied:\n\n0-15: no documentation and use of default options.\n16-39: little documentation and use of default options.\n40-49: some documentation, and use of default options.\n50-59: extensive documentation, and edit of some of the options provided in the notebook (e.g. change north arrow location).\n60-69: extensive well organised and easy to read documentation, and evidence of understanding of options provided in the code (e.g. tweaking existing options).\n70-79: all above, plus clear evidence of code design skills (e.g. customising graphics, combining plots (or tables) into a single output, adding clear axis labels and variable names on graphic outputs, etc.).\n80-100: all as above, plus code containing novel contributions that extend/improve the functionality the code was provided with (e.g. comparative model assessments, novel methods to perform the task, etc.).\n\n\n\n\n\nRowe, Francisco. 2021. “Big Data and Human Geography.” http://dx.doi.org/10.31235/osf.io/phz3e.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "intro-population-science.html#introduction",
    "href": "intro-population-science.html#introduction",
    "title": "2  Introducing Population Science",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nPopulation science sits at the intersection between population studies and data science. As the general field of population studies, population science seeks to quantitatively understand human populations, including the three key demographic processes of population change, namely fertility, mortality and migration. It seeks to understand the size, structure, temporal changes and spatial distribution of populations, and the drivers and impacts that underpin their variations and regularities. It considers the ways in which structural social, economic, political and environmental factors shape population trends. What is unique about population science is that it seeks to leverage on the ongoing digital revolution characterised by technological advances in computer processing, digitalised information storage capacity and digital connectivity (Hilbert and López 2011).\nThe digital revolution ushered in the 1990s has unleashed a data revolution. Technological advances in computational power, storage and digital network platforms have enabled the emergence of “Big Data” or “digital footprint data”. These technological developments have enabled the production, processing, analysis and storage of large volumes of digital data. Analysing 1986-2007 data, Hilbert and López (2011) estimated that the world has already passed the point at which more data were being collected than could be physically stored. They estimated that the global general-purpose computing capacity grew at an annual rate of 58 percent between 1986 and 2007, exceeding that of global storage capacity (23 percent). We can now digitally captured and generated data that previously could not easily be recorded and stored.\nThe unprecedented amount of information that we can now capture through digital technology offers unique opportunities to advance our understanding of micro human behaviour (e.g. individual-level decision making, preferences and choices) and macro population processes (e.g. structural population processes and trends). Digital footprint data offer a continuous flow of information to capture human population dynamics at unprecedentedly fine spatial and temporal resolution in real or near real-time comprising entire social systems. We can capture and study micro individual behaviours such as online time use, purchasing behaviour, visitation patterns and public opinion from data sources, such as mobile phones, social media and retail website platforms. These behaviours can also be aggregated to shed light into macro structural processes and trends, such as urban mobility, consumer demand, transport usage, population ageing and decline. Fundamentally digital footprint data thus have the potential to become a key pillar informing and supporting decision making. They can inform business to increase sales revenue, football clubs to improve team performance, and governments to tackle major societal issues, such as the COVID-19 pandemic and global warming, influencing policy, practice and governance structures.\nYet, the use of digital footprint data also poses major conceptual, methodological and ethical challenges (Rowe 2021). It is these challenges that motivated this module. Digital footprint data are a by-product of an administrative process or service, and it is not purposely collected for research. Turning raw digital footprint data into actionable, usable information thus requires a unique combination of technical computational expertise and subject-specific knowledge. Traditionally university programmes have tended to focus on providing technical training, such as statistics or on specific knowledge subjects. But they are rarely found as a single coherent package. This module aims to fills this gap by offering training in the use of digital footprint data, and sophisticated methodological approaches (including machine learning, artificial intelligence, network science and statistical methods) to tackle important population issues, such as population segmenting, decline and mobility. Access to digital footprint data are highly variable; hence, we do not focus on this here. However, we encourage users of this book to read a report put together by the Joint Research Centre (2022) identifying and discussing key data sources focusing population processes.\nThe name of this module Population Science reflects the inclusive and interdisciplinary perspective we hope to capture. The data revolution has led to the emergence of a range of sub-disciplines, seeking to leverage on the use of digital footprint data to study human behaviour and population processes. These emerging sub-disciplines have tended to focus on discipline-specific issues such as digital demography (Kashyap et al. 2022), or particular methodological approaches, such as the use of networks principles in computational social sciences (Lazer et al. 2009). Population science seeks to integrate these perspectives and provide a fertile framework for critique, collaboration and co-creation across these emerging areas of scholarship in the study of human population. And, of course, take a spatial perspective adopting geographic data science approaches (Singleton and Arribas-Bel 2019).\nSpecifically, this chapter aims to discuss key opportunities and challenges of digital footprint data to analyse human population dynamics. We place a particular focus on the challenges relating to privacy, bias and privacy issues. The chapter starts by defining digital footprint data before discussing the key opportunities offered by these data and the challenges they pose."
  },
  {
    "objectID": "intro-population-science.html#defining-digital-footprint-data",
    "href": "intro-population-science.html#defining-digital-footprint-data",
    "title": "2  Introducing Population Science",
    "section": "2.2 Defining digital footprint data",
    "text": "2.2 Defining digital footprint data\nWe define digital footprint data as:\n\nthe data recorded by digital technology resulting from the interactions of people among themselves or with their social and physical environment, and they can take the form of images, video, text and numbers.\n\nData footprint data are distinctive features in their volume, velocity, variety, exhaustiveness, resolution, relational nature and flexibility (Kitchin 2014). They can take different forms. Traditional data used to be mostly numeric. Digital footprint data has facilitated the collection, storage and analysis of text (e.g. Twitter posts), image (e.g. Instagram photos) and video (e.g. CCTV footage) data.\nMultiple digital systems contribute to the storage and generation of digital footprint data. Kitchin (2014) identified three broad systems directed, automated and volunteered systems. Directed systems comprise digital administrative systems operated by a human recording data on places or people e.g. immigration control, biometric scanning and health records. Automated systems involve digital systems which automatically and autonomously record and process data with little human intervention e.g. mobile phone applications, electronic smartcard ticketing, energy smart meter and traffic sensors. Volunteered systems involve digital spaces in which humans contribute data through interactions on social media platforms (e.g. Twitter and Facebook) or crowdsourcing (e.g. OpenStreetMap and Wikipedia).\n\n\n\nDigital footprint systems"
  },
  {
    "objectID": "intro-population-science.html#opportunities-of-digital-footprint-data",
    "href": "intro-population-science.html#opportunities-of-digital-footprint-data",
    "title": "2  Introducing Population Science",
    "section": "2.3 Opportunities of digital footprint data",
    "text": "2.3 Opportunities of digital footprint data\nDigital footprint data offer unique opportunities for the analysis of human population patterns. As Rowe (2021) argues, digital footprint data offer three key promises in relation to traditional data sources, such as surveys and censuses. They generally provide greater spatio-temporal granularity, wider coverage and timeliness.\nDigital footprint data offer high geographic and temporal granularity. Most digital footprint data are time-stamped and geographically referenced with high precision. Digital technology, such as mobile phone and geographical positioning systems enables the generation of a continuous steams of time-stamped location data. Such information thus provides an opportunity to trace and enhance our understanding human populations over highly granular spatial scales and time intervals, going beyond the static representation afforded by most traditional data sources. Spatial human interactions, and how people use and are influenced by their environment, can be analysed in a temporally dynamic way.\nDigital footprint data provide extensive coverage. Contrasting to traditional random sampling, digital footprint data promise information on universal or near-universal population or geographical systems. Social media platforms, such as Twitter generate data to capture the entire universe of Twitter users. Satellite technology produces imagery snapshots to composite a representation of the Earth. Electronic smartcard ticketing systems produce information to capture the population of users in the system. Because the information is typically consistently collected and storage, the coverage of digital footprint data offer the potential to study human behaviour of entire systems at a global scale based on harmonised definitions, which is rarely possible using traditional data sources.\nDigital footprint data are generated in real-time. Unlike traditional systems of data collection and release, digital footprint data can be streamed continuously in real- or near real-time. Commercial transactions are generally recorded on bank ledgers as bank card payments occur at retail shops. Individual mobile phone’s location are captured as applications ping cellular antennas. Such information offer an opportunity to monitor and response to rapidly evolving situations, such as the COVID-19 pandemic (Green, Pollock, and Rowe 2021), natural disasters (Rowe 2022) and conflicts (Rowe, Neville, and González-Leonardo 2022).\nWe also loudly and clearly argue that while digital footprint data should be seen as a key asset to support government and business decision making processes, they should not be considered at the expenses of traditional data sources. Digital footprint data and traditional data sources should be used to complement each another. As indicated earlier, digital footprint data are the by-product of administrative processes or services. They were not designed with the aim of doing research. They require considerable work of data re-engineering to re-purpose them and turn them into an analysis-ready data product that can be used for further analysis (Arribas-Bel et al. 2021). Yet, as we will discuss below significant challenges remain. As the saying goes “all data are dirty, but some data are useful”. This quote used in the data science community to convey the idea that data are often imperfect, but they can still be used to gain valuable insights. Our message is that digital footprint data and traditional data sources should be triangulated to leverage on their strengths and mitigate their weaknesses."
  },
  {
    "objectID": "intro-population-science.html#challenges-of-digital-footprint-data",
    "href": "intro-population-science.html#challenges-of-digital-footprint-data",
    "title": "2  Introducing Population Science",
    "section": "2.4 Challenges of digital footprint data",
    "text": "2.4 Challenges of digital footprint data\nDigital footprint data also impose key conceptual, methodological and ethical challenges. In this section, we provide a brief explanation of challenges in these areas, focusing particularly on issues around biases, privacy, ethics and new methods. We focus on these issues because they are of practical importance and probably of most interest to the readers of this book. Excellent discussions have been written and, if you are interested in learning more about the challenges relating to digital footprint data, we recommend Kitchin (2014), Cesare et al. (2018), Lazer et al. (2020) and Rowe (2021).\n\n2.4.1 Conceptual challenges\nConceptually, the emergence of digital footprint data has led to the rethinking and questioning of existing theoretical social science approaches (Franklin 2022). On the one hand, digital footprint data provide an opportunity to explore existing theories or hypotheses through different lens and test the consistency of existing beliefs. For example, economics theories discuss the existence of temporal and spatial equilibrium. Resulting hypotheses are generally tested through mathematical theoretical models or empirical analyses relying on temporally static data. The existence of equilibrium has thus remained hard to assess. Digital footprint data provide an opportunity to empirically test temporal and spatial equilibrium ideas based on suitable temporally dynamic data. They can enable the testing of cause and impact hypotheses, rather than only focusing on static associations.\nOn the other hand, digital footprint data sparked new questions. Digital footprint data provide data on previously unmeasured activities. Data now capture activities that were previously difficult to quantify, such as personal communications, social networks, search and information gathering, and location data. These data offer an opportunity to develop new questions expanding existing theories by looking inside the “black box” of households, organisations and markets. They may also open the door to developing entirely new questions such as the role of digital technology in shaping human behaviour, and the role of artificial intelligence on productivity and financial markets.\n\n\n2.4.2 Methodological challenges\nMethodologically, the need for a wide and new set of digital skills and expertise to handle, store and analyse large volumes of data is a key challenge. As indicated earlier, digital footprint data are not created for research purposes. They need to be reengineered for research. Large streams of digital footprint data cannot be stored on local memory. They can rarely be read as a single unit on a local computer and may involve performing the same task numerous times in regular basis, requiring therefore large storage, computational capacity and computer science expertise. The manipulation and storage of digital footprint data often require technical expertise in data management systems, such as SQL, Google Cloud Storage and Amazon S3, as well as in efficient computing involving expertise in distributed computing systems and parallelisation frameworks. The analysis and modelling of digital footprint data may entail competencies in the application of machine learning and artificial intelligence. While these competencies generally form part of a computer science programme, they are rarely taught in an integrated framework focusing on addressing societal or business challenges relating to human populations - where the key focus is their application.\nAn additional methodological challenge is the presence of biases in digital footprint data. Digital footprint data are representative of a specific segment of the population but little is known which segments and how their representation varies across data sets and digital technology. Digital footprint data may comprise multiple sources of biases. They may reflect differences in the use of a digital device (e.g. mobile phone) and/or a piece of digital technology (e.g. a mobile phone application) Schlosser et al. (2021) . They may also reflect differences in frequency in the use of digital technology (e.g. number of times an individual uses a mobile phone application) - and this frequency may in turn reflect differences in algorithmic decisions embedded in digital platforms, such as suggesting content based on prior interactions to increase engagement with a given mobile phone application. Some work has been done on assessing biases as well as developing approaches to mitigate their influence Ribeiro, Benevenuto, and Zagheni (2020).\n\n\n2.4.3 Ethical challenges\nPrivacy represents a major ethical challenge. Digital footprint data are highly sensitive, and hence, anonymisation and disclosure control are required. Individual records must be anonymised so they are not identifiable. The high degree of granularity and personal information of these records may and have been used in ethically questionable ways; for example, Cambridge Analytica used information of Facebook users to segment the population and target politically motivated content (Cadwalladr and Graham-Harrison 2018). Anonymising information, however, imposes a key challenge as there is a trade-off between accuracy and privacy (Petti and Flaxman 2020). Anonymisation may reduce the usability of data. The greater the degree of privacy, the lower is the degree of accuracy of the resulting data and vice versa. Identifying the optimal point balancing the privacy-accuracy trade-off is the key challenge. If doing incorrectly, we could end up drawing inferences that do not reflect the actual population processes displayed in the data, or have artificially been encoded in the data through noise or reshuffling. The application of data differential privacy to the US census provides a recent good example of this challenge. An emblematic case is New York’s Liberty Island which has no resident population, but official US census reported 48 residents which was the result of adding statistical noise to the data, in order to enhance privacy."
  },
  {
    "objectID": "intro-population-science.html#conclusion",
    "href": "intro-population-science.html#conclusion",
    "title": "2  Introducing Population Science",
    "section": "2.5 Conclusion",
    "text": "2.5 Conclusion\nDigital footprint data present unique oppotunites to enhance our understanding of population processes and support individual, business and government decisions to improve targeted processes and outcomes. Businesses have used digital footprint data to segment their consumer populations and improve their targeting of marketing content, products and ultimately increase sales and revenue (Dolega, Rowe, and Branagan 2021). Governments and health care institutions, particularly during the COVID-19 have leverage digital footprint data to monitor the spread of the pandemic and develop appropriate mitigation responses (Green, Pollock, and Rowe 2021). However, the use of digital footprint data poses major conceptual, methodological and ethical challenges - which need to be overcome to unleash their full potential. The aim of this book is to address of the key methodological challenges. In particular, this book seeks to provide applied training on the practical application of commonly used machine learning and artificial intelligence approaches to leverage on digital footprint data in the understanding of human behaviour and population processes.\n\n\n\n\nArribas-Bel, Dani, Mark Green, Francisco Rowe, and Alex Singleton. 2021. “Open Data Products-A Framework for Creating Valuable Analysis Ready Data.” Journal of Geographical Systems 23 (4): 497–514. https://doi.org/10.1007/s10109-021-00363-5.\n\n\nCadwalladr, Carole, and Emma Graham-Harrison. 2018. “Revealed: 50 Million Facebook Profiles Harvested for Cambridge Analytica in Major Data Breach.” The Guardian 17 (1): 22.\n\n\nCesare, Nina, Hedwig Lee, Tyler McCormick, Emma Spiro, and Emilio Zagheni. 2018. “Promises and Pitfalls of Using Digital Traces for Demographic Research.” Demography 55 (5): 1979–99. https://doi.org/10.1007/s13524-018-0715-2.\n\n\nDolega, Les, Francisco Rowe, and Emma Branagan. 2021. “Going Digital? The Impact of Social Media Marketing on Retail Website Traffic, Orders and Sales.” Journal of Retailing and Consumer Services 60 (May): 102501. https://doi.org/10.1016/j.jretconser.2021.102501.\n\n\nFranklin, Rachel. 2022. “Quantitative Methods II: Big Theory.” Progress in Human Geography 47 (1): 178–86. https://doi.org/10.1177/03091325221137334.\n\n\nGreen, Mark, Frances Darlington Pollock, and Francisco Rowe. 2021. “New Forms of Data and New Forms of Opportunities to Monitor and Tackle a Pandemic.” In, 423–29. Springer International Publishing. https://doi.org/10.1007/978-3-030-70179-6_56.\n\n\nHilbert, Martin, and Priscila López. 2011. “The World’s Technological Capacity to Store, Communicate, and Compute Information.” Science 332 (6025): 60–65. https://doi.org/10.1126/science.1200970.\n\n\nJoint Research Centre. 2022. Data innovation in demography, migration and human mobility. LU: European Commission. Publications Office. https://doi.org/10.2760/027157.\n\n\nKashyap, Ridhi, R. Gordon Rinderknecht, Aliakbar Akbaritabar, Diego Alburez-Gutierrez, Sofia Gil-Clavel, André Grow, Jisu Kim, et al. 2022. “Digital and Computational Demography.” http://dx.doi.org/10.31235/osf.io/7bvpt.\n\n\nKitchin, Rob. 2014. “Big Data, New Epistemologies and Paradigm Shifts.” Big Data & Society 1 (1): 205395171452848. https://doi.org/10.1177/2053951714528481.\n\n\nLazer, David, Alex Pentland, Lada Adamic, Sinan Aral, Albert-László Barabási, Devon Brewer, Nicholas Christakis, et al. 2009. “Computational Social Science.” Science 323 (5915): 721–23. https://doi.org/10.1126/science.1167742.\n\n\nLazer, David, Alex Pentland, Duncan J. Watts, Sinan Aral, Susan Athey, Noshir Contractor, Deen Freelon, et al. 2020. “Computational Social Science: Obstacles and Opportunities.” Science 369 (6507): 1060–62. https://doi.org/10.1126/science.aaz8170.\n\n\nLiang, Hai, and King-wa Fu. 2015. “Testing Propositions Derived from Twitter Studies: Generalization and Replication in Computational Social Science.” Edited by Zi-Ke Zhang. PLOS ONE 10 (8): e0134270. https://doi.org/10.1371/journal.pone.0134270.\n\n\nPetti, Samantha, and Abraham Flaxman. 2020. “Differential Privacy in the 2020 US Census: What Will It Do? Quantifying the Accuracy/Privacy Tradeoff.” Gates Open Research 3 (April): 1722. https://doi.org/10.12688/gatesopenres.13089.2.\n\n\nRibeiro, Filipe N., Fabrício Benevenuto, and Emilio Zagheni. 2020. “How Biased Is the Population of Facebook Users? Comparing the Demographics of Facebook Users with Census Data to Generate Correction Factors.” 12th ACM Conference on Web Science, July. https://doi.org/10.1145/3394231.3397923.\n\n\nRowe, Francisco. 2021. “Big Data and Human Geography.” http://dx.doi.org/10.31235/osf.io/phz3e.\n\n\n———. 2022. “Using Digital Footprint Data to Monitor Human Mobility and Support Rapid Humanitarian Responses.” Regional Studies, Regional Science 9 (1): 665–68. https://doi.org/10.1080/21681376.2022.2135458.\n\n\nRowe, Francisco, Ruth Neville, and Miguel González-Leonardo. 2022. “Sensing Population Displacement from Ukraine Using Facebook Data: Potential Impacts and Settlement Areas.” http://dx.doi.org/10.31219/osf.io/7n6wm.\n\n\nSchlosser, Frank, Vedran Sekara, Dirk Brockmann, and Manuel Garcia-Herranz. 2021. “Biases in Human Mobility Data Impact Epidemic Modeling.” https://doi.org/10.48550/ARXIV.2112.12521.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2019. “Geographic Data Science.” Geographical Analysis 53 (1): 61–75. https://doi.org/10.1111/gean.12194.\n\n\nZagheni, Emilio, and Ingmar Weber. 2015. “Demographic Research with Non-Representative Internet Data.” Edited by Nikolaos Askitas and Professor Professor Klaus F. Zimmermann. International Journal of Manpower 36 (1): 13–25. https://doi.org/10.1108/ijm-12-2014-0261."
  },
  {
    "objectID": "geodemographics.html#sec-sec31",
    "href": "geodemographics.html#sec-sec31",
    "title": "3  Geodemographics",
    "section": "3.1 Dependencies",
    "text": "3.1 Dependencies\nThis chapter uses the libraries below. Ensure they are installed on your machine, then execute the following code chunk to load them:\n\n#Support for simple features, a standardised way to encode spatial vector data\nlibrary(sf)\n#Data manipulation\nlibrary(dplyr)\n#A system for creating graphics\nlibrary(ggplot2)\n#Easy viisualisation of a correlation matrix using ggplot2\nlibrary(ggcorrplot)\n#Color maps designed to improve graph readability\nlibrary(viridis)\n#Alternative way of plotting, useful for radial plots\nlibrary(plotrix)\n#Methods for cluster analysis\nlibrary(cluster)\n#Thematic maps can be generated with great flexibility\nlibrary(tmap)\n#Provides some easy-to-use functions to extract and visualize the output of multivariate data analyses\nlibrary(factoextra)\n\n\n #Obtain the working directory, where we will save the data directory\ngetwd()\n\n[1] \"/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps\""
  },
  {
    "objectID": "geodemographics.html#sec-sec32",
    "href": "geodemographics.html#sec-sec32",
    "title": "3  Geodemographics",
    "section": "3.2 Data",
    "text": "3.2 Data\n\n3.2.1 Land use data for Greater Manchester\nLand use patterns offer insights into the demographic characteristics and needs of a population within a specific area. How residents use their land reflects their preferences, demands, and activities. The extent of residential land indicates the size, density, and distribution of the population, as well as housing preferences and household sizes. The extent of commercial and industrial land use relates to economic activities, employment opportunities, and the overall economic vitality of a region. Agricultural land use might indicate the food demands and production capabilities of the population. Additionally, land designated for public services, education, healthcare, and recreation showcases the amenities and social infrastructure necessary to support the population’s needs and quality of life. Classifying regions by their similarity in their land use can therefore provide valuable insights into the lifestyle, economic status, and social requirements of the residents in a given area.\nIn this Chapter we will be looking at land use data provided by What do ‘left behind’ areas look like over time?, a project created by the Geographic Data Science Lab at the University of Liverpool to host open reproducible code and resulting data for the Briefing: “What do ‘left behind’ areas look like over time?”. The land use data is based on digital data from the CORINE Land Cover (CLC) dataset. This well-known dataset is compiled and maintained by the European Environment Agency (EEA) and derived from satellite imagery and aerial photographs, using remote sensing technology and image analysis techniques.\nIn particular, we use the file manchester_land_cover_2011.gpkg based on the file lsoa_land_cover.csv available here, which contains land use data for each Lower Layer Super Output Area (LSOA) in Greater Manchester.\nLSOAs are geographic hierarchies designed to improve the reporting of small area statistics in England and Wales. LSOAs are built from groups of contiguous Output Areas (OAs) and have been automatically generated to be as consistent as possible in population size, with a minimum population of 1,000. For this reason, their spatial extent varies depending on how densely populated a region is. The average population of an LSOA in London in 2010 was 1,722.\n\n\n3.2.2 Import the data\nIn the code chunk below we load the dataset described above, manchester_land_cover_2011.gpkg as a simple feature object and call it sf_LSOA. We will be generating some maps to show the geographical distribution of our data and results. To do this, we need the data that defines the geographical boundaries of the LSOAs. This data is included in the sf_LSOA variable, in the column called geom.\n\n# The raw data can be obtained from link below, but it has been cleaned by Carmen Cabrera-Arnau for this chapter\n# https://github.com/GDSL-UL/APPG-LBA/blob/main/data/lsoa_land_cover.csv\n\nsf_LSOA &lt;- st_read(\"./data/geodemographics/manchester_land_cover_2011.gpkg\")\n\nReading layer `manchester_land_cover_2011' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/geodemographics/manchester_land_cover_2011.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1673 features and 44 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 351662.3 ymin: 381166 xmax: 406087.2 ymax: 421037.7\nProjected CRS: OSGB36 / British National Grid"
  },
  {
    "objectID": "geodemographics.html#sec-sec34",
    "href": "geodemographics.html#sec-sec34",
    "title": "3  Geodemographics",
    "section": "3.3 Preparing the data for GDC",
    "text": "3.3 Preparing the data for GDC\n\n3.3.1 Choice of geographic units\nNormally, GDCs involve the analysis of aggregated data into geographic units. Very small geographic units of data aggregation can provide more detailed results, but if the counts are too low, this could lead to re-identification issues.\nAs mentioned above, the data for this chapter is aggregated into LSOAs. The size of the LSOAs is small enough to produce detailed results and is also a convenient choice, since it is broadly used in the UK Census and other official data-reporting exercises.\nWe can visualise the LSOAs within Greater Manchester simply by plotting the geometry column of st_LSOA, which can be selected with the function st_geometry(). More sophisticated ways of visualising geographical data are available in R as we will see in forthcoming sections.\n\nplot(st_geometry(sf_LSOA), border=adjustcolor(\"gray20\", alpha.f=0.4), lwd=0.6)\n\n\n\n\n\n\n3.3.2 Variables of interest\nAny classification task must be based on certain criteria that determines how elements are grouped into classes. For GDC, these criteria are characteristics of the spatial units under study. In this case, we have prepared the file manchester_land_cover_2011.gpkg to contain some interesting land use data corresponding to each LSOA. The data frame sf_LSOA contains this data and we can visualise its first few lines by using the function head() and first four columns by subsetting the simple feature object:\n\nhead(sf_LSOA[,1:4])\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 368766.7 ymin: 411048.3 xmax: 372572.6 ymax: 414726.9\nProjected CRS: OSGB36 / British National Grid\n   LSOA11CD    LSOA11NM Sea.and.ocean..523...2012.\n1 E01004766 Bolton 005A                          0\n2 E01004767 Bolton 005B                          0\n3 E01004768 Bolton 001A                          0\n4 E01004769 Bolton 003A                          0\n5 E01004770 Bolton 003B                          0\n6 E01004771 Bolton 003C                          0\n  Continuous.urban.fabric..111...2012.                           geom\n1                                    0 MULTIPOLYGON (((371567 4119...\n2                                    0 MULTIPOLYGON (((371807.5 41...\n3                                    0 MULTIPOLYGON (((370197.2 41...\n4                                    0 MULTIPOLYGON (((371924.3 41...\n5                                    0 MULTIPOLYGON (((372572.6 41...\n6                                    0 MULTIPOLYGON (((371554.4 41...\n\n\nAs we can see, each row contains information about an LSOA and each column (starting from the third column) represents a demographic characteristic of the LSOA and the people living there. With the function names(), we can get the names of the columns in sf_LSOA\n\nnames(sf_LSOA)\n\n [1] \"LSOA11CD\"                                                                                           \n [2] \"LSOA11NM\"                                                                                           \n [3] \"Sea.and.ocean..523...2012.\"                                                                         \n [4] \"Continuous.urban.fabric..111...2012.\"                                                               \n [5] \"Discontinuous.urban.fabric..112...2012.\"                                                            \n [6] \"Industrial.or.commercial.units..121...2012.\"                                                        \n [7] \"Sport.and.leisure.facilities..142...2012.\"                                                          \n [8] \"Non.irrigated.arable.land..211...2012.\"                                                             \n [9] \"Mineral.extraction.sites..131...2012.\"                                                              \n[10] \"Pastures..231...2012.\"                                                                              \n[11] \"Natural.grasslands..321...2012.\"                                                                    \n[12] \"Moors.and.heathland..322...2012.\"                                                                   \n[13] \"Peat.bogs..412...2012.\"                                                                             \n[14] \"Transitional.woodland.shrub..324...2012.\"                                                           \n[15] \"Water.bodies..512...2012.\"                                                                          \n[16] \"Sparsely.vegetated.areas..333...2012.\"                                                              \n[17] \"Coniferous.forest..312...2012.\"                                                                     \n[18] \"Mixed.forest..313...2012.\"                                                                          \n[19] \"Broad.leaved.forest..311...2012.\"                                                                   \n[20] \"Construction.sites..133...2012.\"                                                                    \n[21] \"Green.urban.areas..141...2012.\"                                                                     \n[22] \"Port.areas..123...2012.\"                                                                            \n[23] \"Land.principally.occupied.by.agriculture..with.significant.areas.of.natural.vegetation..243...2012.\"\n[24] \"Water.courses..511...2012.\"                                                                         \n[25] \"Road.and.rail.networks.and.associated.land..122...2012.\"                                            \n[26] \"Dump.sites..132...2012.\"                                                                            \n[27] \"Airports..124...2012.\"                                                                              \n[28] \"Intertidal.flats..423...2012.\"                                                                      \n[29] \"Estuaries..522...2012.\"                                                                             \n[30] \"Beaches..dunes..sands..331...2012.\"                                                                 \n[31] \"Inland.marshes..411...2012.\"                                                                        \n[32] \"Salt.marshes..421...2012.\"                                                                          \n[33] \"Complex.cultivation.patterns..242...2012.\"                                                          \n[34] \"Coastal.lagoons..521...2012.\"                                                                       \n[35] \"Bare.rocks..332...2012.\"                                                                            \n[36] \"Fruit.trees.and.berry.plantations..222...2012.\"                                                     \n[37] \"Burnt.areas..334...2012.\"                                                                           \n[38] \"Agro.forestry.areas..244...2012.\"                                                                   \n[39] \"LSOA11NMW\"                                                                                          \n[40] \"BNG_E\"                                                                                              \n[41] \"BNG_N\"                                                                                              \n[42] \"LONG\"                                                                                               \n[43] \"LAT\"                                                                                                \n[44] \"GlobalID\"                                                                                           \n[45] \"geom\"                                                                                               \n\n\nWe can explore the summary statistics for each of the variables with the summary() function applied on the variable of interest. For example, to obtain the summary statistics for the proportion of land dedicated to discontinuous urban fabric in 20212 Discontinuous.urban.fabric..112...2012, we can run the code below:\n\nsummary(sf_LSOA$Discontinuous.urban.fabric..112...2012.)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.4445  0.7474  0.6823  0.9958  1.0000 \n\n\nThis tells us that the mean or average proportion of land dedicated to discontinuous urban fabric in LSOAs within Greater Manchester is 0.6823. It also tells us that 1 is the proportion of land dedicated to discontinuous urban fabric in the LSOA with the maximum proportion of land dedicated to this use.\nTo visualise the whole distribution of the variable Discontinuous.urban.fabric..112...2012., we can plot a histogram:\n\nhist(sf_LSOA$Discontinuous.urban.fabric..112...2012., breaks=15, xlab=\"% Discontinuous.urban.fabric..112...2012.\", ylab='Number of LSOAs', main=NULL)\n\n\n\n\nThe histogram reveals that many LSOAs have a high proportion of discontinuous urban fabric, but there are a few with more than 50% of their land dedicated to this use.\nNow the question is whether the LSOAs with similar proportions of discontinuous urban fabric are also spatially close. To find out, we need to map the data. We can do this by using the tmap library functions. We observe that, LSOAs in the South of Greater Manchester tend to have high proportions of discontinuous urban fabric, while LSOAs in the peripheries have a low proportion of this type of land use.\n\nlegend_title = expression(\"% Discontinuous urban fabric (2012)\")\nmap_discountinuous = tm_shape(sf_LSOA) +\n  tm_fill(col = \"Discontinuous.urban.fabric..112...2012.\", title = legend_title, text.size = 28, palette = viridis(256), style = \"cont\") + # add fill\n  tm_layout(legend.position = c(0.69, 0.02), legend.title.size=0.9, inner.margins=c(0.05, 0.05, 0.05, 0.26)) +\n  tm_borders(col = \"white\", lwd = .01)  + # add borders\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 2) + # add compass\n  tm_scale_bar(breaks = c(0,2,4,6,8,10), text.size = 0.8, position =  c(\"left\", \"bottom\")) # add scale bar\n\nmap_discountinuous"
  },
  {
    "objectID": "geodemographics.html#sec-sec3",
    "href": "geodemographics.html#sec-sec3",
    "title": "3  Geodemographics",
    "section": "3.4 Standardisation",
    "text": "3.4 Standardisation\n\n3.4.1 Across geographic units\nLSOAs have been designed to have similar population sizes, so their area fluctuates. If the area of an LSOA is bigger or smaller, this can affect the figures corresponding to land use (e.g. presumably, the larger the total area, the hectares will be dedicated to a certain use).\nTo counter the effect of varying sizes across geographic units, we always need to standardise the data so it is given as a proportion or a percentage. This has already been done in the dataset manchester_land_cover_2011.gpkg, however, if you were to create your own dataset, you need to take this into account. To compute the right percentages, it is important to consider the right denominator. For example, if we are computing the percentage of people over the age of 65 in a given geographic unit, we can divide the number of people over 65 by the total population in that geographic unit, then multiply by 100. However, if we are computing the percentage of single-person households, we need to divide the number of single-person households by the total number of households (and not by the total population), then multiply by 100.\n\n\n3.4.2 Variable standardisation\nData outliers are often present when analysing data from the real-world. These values are generally extreme and difficult to treat statistically. In GDC, they could end up dominating the classification process. To avoid this, we need to standardise the input variables as well, so that they all contribute equally to the classification process.\nThere are different methods for variable standardisation, but here we will achieve this by computing the Z-scores for each variable, i.e. for variable \\(X\\), Z-score = \\(\\dfrac{X-mean(X)}{std(X)}\\) where \\(std()\\) refers to standard deviation. In R, obtaining the Z-score of a variable is very simple with the function scale(). Since we want to obtain the Z-scores of all the variables under consideration, we can loop over the columns corresponding to the variables that we want to standardise. Before doing this, we create a dataframe based on sf_LSOA but dropping the geometry column:\n\n# creates a new data frame based on sf_LSOA\ndf_std &lt;- sf_LSOA %&gt;% st_drop_geometry()\n# extracts column names from df_std\ncolnames_df_std &lt;- colnames(df_std)\n\n# loops columns from position 3 : the last column\nfor(i in 3: 38){\ndf_std[[c(colnames_df_std[i])]] &lt;- scale(as.numeric(df_std[, colnames_df_std[i]]))\n}\n\nAfter the standardisation, some columns in df_std contain NaN. For simplicity, we will remove these columns from the resulting dataframe as below, and we will redefine the variable containing the column names so that it only stores the column names corresponding to landuse variables:\n\ndf_std &lt;- df_std %&gt;% select_if(~ !any(is.na(.)))\ncolnames_df_std &lt;- colnames(df_std)[3:(ncol(df_std)-6)]"
  },
  {
    "objectID": "geodemographics.html#sec-sec35",
    "href": "geodemographics.html#sec-sec35",
    "title": "3  Geodemographics",
    "section": "3.5 Checking for variable association",
    "text": "3.5 Checking for variable association\nBefore diving into the clustering process, it is necessary to check for variable associations. Two variables that are strongly associated could be conveying essentially the same information. Consequently, excessive weight could be attributed to the phenomenon they refer to in the clustering process. There are different techniques to check for variable association, but here we focus on the Pearson’s correlation matrix.\nEach row and column in a Pearson’s correlation matrix represents a variable. Each entry in the matrix represents the level of correlation between the variables represented by the corresponding row and column. In R, a Pearson’s correlation matrix can be created very easily with the corr() function, where the method parameter is set to “pearson”. As a general rule, two variables with correlation coefficient greater than 0.8 or smaller than -0.8 are considered to be highly correlated. If this is the case, we might want to discard one of the two variables since the information they convey is redundant. However, in some cases, it might be reasonable to keep both variables if we can argue that they both have a similar but unique meaning.\nThe correlation coefficients by themselves are not enough to conclude whether two variables are correlated. Each correlation coefficient must be computed in combination with its p-value. For this reason, we also apply the cor_pmat() function below, which outputs a matrix of p-values corresponding to each correlation coefficient. Here, we set the confidence level at 0.9, therefore, p-values smaller than 0.1 are considered to be statistically significant. In the correlation matrix plot, we add crosses to indicate which correlation coefficients are not significant (i.e. those above 0.1). Those crosses indicate that there is not enough statistical evidence to reject the claim that the variables in the corresponding row and column are uncorrelated.\n\n# Matrix of Pearson correlation coefficients\ncorr_mat &lt;- cor(df_std[,c(colnames_df_std)], method = \"pearson\")\n# Matrix of p-values\ncorr_pmat &lt;- cor_pmat(df_std[,c(colnames_df_std)], method = \"pearson\", conf.level = 0.95)\n# Barring the no significant coefficient\nggcorrplot(corr_mat, tl.cex=5, hc.order = TRUE, outline.color = \"white\", p.mat = corr_pmat, colors = c(viridis(3)), lab=TRUE, lab_size=1.5)\n\n\n\n\nAmong the statistically significant values in the correlation matrix, we can see that none of the variables display a strong correlation (i.e. close to -1 or 1), except of course, each variable with itself (as shown in the diagonal cells of the correlogram). In fact, many of the measured correlations are not significant. Therefore, we do not need to remove any variables from the analysis. However, if we had found a strong correlation between a pair of variables which is statistically significant, we could remove one of the two variables, since they would be both capturing very similar information."
  },
  {
    "objectID": "geodemographics.html#sec-sec36",
    "href": "geodemographics.html#sec-sec36",
    "title": "3  Geodemographics",
    "section": "3.6 The clustering process",
    "text": "3.6 The clustering process\n\n3.6.1 K-means\nK-means clustering is a way of grouping similar items together. To illustrate the method, imagine you have a bag filled with vegetables, and you want to separate them into smaller bags based on their color, size and flavour. K-means would do this for you by first randomly selecting a number k of vegetables (you provide k, e.g. k=4), and then grouping all the other vegetables based on which of the k vegetables selected initially they are closest to in color, size and flavour. This process is repeated a few times until the vegetables are grouped as best as possible. The end result is k smaller bags, each containing veg of similar color, size and flavour. This is similar to how k-means groups similar items in a data set into clusters.\nMore technically, k-means clustering is actually an algorithm of unsupervised learning (we will learn more about this in Chapter 10) that partitions a set of points into k clusters, where k is a user-specified number. The algorithm iteratively assigns each point to the closest cluster, based on the mean of the points in the cluster, until no point can be moved to a different cluster to decrease the sum of squared distances between points and their assigned cluster mean. The result is a partitioning of the points into k clusters, where the points within a cluster are as similar as possible to each other, and as dissimilar as possible from points in other clusters.\nIn R, k-means can be easily applied by using the function k-means(), where some of the required arguments are: the dataset, the number of clusters (which is called centers), the number of random sets to choose (nstart) or the maximum number of iterations allowed. For example, for a 4-cluster classification, we would run the following line of code:\n\nKm &lt;- kmeans(df_std[,c(colnames_df_std)], centers=4, nstart=20, iter.max = 1000)\n\n\n\n3.6.2 Number of clusters\nAs mentioned above, the number of clusters k is a parameter of the algorithm that has to be specified by the user. Ultimately, there is no right or wrong answer to the question ‘what is the optimum number of clusters?’. Deciding the value of k in the k-means algorithm can be a somewhat subjective process where in most cases, common sense is the most useful approach. For example, you can ask yourself if the obtained groups are meaningful and easy to interpret or if, on the other hand, there are too few or too many clusters, making the results unclear.\nHowever, there are some techniques and guidelines to help us decide what the right number of clusters is. Here we explore the silhouette score method.\nThe silhouette score of a data point (in this case an LSOA and its demographic data) is a measure of how similar this data point is to the data points in its own cluster compared to the data points in other clusters. The silhouette score ranges from -1 to 1, with a higher value indicating that the data point is well matched to its own cluster and poorly matched to neighbouring clusters. A score close to 1 means that the data point is distinctly separate from other clusters, whereas a score close to -1 means the data point may have been assigned to the wrong cluster. Given a number of clusters k obtained with k-means, we can compute the average silhouette score over all the data points. Then, we can plot the average silhouette score against k. The optimal value of k will be the one with the highest k score.\nYou can run the code below to compute the average silhouette score corresponding to different values of k ranging from 2 to 20. The optimum number of clusters is given by the value of k at which the average silhouette is maximised.\n\n# Specify a random number generator seed for reproducibility\nset.seed(123) \n\nsilhouette_score &lt;- function(k){\n  km &lt;- kmeans(df_std[,c(colnames_df_std)], centers = k, nstart=5, iter.max = 1000)\n  ss &lt;- silhouette(km$cluster, dist(df_std[,c(colnames_df_std)]))\n  mean(ss[, 3])\n}\nk &lt;- 2:20\navg_sil &lt;- sapply(k, silhouette_score)\nplot(k, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)\n\n\n\n\nFrom the figure, we can see that the optimum k is 12. Note, this number might be different when you run the programme since the clustering algorithm involves some random steps, unless you set the seed to 123 as we did above. A number of clusters of k=12 is too high for our results to be insightful and interpretable. Therefore, to keep our interpretations simpler, we will pick a lower number of clusters to help us undertand our data better, such as 5.\n\n# Specify a random number generator seed for reproducibility\nset.seed(123) \nKm &lt;- kmeans(df_std[,c(colnames_df_std)], centers=5, nstart=20, iter.max = 1000)\n\n\n\n3.6.3 Other clustering methods\nThere are several other clustering methods apart from k-means. Each method has its own advantages and disadvantages, and the choice of method will ultimately depend on the specific data and clustering problem. We will not explore these methods in detail, but below we include some of their names and a brief description. If you want to learn about them, you can refer to the book “Pattern Recognition and Machine Learning” by Christopher Bishop (Bishop 2006).\n\nFuzzy C-means: a variation of k-means where a data point can belong to multiple clusters with different membership levels.\nHierarchical clustering: this method forms a tree-based representation of the data, where each leaf node represents a single data point and the branches represent clusters. A popular version of this method is agglomerative hierarchical clustering, where individual data points start as their own clusters, and are merged together in a bottom-up fashion based on similarity.\nDBSCAN: a density-based clustering method that groups together nearby points and marks as outliers those points that are far away from any cluster.\nGaussian Mixture Model (GMM): GMMs are probabilistic models that assume each cluster is generated from a Gaussian distribution. They can handle clusters of different shapes, sizes, and orientations."
  },
  {
    "objectID": "geodemographics.html#sec-sec37",
    "href": "geodemographics.html#sec-sec37",
    "title": "3  Geodemographics",
    "section": "3.7 GDC results",
    "text": "3.7 GDC results\n\n3.7.1 Mapping the clusters\nOur LSOAs are now grouped into 5 clusters according to the similarity in their land use characteristics. We can include to our dataset the cluster where each geographical unit belongs to:\n\ndf_std$cluster &lt;- Km$cluster\n\nTo map the results from clustering, we add the spatial inforamtion which we recover from sf_LSOA.\n\n# Get the geometry column from the sf object\ngeometry_column &lt;- st_geometry(sf_LSOA)\n\n# Set the geometry column in the dataframe which becomes a simple feature object\nsf_std &lt;- st_set_geometry(df_std, geometry_column)\n\nFinally, we can plot the results of the clustering process on a map using functions from the tmap library:\n\nmap_cluster = tm_shape(sf_std) +\n  tm_fill(col = \"cluster\", title = \"Cluster\", palette = viridis(256), style = \"cont\") + # add fill\n  tm_borders(col = \"white\", lwd = .01)  + # add borders\n  tm_layout(legend.position = c(0.9, 0.02), legend.title.size=0.9, inner.margins=c(0.05, 0.05, 0.05, 0.05)) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\") , size = 2) + # add compass\n  tm_scale_bar(breaks = c(0,2,4,6,8,10), text.size = 0.8, position =  c(\"left\", \"bottom\")) # add scale bar\n\nmap_cluster\n\n\n\n\nNote: sometimes, the number of items in a cluster may be very small. In that case, you may want to merge two clusterx to make the number of items in each group more homogeneous or perhaps change k in the k-means algorithm.\n\n\n3.7.2 Cluster interpretation\nThe map above not only displays the clusters where each LSOA belongs, but it also shows that there is a tendency for LSOAs belonging to the same cluster to be geographically close. This indicates that LSOAs with similar land use characteristics are close to each other. However, we still need to understand what each cluster represents.\nThe so-called cluster centers (kmCenters) are the data points that, within each cluster, provide a clear indication of the average characteristics of the cluster where they belong based on the variables used in the classification. The data used in the clustering process was Z-score standardised, so the values of each variable corresponding to the cluster centers are still presented as Z-scores. Zero indicates the mean for each variable across all the data points in the sample, and values above or below zero indicate the number of standard deviations from the average. This makes it easy to understand the unique characteristics of each cluster relative to the entire sample. To visualise the characteristics and meaning of the clusters centers and their corresponding clusters, we use radial plots. Below we produce a radial plot for cluster 1. Can you see which variables are higher or lower than average in this cluster? If you want to visualise the radial plot for other clusters, you will need to change the number inside the brackets of KmCenters\\[1,\\].\n\n# creates a radial plot for cluster 1\n# the boxed.radial (False) prevents white boxes forming under labels\n# radlab rotates the labels\nKmCenters &lt;- as.matrix(Km$centers)\nKmCenters &lt;- as.data.frame(KmCenters)\nradial.plot(KmCenters[1,], labels = colnames(KmCenters),\nboxed.radial = FALSE, show.grid = TRUE,\nline.col = \"blue\", radlab = TRUE, rp.type=\"p\", label.prop=1, mar=c(3,3,3,3))\n\n\n\n\n\n\n3.7.3 Testing\nWe will evalueate the fit of the k-means model with 5 clusters by creating an x-y plot of the of the first two principal components of each data point. Each point is coloured according to the cluster where it belongs. Remember that the aim of principal component analysis is to create the minimum number of new variables based on a combination of the original variables that can explain the variability in the data. The first principal component is the new variable that captures the most variability.\nIn the plot below, we can see the first and second principal components in the x and y axes respectively, with the axis label indicating the amount of variability that these components are able to explain. To create the plot, we use the fviz_cluster() function from the factoextra library.\n\nfviz_cluster(Km, data = df_std[,c(colnames_df_std)], geom = \"point\", ellipse = F, pointsize = 0.5,\nggtheme = theme_classic())\n\n\n\n\nThere are obvious clusters in the plot, but some points are in the overlapping regions of two or more clusters, making it unclear to what cluster they should really belong. This is a result of the fact that the plot is only representing two of the principal components, and there are other variables that are not captured in this 2-dimensional representation."
  },
  {
    "objectID": "geodemographics.html#questions",
    "href": "geodemographics.html#questions",
    "title": "3  Geodemographics",
    "section": "3.8 Questions",
    "text": "3.8 Questions\nFor this set of questions, we will be using the same dataset that we used for Chapter 3, but for Greater London:\n\nsf_LSOA &lt;- st_read(\"./data/geodemographics/london_land_cover_2011.gpkg\")\n\nReading layer `london_land_cover_2011' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/geodemographics/london_land_cover_2011.gpkg' \n  using driver `GPKG'\nSimple feature collection with 4835 features and 44 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n\nPrepare your data for a geodemographic classification (GDC). To do this, start by standardising the land use variables. Then, check for variable association using a correlation matrix. Discard any variables if necessary. Now you should be ready to group the data into clusters using the k-means algorithm. Report the optimal number of clusters based on the average silhouette score method. Select the number of clusters for a GDC with k-means according to this method or otherwise. Join the resulting dataset with the LSOA boundary data. Every time you apply the kmeans() function, you should set nstart=20 and iter.max=1000.\n\nEssay questions:\n\nDescribe how you prepared your data for the GDC. There is no need to include figures, but you should briefly explain how you reached certain decisions. For example, did you discard any variables due to their strong association with other variables in the dataset? How did you pick the number of clusters for your GDC?\nMap the resulting clusters and generate a radial plot for one of the clusters. You should create just one figure with as many subplots as needed.\nDescribe what you observe and comment on your results. Do you observe any interesting patterns? Do the results of this GDC agree with what you would expect? Justify your answer.\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer."
  },
  {
    "objectID": "sequence-analysis.html#dependencies",
    "href": "sequence-analysis.html#dependencies",
    "title": "4  Sequence Analysis",
    "section": "4.1 Dependencies",
    "text": "4.1 Dependencies\nWe use the libraries below. Note that to use the theme_tufte2() used for ggplot() objects in this chapter, you need to call the file data-viz-themes.R in the repository.\n\n# data manipulation\nlibrary(tidyverse)\n# spatial data manipulation\nlibrary(stars)\nlibrary(sf)\n# download world pop data\nlibrary(wpgpDownloadR) # you may need to install this package running `install.packages(\"devtools\")` `devtools::install_github(\"wpgp/wpgpDownloadR\")'\n# data visualisation\nlibrary(viridis)\nlibrary(RColorBrewer)\nlibrary(patchwork)\nlibrary(ggseqplot) # may need to install by running `devtools::install_github(\"maraab23/ggseqplot\")`\n# sequence analysis\nlibrary(TraMineR)\n# cluster analysis\nlibrary(cluster)\n\nKey packages to this chapter are TraMineR,stars and ggseqplot. TraMineR is the go-to package in social sciences for exploring, analysing and rendering sequences based on categorical data. stars is designed to handle spatio-temporal data in the form of dense arrays, with space and time as dimensions. stars provides classes and methods for reading, manipulating, plotting and writing data cubes. It is a very powerful package. It interacts nicely with sf and is suggested to be superior to raster and terra, which are also known for their capacity to work with multilayer rasters. stars is suggested to deal with more complex data types and be faster than raster and terra. ggseqplot provides functionality to visualise categorical sequence data based on ggplot capabilities. This differs from TraMineR which is based on the base function plot. We prefer ggseqplot for the wide usage of ggplot as a data visualisation tool in R."
  },
  {
    "objectID": "sequence-analysis.html#data",
    "href": "sequence-analysis.html#data",
    "title": "4  Sequence Analysis",
    "section": "4.2 Data",
    "text": "4.2 Data\nThe key aim of this chapter is to define representative trajectories of population decline using sequence analysis and WorldPop data. We use WorldPop data for the period extending from 2000 to 2020. WorldPop offers open access gridded population estimates at a high spatial resolution for all countries in the world. WoldPop produces these gridded datasets using a top-down (i.e. dissagregating administrative area counts into smaller grid cells) or bottom-up (i.e. interpolating data from counts from sample locations into grid cells) approach. You can learn about about these approaches and the data available from WorldPop.\nWorldPop population data are available in various formats:\n\nTwo spatial resolutions: 100m and 1km;\nConstrained and unconstrained counts to built settlement areas;\nAdjusted or unadjusted to United Nations’ (UN) national population counts;\nTwo formats i.e. tiff and csv formats.\n\nWe use annual 1km gridded, UN adjusted, unconstrained population count data for Ukraine during 2000-2021 in tiff format. We use tiff formats to illustrate the manipulation of raster data. Such skills will come handy if you ever decide to work with satellite imagery or image data in general.\nBefore calling the data, let’s see how we can use wpgpDownloadR package. Let’s browse the data catalogue.\n\nwpgpListCountries() %&gt;% \n  head()\n\nWarning in readLines(con, n = 1): incomplete final line found on\n'/var/folders/9z/ql42lpgn22x_c5353k3ycqfr0000gn/T//RtmpzFoa5L/wpgpDatasets.md5'\n\n\n  ISO ISO3            Country\n1 643  RUS             Russia\n2 360  IDN          Indonesia\n3 840  USA      United States\n4 850  VIR Virgin_Islands_U_S\n5 304  GRL          Greenland\n6 156  CHN              China\n\n\nBy using the ISO3 country code, let’s look for the available datasets for Ukraine.\n\nwpgpListCountryDatasets(ISO3 = \"UKR\") %&gt;% \n  head()\n\nWarning in readLines(con, n = 1): incomplete final line found on\n'/var/folders/9z/ql42lpgn22x_c5353k3ycqfr0000gn/T//RtmpzFoa5L/wpgpDatasets.md5'\n\n\n     ISO ISO3 Country Covariate\n232  804  UKR Ukraine  ppp_2000\n481  804  UKR Ukraine  ppp_2001\n730  804  UKR Ukraine  ppp_2002\n979  804  UKR Ukraine  ppp_2003\n1228 804  UKR Ukraine  ppp_2004\n1477 804  UKR Ukraine  ppp_2005\n                                                                                                                                                                 Description\n232  Estimated total number of people per grid-cell 2000 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n481  Estimated total number of people per grid-cell 2001 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n730  Estimated total number of people per grid-cell 2002 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n979  Estimated total number of people per grid-cell 2003 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n1228 Estimated total number of people per grid-cell 2004 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n1477 Estimated total number of people per grid-cell 2005 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n\n\nThe wpgpDownloadR package includes 100m resolution data. To keep things efficient, we use 1km gridded population counts from the WorldPop data page. Obtain population data for Ukraine 2000-2020. We start by reading the set of tiff files using the read_stars function from the star package.\n\n# create a list of file names\nfile_list &lt;- fs::dir_ls(\"./data/sequence-analysis/raster\")\nfile_list\n\n./data/sequence-analysis/raster/ukr_ppp_2000_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2001_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2002_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2003_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2004_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2005_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2006_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2007_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2008_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2009_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2010_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2011_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2012_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2013_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2014_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2015_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2016_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2017_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2018_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2019_1km_Aggregated_UNadj.tif\n./data/sequence-analysis/raster/ukr_ppp_2020_1km_Aggregated_UNadj.tif\n\n# read a list of raster data\npop_raster &lt;- read_stars(file_list, quiet = TRUE)\n\nWe map the data for 2000 to get a quick understanding of the data.\n\nplot(pop_raster[1], col = inferno(100))\n\ndownsample set to 2\n\n\n\n\n\nNext we read shapefile of administrative boundaries in the form of polygons. We obtain these data from the GADM website. GADM provides maps and spatial data for individuals countries at the national and sub-national administrative divisions. In this chapter, we will work with these data as they come directly from the website which provides a more realistic and similar context to which you will probably come across in the “real-world”.\n\n# read spatial data frame\nukr_shp &lt;- st_read(\"./data/sequence-analysis/ukr_shp/gadm41_UKR_2.shp\") %&gt;% \n  st_simplify(., # simplify boundaries for efficiency\n              preserveTopology = T,\n              dTolerance = 1000) %&gt;%  # 1km\n  sf::st_make_valid(.) %&gt;% \n  fortify(.) %&gt;%  # turns maps into a data frame so they can more easily be plotted with ggplot2\n  st_transform(., \"EPSG:4326\") # set projection system\n\nReading layer `gadm41_UKR_2' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/sequence-analysis/ukr_shp/gadm41_UKR_2.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 629 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 22.14045 ymin: 44.38597 xmax: 40.21807 ymax: 52.37503\nGeodetic CRS:  WGS 84\n\nukr_shp\n\nSimple feature collection with 629 features and 13 fields (with 1 geometry empty)\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 22.14519 ymin: 44.38681 xmax: 40.21807 ymax: 52.375\nGeodetic CRS:  WGS 84\nFirst 10 features:\n       GID_2 GID_0 COUNTRY   GID_1   NAME_1 NL_NAME_1            NAME_2\n1          ?   UKR Ukraine       ?        ?         ?                 ?\n2  UKR.1.1_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська        Cherkas'ka\n3  UKR.1.2_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська       Cherkas'kyi\n4  UKR.1.3_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська   Chornobaivs'kyi\n5  UKR.1.4_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська     Chyhyryns'kyi\n6  UKR.1.5_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська       Drabivs'kyi\n7  UKR.1.6_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська Horodyshchens'kyi\n8  UKR.1.7_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська       Kamians'kyi\n9  UKR.1.8_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська         Kanivs'ka\n10 UKR.1.9_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська        Kanivs'kyi\n         VARNAME_2 NL_NAME_2      TYPE_2                     ENGTYPE_2 CC_2\n1                ?        NA           ?                            NA   NA\n2               NA        NA Mis'ka Rada City of Regional Significance   NA\n3               NA        NA       Raion                      District   NA\n4  Chornobayivskyi        NA       Raion                      District   NA\n5               NA        NA       Raion                      District   NA\n6               NA        NA       Raion                      District   NA\n7  Gorodyschenskyi        NA       Raion                      District   NA\n8               NA        NA       Raion                      District   NA\n9               NA        NA       Misto                          City   NA\n10              NA        NA       Raion                      District   NA\n     HASC_2                       geometry\n1         ? POLYGON ((30.59574 50.40547...\n2  UA.CK.CM POLYGON ((32.1715 49.43881,...\n3  UA.CK.CR POLYGON ((32.03393 49.49881...\n4  UA.CK.CB POLYGON ((32.17991 49.44486...\n5  UA.CK.CY POLYGON ((32.26144 49.20893...\n6  UA.CK.DR POLYGON ((32.41852 49.83724...\n7  UA.CK.HO POLYGON ((31.56959 49.42509...\n8  UA.CK.KN POLYGON ((32.19797 49.20946...\n9  UA.CK.KM MULTIPOLYGON (((31.4459 49....\n10 UA.CK.KR POLYGON ((31.5851 49.62482,...\n\n\nLet’s have a quick look at the resolution of the administrative areas we will be working. The areas below represent areas at the administrative area level 2 in the spatial data frame ukr_shp.\n\nplot(ukr_shp$geometry)\n\n\n\n\nWe ensure that the pop_raster object is in the same projection system as ukr_shp. So we can make both objects to work together.\n\npop_raster &lt;- st_transform(pop_raster, st_crs(ukr_shp))                      \n\n\n4.2.1 Data wrangling\nFor our application, we want to work with administrative areas for three reasons. First, public policy and planning decisions are often made based on administrative areas. These are the areas local governments have jurisdiction, represent and can exert power. Second, migration is a key component of population change and hence directly determines population decline. At a small area, residential mobility may also impact patterns of population potentially adding more complexity and variability to the process. Third, WorldPop data are modelled population estimates with potentially high levels of uncertainty or errors in certain locations. Our aim is to mitigate the potential impacts of these errors.\nWe therefore recommend working with aggregated data. We aggregate the 1km gridded population data to administrative areas in Ukraine. We use system.time to time the duration of the proccess of aggregation which could take some time depending on your local computational environment.\n\nsystem.time({\n\npopbyarea_df = aggregate(x = pop_raster, \n                                   by = ukr_shp, \n                                   FUN = sum, \n                                   na.rm = TRUE) \n})\n\n   user  system elapsed \n 64.950   8.227  73.350 \n\n\nSub-national population\nThe chunk code above returns a list of raster data. We want to create a spatial data frame containing population counts for individual sub-national areas and years. We achieve this by running the following code:\n\n# create a function to bind the population data frame to the shapefile\nadd_population &lt;- function(x) mutate(ukr_shp, \n                      population = x)\n\n# obtain sub-national population counts\nukr_eshp &lt;- lapply(popbyarea_df, add_population)\n\n# create a dataframe with sub-national populations\nselect_pop &lt;- function(x) dplyr::select(x, GID_2, NAME_2, population)\npopulation_df &lt;- lapply(ukr_eshp, select_pop) %&gt;% \n  do.call(rbind, .)\npopulation_df$year &lt;- rep(seq(2000, 2020), times = 1, each = nrow(ukr_shp))\nrownames(population_df) &lt;- rep(seq(1, nrow(population_df), by=1), times = 1)\n\n# sub-national spatial data frame\npopulation_df \n\nSimple feature collection with 13209 features and 4 fields (with 21 geometries empty)\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 22.14519 ymin: 44.38681 xmax: 40.21807 ymax: 52.375\nGeodetic CRS:  WGS 84\nFirst 10 features:\n       GID_2            NAME_2 population                       geometry year\n1          ?                 ?  301849.00 POLYGON ((30.59574 50.40547... 2000\n2  UKR.1.1_1        Cherkas'ka  280917.39 POLYGON ((32.1715 49.43881,... 2000\n3  UKR.1.2_1       Cherkas'kyi   89116.78 POLYGON ((32.03393 49.49881... 2000\n4  UKR.1.3_1   Chornobaivs'kyi   50096.24 POLYGON ((32.17991 49.44486... 2000\n5  UKR.1.4_1     Chyhyryns'kyi   36646.73 POLYGON ((32.26144 49.20893... 2000\n6  UKR.1.5_1       Drabivs'kyi   42467.86 POLYGON ((32.41852 49.83724... 2000\n7  UKR.1.6_1 Horodyshchens'kyi   49886.59 POLYGON ((31.56959 49.42509... 2000\n8  UKR.1.7_1       Kamians'kyi   35587.28 POLYGON ((32.19797 49.20946... 2000\n9  UKR.1.8_1         Kanivs'ka   14406.93 MULTIPOLYGON (((31.4459 49.... 2000\n10 UKR.1.9_1        Kanivs'kyi   37495.04 POLYGON ((31.5851 49.62482,... 2000\n\n\nNational population\nWe also create a data frame providing population counts at the national level.\n\n# obtain national population counts\npopulation_count &lt;- map_dbl(ukr_eshp, ~.x %&gt;% \n          pull(population) %&gt;% \n          sum(na.rm = TRUE)\n        ) %&gt;% \n  as.data.frame()\n\n# change labels\ncolnames(population_count) &lt;-  c(\"population\")\nrownames(population_count) &lt;- rep(seq(2000, 2020, by=1), times = 1)\npopulation_count$year &lt;- rep(seq(2000, 2020, by=1), times = 1)\n\n# national annual population counts\npopulation_count\n\n     population year\n2000   47955683 2000\n2001   47520197 2001\n2002   47094225 2002\n2003   46700872 2003\n2004   46330322 2004\n2005   46011048 2005\n2006   45734099 2006\n2007   45502336 2007\n2008   45286748 2008\n2009   45090608 2009\n2010   44923112 2010\n2011   44744969 2011\n2012   44593427 2012\n2013   44424702 2013\n2014   44250993 2014\n2015   44068072 2015\n2016   43856852 2016\n2017   43622605 2017\n2018   43391259 2018\n2019   43140679 2019\n2020   42880388 2020\n\n\n\n\n4.2.2 Exploratory data analysis\nNow we are ready to start analysing the data. Before building complexity on our analysis, conducting some exploratory data analysis to understand the data is generally a good starting point, particularly given the multi-layer nature of the data at hand - capturing space, time and population levels.\nNational patterns\nWe first analyse national population trends. We want to know to what extent the population of Ukraine has declined over time over the last 20 years. An effective way to do this is to compute summary statistics and visualise the data. Below we look at year-to-year changes in population levels and as a percentage change. By using patchwork, we combine two plots into a single figure.\n\n# visualise national population trends\npop_level_p &lt;- ggplot(population_count, \n       aes(x = year, y = population/1000000 )) +\n  geom_line(size = 1) +\n  theme_tufte2() +\n  ylim(0, 48) + \n  labs(y = \"Population \\n(million)\",\n       x = \"Year\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# visualise percentage change in population\npop_percent_p &lt;- population_count %&gt;% \n  mutate(\n    pct_change = ( ( population - 47955683) / 47955683) * 100\n  ) %&gt;% \nggplot(aes(x = year, y = pct_change )) +\n  geom_line(size = 1) +\n  theme_tufte2() + \n  labs(y = \"Population \\npercentage change (%)\",\n       x = \"Year\") \n\npop_level_p | pop_percent_p\n\n\n\n\nSub-national\nPopulation losses are likely vary across the country. From previous research we know that rural and less well connected areas tend to lose population through the internal migration of young individuals as they move for work and job opportunities (Rowe, Corcoran, and Bell 2016). We also know that they tend to move to large, densely populated cities where these opportunities are concentrated and because they also offer a wide variety of amenities and activities. Cities tend to work as accelarators enabling fast career development and occupational progression (Fielding 1992). Though, we have also seen the shrinkage of populations in cities, particularly in eastern European countries (Turok and Mykhnenko 2007).\nTo examine the patterns of sun-national population losses, we compute two summary measures: (1) annual percentage change in population; and, (2) overall percentage change in population between 2000 and 2021. We start by looking the overall percentage change as it is easier to visualise. To this end, we categorise our measure of overall percentage change into seven different classes. Based on previous work by González-Leonardo, Newsham, and Rowe (2023), we classify changes into high decline (\\(\\leq\\) -3), decline (&gt; -3 and \\(\\leq\\) -1.5), moderate decline (&gt; -1.5 and \\(\\leq\\) -0.3), stable (&lt; -0.3 and &lt; 0.3), moderate growth (\\(\\geq\\) 0.3 and &lt; 1.5), growth (\\(\\geq\\) 1.5 and &lt; 3) and high growth (\\(\\geq\\) 3). Let’s first create the measures of population change.\n\n# compute population change metrics\npopulation_df &lt;- population_df %&gt;% \n  dplyr::group_by(GID_2) %&gt;% \n  arrange(-year, .by_group = TRUE ) %&gt;% \n  mutate(\n  pct_change = ( population / lead(population) - 1) * 100, # rate of population change\n  pct_change_2000_21 = ( population[year == \"2020\"] / population[year == \"2000\"] - 1) * 100, # overall rate of change\n  ave_pct_change_2000_21 = mean(pct_change, na.rm = TRUE)\n) %&gt;% \n  ungroup()\n\nLet’s map the overall percentage change in population between 2000 and 2020. We see a wide spread pattern of population decline across Ukraine. We observe a large spatial cluster of high population decline across the country with moderate population decline in some areas. Administrative areas containing large cities seem to record overall population growth between 2000 and 2020, potentially absorbing population movements from the rest of the country. What else do you think may be driving population growth in cities? And in contrast, what do you think is contributing to population decline in most of Ukraine?\n\n# set colours\ncols &lt;- c(\"#7f3b08\", \"#b35806\", \"#e08214\", \"#faf0e6\", \"#8073ac\", \"#542788\", \"#2d004b\")\n# reverse order\ncols &lt;- rev(cols)\n\npopulation_df %&gt;% dplyr::filter( year == 2020) %&gt;%\n  drop_na(pct_change_2000_21) %&gt;% \n    mutate(\n    ove_pop_class = case_when( pct_change_2000_21 &lt;= -3 ~ 'high_decline',\n                           pct_change_2000_21 &lt;= -1.5 & pct_change_2000_21 &gt; -3 ~ 'decline',\n                           pct_change_2000_21 &lt;= -.3 & pct_change_2000_21 &gt; -1.5 ~ 'moderate_decline',\n                           pct_change_2000_21 &gt; -0.3 & pct_change_2000_21 &lt; 0.3 ~ 'stable',\n                           pct_change_2000_21 &gt;= 0.3 & pct_change_2000_21 &lt; 1.5 ~ 'moderate_growth',\n                           pct_change_2000_21 &gt;= 1.5 & pct_change_2000_21 &lt; 3 ~ 'growth',\n                           pct_change_2000_21 &gt;= 3 ~ 'high_growth'),\n    ove_pop_class = factor(ove_pop_class, \n         levels = c(\"high_decline\", \"decline\", \"moderate_decline\", \"stable\", \"moderate_growth\", \"growth\", \"high_growth\") )\n    ) %&gt;% \n  ggplot(aes(fill = ove_pop_class)) +\n  geom_sf(col = \"white\", size = .1) +\n  scale_fill_manual(values = cols,\n                    name = \"Population change\") +\n  theme_map_tufte() \n\n\n\n\nNow that we have understanding of population changes over the whole 2000-2020 period. Let’s try to understand how different places arrive to different outcomes. A way to do this is to look at the evolution of population changes. Different trajectories of population change could underpin the outcomes of population change that we observe today. Current outcomes could be the result of a consistent pattern of population decline over the last 20 years. They could be the result of acceleration in population loss after a major natural or war event, or they could reflect a gradual process of erosion. We visualise way to get an understanding of this is to analyse annual percentage population changes across individual areas. We use a Hovmöller Plot as illustrated by Rowe and Arribas-Bel (2022) for the analysis of spatio-temporal data.\n\npopulation_df %&gt;% dplyr::filter( ave_pct_change_2000_21 &lt; 0) %&gt;% \n  tail(., 40*21) %&gt;% \n  ggplot(data = ., \n           mapping = aes(x= year, y= reorder(NAME_2, pct_change), fill= pct_change)) +\n  geom_tile() +\n  scale_fill_viridis(name=\"Population\", option =\"plasma\", begin = .2, end = .8, direction = 1) +\n  theme_tufte2() +\n  labs(title= paste(\" \"), x=\"Year\", y=\"Area\") +\n  theme(text = element_text(size=14)) + \n  theme(axis.text.y = element_text(size=8))\n\n\n\n\nThe Hovmöller Plot shows that most of the selected areas tend to experience annual population decline, with varying spells of population growth. Percentage population changes range between 1 and -2.5. We also observe areas with consistent trajectories of annual population decline, like Zolochivs’kyi and Barvinkivs’kyi, and areas with strong decline in the first few years between 2000 and 2005 but moderate decline later on, such as Novovorontsovk’kyi. Yet, Hovmöller Plots provide a limited understanding of the annual population changes for a handful of areas at the time and it is therefore difficult to identify systematic representative patterns. Here we have selected 40 areas of a total of 629. Displaying the total number of areas in a Hovmöller Plot will not produce readable results. Even if that was the case, it would be difficult to identify systematic patterns. As we will seek to persuade you below, sequence analysis provides a very novel way to define representative trajectories in the data, identify systematic patterns and extract distinctive features characterising those trajectories."
  },
  {
    "objectID": "sequence-analysis.html#application",
    "href": "sequence-analysis.html#application",
    "title": "4  Sequence Analysis",
    "section": "4.3 Application",
    "text": "4.3 Application\nNext, we focus on the application of sequence analysis to identify representative trajectories of population decline at the sub-national level between 2000 and 2020 in Ukraine. Intuitively, sequence analysis can be seen as a four-stage process. First, it requires the definition of longitudinal categorical outcome. Second, it measures the dissimilarity of individual sequences via a process known as optimal matching (OM). Third, it uses these dissimilarity measures to define a typology of representative trajectories using unsupervised machine learning clustering techniques. Fourth, trajectories can be visualised and their distinctive features can be measured. Below we describe the implementation of each stage to identify representative trajectories of population decline.\n\n4.3.1 Defining outcome process\nSequence analysis requires longitudinal categorical data as an input. We therefore classify our population count data into distinct categorical categories, henceforth referred to as states of population change. We compute the annual percentage rate of population change for individual areas and use these rates to measure the extent and pace of population change. The annual rate of population change is computed as follows:\n\\[\n{p(t1) - p(t0) \\over p(t0)}*100\n\\]\nwhere: \\(p(t0)\\) is the population at year t0 and \\(p(t1)\\) is the population at t + 1.\nAs previously, we differentiate areas of high decline, decline, moderate decline, stable, moderate growth, growth and high growth. For the analysis, we focus on areas recording population losses between 2000 and 2020. The histogram shows the magnitude and distribution of population decline over this period. We observe that most occurrences of decline are moderate around zero, while very few exceed 5%.\n\n# select areas reporting losses between 2000 and 2020\npopulation_loss_df &lt;- population_df %&gt;% \n  dplyr::filter( pct_change_2000_21 &lt; 0)\n# plot distribution of percentage change \npopulation_loss_df %&gt;% \n  dplyr::filter(pct_change  &lt; 0) %&gt;% \n  ggplot(data =  ) +\n  geom_density(alpha=0.8, colour=\"black\", fill=\"lightblue\", aes(x = pct_change)) +\n  theme_tufte2()\n\n\n\n\nNext we classify the annual percentage of population change into our seven states.\n\n# remove 2000 as it has no observations of population change\npopulation_loss_df &lt;- population_loss_df %&gt;% \n  dplyr::filter( year != 2000)\n# clasify data\npopulation_loss_df &lt;- population_loss_df %&gt;%\n  mutate(\n    pop_class = case_when( pct_change &lt;= -3 ~ 'high_decline',\n                           pct_change &lt;= -1.5 & pct_change &gt; -3 ~ 'decline',\n                           pct_change &lt;= -.3 & pct_change &gt; -1.5 ~ 'moderate_decline',\n                           pct_change &gt; -0.3 & pct_change &lt; 0.3 ~ 'stable',\n                           pct_change &gt;= 0.3 & pct_change &lt; 1.5 ~ 'moderate_growth',\n                           pct_change &gt;= 1.5 & pct_change &lt; 3 ~ 'growth',\n                           pct_change &gt;= 3 ~ 'high_growth')\n)\n\n\n\n4.3.2 Optimal matching\nWe measure the extent of dissimilarity between individual sequence of population decline. To this end, we used a sequence analysis technique, OM, which computes distances between sequences as a function of the number of transformations required to make sequences identical. Two sets of operations are generally used: (1) insertion/deletion (known as indel) and (2) substitution operations. Both of these operations represent the cost of transforming one sequence into another. These costs are challenging to define and below we discuss what is generally used in empirical work. Intuitively, the idea of OM is to estimate the cost of transforming one sequence into another so that the greater the cost to make two sequences identical, the greater the dissimilarity and vice versa.\nIndel operations involve the addition or removal of an element within the sequence and substitution operations are the replacement of one element for another. Each of these operations is assigned a cost, and the distance between two sequences is defined as the minimum cost to transform one sequence to another (Abbott and Tsay 2000). By default, indel costs are set to 1. To illustrate indel operations, let’s consider an example of sequences of annual population change for three areas during 2000 and 2003. The sequences are identical, except for 2003. In this case, indel operations involve the cost of transforming the status stable in the sequence for area 1 to high decline in the sequence for area 2, and thus this operation would return a cost is 2. Why 2? It is 2 because you would need to delete stable and add high decline. Now, let’s try the cost of transforming the status stable in the sequence for area 1 to the status in the sequence for area 3 using indel operations. What is the cost? The answer is 1 because we only need to delete stable to make it identical.\n\n\n\nArea\n2000\n2001\n2002\n2003\n\n\n\n\n1\ndecline\ndecline\ndecline\nstable\n\n\n2\ndecline\ndecline\ndecline\nhigh decline\n\n\n3\ndecline\ndecline\ndecline\n-\n\n\n\nSubstitution operations or costs represent transition costs; that is, the cost for substituting each state with another. Substitution costs are defined in one of two ways (Salmela-Aro et al. 2011). One approach is the theory-driven approach. In such approach, substitution costs are grounded in theory suggesting that, for example, transforming state 1 to state 2 should have a greater cost than transforming state 1 to state 3, or performing the opposite operation i.e. transforming state 2 to state 1. An example could be that it is more financially costly to transition from full-time employment to full-time education than transition from full-time education to full-time employment.\nA second approach and most commonly used in empirical work is a data-driven approach. In this approach, substitution costs are empirically derived from transition rates between states. The cost of substitution is inversely related to the frequency of observed transitions within the data. This means that infrequent transitions between states have a higher substitution cost. For example, as we will see, transitions from the state of high decline to high growth are rarer than from high growth to high decline in Ukraine. The transition rate between state \\(i\\) and state \\(j\\) is the probability of observing state \\(j\\) at time \\(t1\\) given that the state \\(i\\) is observed at time \\(t\\) for \\(i \\neq j\\). The substitution cost between states \\(i\\) and \\(j\\) is computed as:\n\\[\n2 - {p(i | j) - p(j | i)}\n\\] where \\(p(i | j)\\) is the transition rate between state \\(i\\) and \\(j\\).\nTo implement OM, we first need to rearrange the structure of our data from long to wide format. You can now see now that individual rows represent areas (column 1) and columns from 2 to 21 represent years.\n\nsee Rowe and Arribas-Bel (2022) for a description on different spatio-temporal data structures and their manipulation using tidyverse principles.\n\n\n# transform from long to wide format\nwide_population_loss_df &lt;- population_loss_df %&gt;% \n  as_tibble() %&gt;% \n  group_by(GID_2) %&gt;% \n  arrange(year, .by_group = TRUE ) %&gt;%\n  ungroup() %&gt;% \n  tidyr::pivot_wider(\n  id_cols =  GID_2,\n  names_from = \"year\",\n  values_from = \"pop_class\"\n)\n  \nwide_population_loss_df\n\n# A tibble: 571 × 21\n   GID_2   `2001` `2002` `2003` `2004` `2005` `2006` `2007` `2008` `2009` `2010`\n   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; \n 1 UKR.1.… decli… moder… decli… decli… decli… moder… moder… moder… moder… moder…\n 2 UKR.1.… moder… moder… moder… moder… moder… moder… moder… moder… moder… moder…\n 3 UKR.1.… moder… moder… moder… moder… moder… moder… moder… moder… moder… moder…\n 4 UKR.1.… decli… decli… moder… decli… moder… moder… moder… moder… moder… moder…\n 5 UKR.1.… decli… decli… moder… decli… moder… moder… moder… moder… moder… moder…\n 6 UKR.1.… moder… moder… moder… moder… moder… moder… moder… moder… moder… moder…\n 7 UKR.1.… moder… moder… moder… moder… moder… moder… moder… moder… moder… moder…\n 8 UKR.1.… moder… moder… moder… moder… moder… moder… moder… moder… moder… moder…\n 9 UKR.1.… decli… decli… decli… decli… decli… decli… decli… decli… moder… decli…\n10 UKR.1.… moder… stable moder… moder… stable moder… moder… stable decli… stable\n# ℹ 561 more rows\n# ℹ 10 more variables: `2011` &lt;chr&gt;, `2012` &lt;chr&gt;, `2013` &lt;chr&gt;, `2014` &lt;chr&gt;,\n#   `2015` &lt;chr&gt;, `2016` &lt;chr&gt;, `2017` &lt;chr&gt;, `2018` &lt;chr&gt;, `2019` &lt;chr&gt;,\n#   `2020` &lt;chr&gt;\n\n\nOnce the data frame has been reshaped into a wide format, we define the data as a state sequence object using the R package TraMineR. Key here is to appropriately define the labels and an appropriate palette of colours. Depending on the patterns you are seeking to capture a diverging, sequential or qualitative colour palette may be more appropriate. For this chapter, we use a diverging colour palette as we want to effectively represent areas experiencing diverging patterns of population decline or growth.\n\nNote: various types of sequence data representation exist in TraMineR. These representations vary in the way they capture states or events. Chapter 4 in Gabadinho et al. (2009) describes the various representations that TraMineR can handle. In any case, the state sequence representation used in this chapter is the most commonly used and internal format used by TraMineR. Hence we focus on it.\n\n\n# alphabet\nseq.alphab &lt;- c(\"high_growth\", \"growth\", \"moderate_growth\", \"stable\", \"moderate_decline\", \"decline\", \"high_decline\")\n# labels\nseq.lab &lt;- c(\"High growth\", \"Growth\", \"Moderate growth\", \"Stable\", \"Moderate decline\", \"Decline\", \"High decline\")\n# define state sequence object\nseq.cl &lt;- seqdef(wide_population_loss_df, \n                 2:21, \n                 alphabet = seq.alphab,\n                 labels = seq.lab,\n                 cnames = c(\"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"),\n                 cpal =c(\"1\" = \"#7f3b08\", \n                         \"2\" = \"#b35806\",\n                         \"3\" = \"#e08214\",\n                         \"4\" = \"#faf0e6\",\n                         \"5\" = \"#8073ac\",\n                         \"6\" = \"#542788\",\n                         \"7\" = \"#2d004b\"))\n\n [&gt;] 7 distinct states appear in the data: \n\n\n     1 = decline\n\n\n     2 = growth\n\n\n     3 = high_decline\n\n\n     4 = high_growth\n\n\n     5 = moderate_decline\n\n\n     6 = moderate_growth\n\n\n     7 = stable\n\n\n [&gt;] state coding:\n\n\n       [alphabet]       [label]          [long label] \n\n\n     1  high_growth      high_growth      High growth\n\n\n     2  growth           growth           Growth\n\n\n     3  moderate_growth  moderate_growth  Moderate growth\n\n\n     4  stable           stable           Stable\n\n\n     5  moderate_decline moderate_decline Moderate decline\n\n\n     6  decline          decline          Decline\n\n\n     7  high_decline     high_decline     High decline\n\n\n [&gt;] 571 sequences in the data set\n\n\n [&gt;] min/max sequence length: 20/20\n\n\nUsing the sequence data object, we create a state distribution plot to get an understanding of the data. The plot shows the distribution of areas across status of population change in individual years. The overall picture emerging from the plot is an overall pattern of population decline between 2000 and 2020, predominantly moderate decline and limited spells of high population decline or growth. This aligns with the predominant trajectory of population decline observed in Ukrain based on more aggregate data at the regional level (Newsham and Rowe 2022a).\n\nseqplot(seq.cl, \n        title=\"State distribution plot\", \n        type = \"d\",\n        with.legend = \"right\",\n        border = NA)\n\n\n\n\nWe now move on to compute the substitution costs for our population states. From the equation above, you may have realised that transition costs vary between 0 and 2, with the former indicating zero cost. The latter indicates the maximum cost of converting one state into another. The option TRATE in method states to derive costs from observed transition rates. That is the data-driven approach discussed above. From the matrix below, you can see that it is more costly to convert a status “high growth” to “moderate decline” than from “growth” to “moderate”. This makes sense. We expect gradual changes in population along a given trajectory if they were to occur due to natural causes.\n\nNote we are considering a fixed measure of transition rates. That means that we are using the whole dataset to compute an average transition rate between states. That assumes that the rate of change between states does not change over time. Yet, there may be good reasons to believe they do as areas move across different states. In empirical work, time varying transition rates are more often considered. That means we use temporal slices of the data to compute transition rates; for example, using data from 2001, 2002 and so on. In this way, we end up with potentially different transition rates for every year.\n\n\n# Calculate transition rates\nsubs_costs &lt;- seqsubm(seq.cl, \n                      method = \"TRATE\",\n                      #time.varying = TRUE\n                      )\n\nsubs_costs\n\n                 high_growth   growth moderate_growth   stable moderate_decline\nhigh_growth         0.000000 1.975000        1.940066 1.949203         1.949277\ngrowth              1.975000 0.000000        1.867730 1.867946         1.634772\nmoderate_growth     1.940066 1.867730        0.000000 1.709425         1.547832\nstable              1.949203 1.867946        1.709425 0.000000         1.501723\nmoderate_decline    1.949277 1.634772        1.547832 1.501723         0.000000\ndecline             1.847681 1.848052        1.826810 1.915740         1.643018\nhigh_decline        1.288462 1.700000        1.773408 1.917223         1.805088\n                  decline high_decline\nhigh_growth      1.847681     1.288462\ngrowth           1.848052     1.700000\nmoderate_growth  1.826810     1.773408\nstable           1.915740     1.917223\nmoderate_decline 1.643018     1.805088\ndecline          0.000000     1.844570\nhigh_decline     1.844570     0.000000\n\n\nTo understand better the idea of substitution costs, we can have direct look at transition rates underpinning these costs. Transition rates can be computed via seqtrate . By definition, transition rates vary between 0 and 1, with zero indicating no probability of a transition occurring. One indicates a 100% probability of a transition taking place. Thus, for example, the matrix below tell us that there is a 5% probability of observing a transition from “high growth” to “moderate decline” in our sample. Examining transition rates could provide very valuable information about the process in analysis.\n\nseq.trate &lt;- seqtrate(seq.cl)\n\n [&gt;] computing transition probabilities for states high_growth/growth/moderate_growth/stable/moderate_decline/decline/high_decline ...\n\nround(seq.trate, 2)\n\n                      [-&gt; high_growth] [-&gt; growth] [-&gt; moderate_growth]\n[high_growth -&gt;]                  0.12        0.03                 0.05\n[growth -&gt;]                       0.00        0.05                 0.11\n[moderate_growth -&gt;]              0.01        0.02                 0.09\n[stable -&gt;]                       0.00        0.00                 0.04\n[moderate_decline -&gt;]             0.00        0.00                 0.02\n[decline -&gt;]                      0.00        0.01                 0.02\n[high_decline -&gt;]                 0.16        0.10                 0.18\n                      [-&gt; stable] [-&gt; moderate_decline] [-&gt; decline]\n[high_growth -&gt;]             0.05                  0.05         0.15\n[growth -&gt;]                  0.13                  0.36         0.15\n[moderate_growth -&gt;]         0.25                  0.43         0.15\n[stable -&gt;]                  0.46                  0.42         0.05\n[moderate_decline -&gt;]        0.08                  0.82         0.08\n[decline -&gt;]                 0.03                  0.28         0.65\n[high_decline -&gt;]            0.07                  0.19         0.15\n                      [-&gt; high_decline]\n[high_growth -&gt;]                   0.55\n[growth -&gt;]                        0.20\n[moderate_growth -&gt;]               0.05\n[stable -&gt;]                        0.01\n[moderate_decline -&gt;]              0.00\n[decline -&gt;]                       0.01\n[high_decline -&gt;]                  0.15\n\n\nNow we focus on the probably most important component of sequence analysis; that is, the calculation of dissimilarity. Recall our aim is to identify representative trajectories. To this end, we need a way to measure how similar or different sequences are - which is known as OM. Above, we described that we can use indel and substitution operations to measure the dissimilarity or costs between individual sequences. The code chunk implements OM based on indel and substitution operations. The algorithm takes an individual sequence and compares it with all of the sequences in the dataset, and identifies the sequence with the minimum cost i.e. the most similar sequence. The result of this computing intensive process is a distance matrix encoding the similarity or dissimilarity between individual sequences.\n\nFor indel, auto sets the indel as max(sm)/2 when sm is a matrix. For more details, run ?seqdist on your console\n\n\n# Calculate a distance matrix\nseq.om &lt;- seqdist(seq.cl,\n                  method = \"OM\", # specify the method\n                  indel = \"auto\", # specify indel costs\n                  sm = subs_costs) # specify substitution costs\n\nAs highlighted above, if you would like to apply varying substitution costs, you can do this directly here by using the option method = DHD .\n\n\n4.3.3 Clustering\nThe resulting distance matrix from OM seq.om indicates the degree of similarity between individual sequences. To identify representative trajectories, we then need to a way to group together similar sequences to produce a typology, in this case of population decline trajectories. Unsupervised cluster analysis is generally used for this task. Trusting you have built an understanding of cluster analysis from the previous chapter, we will not provide an elaborate description here. If you would like to know more about cluster analysis, we recommend the introductory book by Kaufman and Rousseeuw (2009). We use a clustering method called k-meloids . This methods is known to be more robust to noise and outliers than the conventional k-means procedure (Backman, Lopez, and Rowe 2020). This is because the medoid algorithm clusters the data by minimising a sum of pair-wise dissimilarities (Kaufman and Rousseeuw 2009), rather than a sum of squared Euclidean distances. We run cluster analyses at different numbers of k starting from 2 to 20.\n\n# run PAMs\nfor (k in 2:20)\n  pam_sol &lt;- pam(seq.om, k)\n\nWe then seek to determine the optimal number of clusters k. We use silhouette scores, but as we noted Chapter 3, the optimal number of clusters is better determined by the user given the context and use case. It is an art. There is no wrong or right answer. As can be seen from the results below from the average silhouette score, two clusters is suggested as the optimal solution. However, we could argue that we gain very little from such coarse partition of the data. We suggest to take this as guidance and a starting point to look to identify an appropriate data partition. We suggest to visualise different solution and gain an understanding of what data get split and decide on whether the resulting patterns contribute to the understanding of the process at hand.\n\n# compute average silhouette scores for all 20 cluster solutions\nasw &lt;- numeric(20)\nfor (k in 2:20)\n  asw[k] &lt;- pam(seq.om, k) $ silinfo $ avg.width\n  k.best &lt;- which.max(asw)\n  cat(\"silhouette-optimal number of clusters:\", k.best, \"\\n\")\n\nsilhouette-optimal number of clusters: 2 \n\n  asw\n\n [1] 0.0000000 0.5729062 0.5363097 0.4923203 0.4629785 0.4425575 0.4417958\n [8] 0.4559577 0.4597152 0.4569222 0.4682526 0.4832332 0.4359832 0.4365490\n[15] 0.4249453 0.4193369 0.4317815 0.4323037 0.4333972 0.4463586\n\n\nWe rerun and save the results for a 7k cluster partition. If you inspect the resulting data frame, it provides an identifier for each cluster. Each individual area is attributed to a cluster. Next the question that we seek to answer is what sort of pattern do these clusters capture?\n\n# rerun pam for k=7\npam_optimal &lt;- pam(seq.om, 7)\n\n\n\n4.3.4 Visualising\nTo understand the representative patterns captured in our data partition, we use visualisation. There is a battery of different visualisation tools to extract information and identify distinctive features of the identified trajectories. We start by using individual sequence plots by trajectory type. They provide a visual representation of how individual areas in each trajectory type moves between states. Recall that we are capturing representative trajectories; hence, there is still quite a bit of variability in terms of the patterns encapsulated in each representative trajectory. Back to the individual sequence plots, each line in these plots represents an area. Time is displayed horizontally and colours encode different states - in our case of population change. Numbers on the y-axis display the number of areas in each cluster. The figure immediate below relies on the base plot library, and by default, it is not very visually appealing.\n\n# create individual sequence plots\npar(mar=c(1,1,1,1))\nseqplot(seq.cl, \n        group = pam_optimal$clustering,\n        type = \"I\",\n        border = NA, \n        cex.axis = 1.5, \n        cex.lab = 1.5,\n        sortv = seq.om)\n\n\n\n\nWe therefore switch to the R library ggseqplot which enables visualisation of sequence data based on ggplot functionalities. This package may provide more flexibility if we are more familiar with ggplot.\nThe figure below offers a clear representation of the systematic sequencing of states that each trajectory captures. It provides information on two key features of trajectories: sequencing and size. For example, trajectory 1 seems to capture a sequencing pattern of transitions from moderate population decline to stability and back to moderate decline. Trajectory 2 shows a pattern high population decline during the first few years and then consistent moderate decline. Trajectory 3 displays a predominant pattern of moderate population decline. Trajectory 4 represents patterns of areas experiencing decline with spells of high population decline. Trajectory 5 shows a pattern of decline in the first few years followed by moderate decline and decline again. Trajectory 6 shows a similar pattern with more prevalent spells of population decline across the entire period. Trajectory 7 displays a trend of temporary decline, with spells of population growth and stability. From these plots, you can also identify which trajectories tend to be more common. In our example, trajectory and 3 accounts the largest number of areas: 189.\n\n# create individual sequence plots based on ggplot\nggseqiplot(seq.cl, \n        group = pam_optimal$clustering,\n        sortv = seq.om,\n        facet_ncol = 4) +\n  scale_fill_manual(values = rev(cols)) +\n  scale_color_manual(values = rev(cols)) \n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\nWe can also get a better understanding of the resulting trajectories by analysing state frequency plots. They show the number of occurrences of a given state in individual years. These plots examine the data from a vertical perspective i.e. looking at individual years across areas, rather than at individual areas over time. State frequency plots reveal that predominant states in each year and changes in their prevalence. Focusing on trajectory 1, for example, we observe that moderate decline was the predominant state between 2000 and 2007 and stability became equally prevalent during 2008 and 2015.\n\n# create state frequency plots based on ggplot\nggseqdplot(seq.cl, \n        group = pam_optimal$clustering,\n        facet_ncol = 4) +\n  scale_fill_manual(values = cols) +\n  scale_color_manual(values = cols) \n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\nWe can also examine time spent in individual states in each trajectory. Time spent plots report the average time spent in each state. The measure of time depends on the original data used in the analysis. We use years so the y-axis refers to the average number of years that a given status appears in a representative trajectory type. For example, a score over 5 for stable in trajectory 1 indicates that the average number of years that areas in that typology are classified in that category is over 5.\n\n# create time spent plots based on ggplot\nggseqmtplot(seq.cl, \n        group = pam_optimal$clustering,\n        facet_ncol = 4) +\n  scale_fill_manual(values = rev(cols)) +\n  scale_color_manual(values = rev(cols)) +\n  scale_x_discrete(labels=c(\"high_growth\" = \"HG\", \"growth\" = \"G\",\n                              \"moderate_growth\" = \"MG\", \"stable\" = \"S\", \"moderate_decline\" = \"MD\", \"decline\" = \"D\", \"high_decline\" = \"HD\" ))\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\nFinally we analyse entropy index plots. Entropy is a measure of diversity. The greater the score, the greater the entropy or diversity of states. The plot below displays the entropy index computed for individual trajectories each year. Each line represents the entropy index for a trajectory in each year. The top yellow line in 2001 indicates that in 2001 areas following a trajectory 7 type were distributed across a larger number of states than any other trajectory, reflecting the fact that areas experiencing this trajectory moves between states of decline, stability and growth. In other word, it indicates that there was more diversity of states.\n\n# create entropy index plots based on ggplot\nggseqeplot(seq.cl, \n        group = pam_optimal$clustering) +\n  scale_colour_viridis_d()\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale."
  },
  {
    "objectID": "sequence-analysis.html#questions",
    "href": "sequence-analysis.html#questions",
    "title": "4  Sequence Analysis",
    "section": "4.4 Questions",
    "text": "4.4 Questions\nFor the first assignment, we will continue to focus on London as our area of analysis. We will use population count estimates from the Office of National Statistics (ONS). The dataset provides information on area, population numbers and population density at national, regional and smaller sub-national area level, including Unitary Authority, Metropolitan County, Metropolitan District, County, Non-metropolitan District, London Borough, Council Area and Local Government District for the period from 2001 to 2020.\n\npop_df &lt;- read_csv(\"./data/sequence-analysis/population_uk/population-uk-2011_20.csv\")\npop_df\n\n# A tibble: 420 × 44\n   Code      Name              Geography   `Area (sq km)` Estimated population…¹\n   &lt;chr&gt;     &lt;chr&gt;             &lt;chr&gt;                &lt;dbl&gt;                  &lt;dbl&gt;\n 1 K02000001 UNITED KINGDOM    Country           242741.                67081234\n 2 K03000001 GREAT BRITAIN     Country           228948.                65185724\n 3 K04000001 ENGLAND AND WALES Country           151047.                59719724\n 4 E92000001 ENGLAND           Country           130310.                56550138\n 5 E12000001 NORTH EAST        Region              8581.                 2680763\n 6 E06000047 County Durham     Unitary Au…         2226.                  533149\n 7 E06000005 Darlington        Unitary Au…          197.                  107402\n 8 E06000001 Hartlepool        Unitary Au…           93.7                  93836\n 9 E06000002 Middlesbrough     Unitary Au…           53.9                 141285\n10 E06000057 Northumberland    Unitary Au…         5020.                  323820\n# ℹ 410 more rows\n# ℹ abbreviated name: ¹​`Estimated population mid-2020`\n# ℹ 39 more variables: `2020 people per sq. km` &lt;dbl&gt;,\n#   `Estimated Population mid-2019` &lt;dbl&gt;, `2019 people per sq. km` &lt;dbl&gt;,\n#   `Estimated Population mid-2018` &lt;dbl&gt;, `2018 people per sq. km` &lt;dbl&gt;,\n#   `Estimated Population mid-2017` &lt;dbl&gt;, `2017 people per sq. km` &lt;dbl&gt;,\n#   `Estimated Population mid-2016` &lt;dbl&gt;, `2016 people per sq. km` &lt;dbl&gt;, …\n\n\nFor the assignment, you should only work with smaller sub-national areas. Filter out country and regional area. You should address the following questions:\n\nUse sequence analysis to identify representative trajectories of population change and discuss the type of trajectories identified in London Boroughs.\nUse individual sequence plot to identify distinctive features in the resulting trajectories.\n\nFor the analysis, aim to focus on the area of London so you can link your narrative to the rest of analyses you will be conducting.\nEnsure you justify the number of optimal clusters you will use in your analysis and provide a brief description of the trajectories identified. Describe how they are unique.\n\n\n\n\nAbbott, Andrew, and Angela Tsay. 2000. “Sequence Analysis and Optimal Matching Methods in Sociology: Review and Prospect.” Sociological Methods & Research 29 (1): 3–33.\n\n\nBackman, Mikaela, Esteban Lopez, and Francisco Rowe. 2020. “The Occupational Trajectories and Outcomes of Forced Migrants in Sweden. Entrepreneurship, Employment or Persistent Inactivity?” Small Business Economics 56 (3): 963–83. https://doi.org/10.1007/s11187-019-00312-z.\n\n\nFielding, A. J. 1992. “Migration and Social Mobility: South East England as an Escalator Region.” Regional Studies 26 (1): 1–15. https://doi.org/10.1080/00343409212331346741.\n\n\nGabadinho, Alexis, Gilbert Ritschard, Nicolas S. Müller, and Matthias Studer. 2011. “Analyzing and Visualizing State Sequences inRwithTraMineR.” Journal of Statistical Software 40 (4). https://doi.org/10.18637/jss.v040.i04.\n\n\nGabadinho, Alexis, Gilbert Ritschard, Matthias Studer, and Nicolas S Müller. 2009. “Mining Sequence Data in r with the TraMineR Package: A User’s Guide.” Geneva: Department of Econometrics and Laboratory of Demography, University of Geneva.\n\n\nGonzález-Leonardo, Miguel, Niall Newsham, and Francisco Rowe. 2023. “Understanding Population Decline Trajectories in Spain Using Sequence Analysis.” Geographical Analysis, January. https://doi.org/10.1111/gean.12357.\n\n\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in Data: An Introduction to Cluster Analysis. John Wiley & Sons.\n\n\nNewsham, Niall, and Francisco Rowe. 2022a. “Understanding the Trajectories of Population Decline Across Rural and Urban Europe: A Sequence Analysis.” https://doi.org/10.48550/ARXIV.2203.09798.\n\n\n———. 2022b. “Understanding Trajectories of Population Decline Across Rural and Urban Europe: A Sequence Analysis.” Population, Space and Place, December. https://doi.org/10.1002/psp.2630.\n\n\nPatias, Nikos, Francisco Rowe, and Dani Arribas-Bel. 2021. “Trajectories of Neighbourhood Inequality in Britain: Unpacking Inter-Regional Socioeconomic Imbalances, 1971-2011.” The Geographical Journal 188 (2): 150–65. https://doi.org/10.1111/geoj.12420.\n\n\nPatias, Nikos, Francisco Rowe, Stefano Cavazzi, and Dani Arribas-Bel. 2021. “Sustainable Urban Development Indicators in Great Britain from 2001 to 2016.” Landscape and Urban Planning 214 (October): 104148. https://doi.org/10.1016/j.landurbplan.2021.104148.\n\n\nRowe, Francisco, and Dani Arribas-Bel. 2022. “Spatial Modelling for Data Scientists.” Open Science Framework, August. https://doi.org/10.17605/OSF.IO/8F6XR.\n\n\nRowe, Francisco, Jonathan Corcoran, and Martin Bell. 2016. “The Returns to Migration and Human Capital Accumulation Pathways: Non-Metropolitan Youth in the School-to-Work Transition.” The Annals of Regional Science 59 (3): 819–45. https://doi.org/10.1007/s00168-016-0771-8.\n\n\nSalmela-Aro, Katariina, Noona Kiuru, Jari-Erik Nurmi, and Mervi Eerola. 2011. “Mapping Pathways to Adulthood Among Finnish University Students: Sequences, Patterns, Variations in Family- and Work-Related Roles.” Advances in Life Course Research 16 (1): 25–41. https://doi.org/10.1016/j.alcr.2011.01.003.\n\n\nTatem, Andrew J. 2017. “WorldPop, Open Data for Spatial Demography.” Scientific Data 4 (1). https://doi.org/10.1038/sdata.2017.4.\n\n\nTurok, Ivan, and Vlad Mykhnenko. 2007. “The Trajectories of European Cities, 19602005.” Cities 24 (3): 165–82. https://doi.org/10.1016/j.cities.2007.01.007."
  },
  {
    "objectID": "network.html#sec-sec_dependencies",
    "href": "network.html#sec-sec_dependencies",
    "title": "5  Network Analysis",
    "section": "5.1 Dependencies",
    "text": "5.1 Dependencies\nTo run the code in the rest of this workbook, we will need to load the following R packages:\n\n#Support for simple features, a standardised way to encode spatial vector data\nlibrary(sf)\n#Data manipulation\nlibrary(dplyr)\n# An R package for network manipulation and analysis\nlibrary(igraph)\n# Provides a number of useful functions for working with character strings in R\nlibrary(stringr)"
  },
  {
    "objectID": "network.html#sec-sec_data",
    "href": "network.html#sec-sec_data",
    "title": "5  Network Analysis",
    "section": "5.2 Data",
    "text": "5.2 Data\n\n5.2.1 Internal migration in the UK estimated from Twitter data\nIn this Chapter we will be looking at data derived from Twitter, now known as X. In particular, we will use the file internal_migration_uk.csv, which contains migration data between UK local authorities in two consecutive months. The dataset was originally created to analyse internal migration movements before and during COVID-19. For more details on the methodology and the results of the analysis, you can check the paper (Wang et al. 2022).\n\n\n5.2.2 Import the data\nBefore importing the data, ensure to set the path to the directory where you stored it. Please modify the following line of code as needed.\n\ndf &lt;- read.csv(\"./data/networks/internal_migration_uk.csv\")\n\nIn this practical, we will only analyse population movements between April 2019 and May 2019, so we can filter the dataset accordingly.\n\ndf &lt;- df %&gt;% filter(month_start=='2019-04') \n\nEach row corresponds to an origin-destination pair, so we can obtain the total number of reported migratory movements with the following command:\n\nnrow(df)\n\n[1] 4271",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "network.html#sec-sec_create",
    "href": "network.html#sec-sec_create",
    "title": "5  Network Analysis",
    "section": "5.3 Creating networks",
    "text": "5.3 Creating networks\nBefore we start to analyse the data introduced in Section 5.2, let us first take a step back to consider the main object of study of this Chapter: the so-called networks. In the most general sense, a network (also known as a graph) is a structure formed by a set of objects which may have some connections between them. The objects are represented by nodes (a.k.a. vertices) and the connections between these objects are represented by edges (a.k.a. links). Networks are used as a tool to conceptualise many real-life contexts, such as the friendships between the members of a year group at school, the direct airline connections between cities in a continent or the presence of hyperlinks between a set of websites. In this session, we will use networks to model the migratory flows between locations.\n\n5.3.1 Starting from the basics\nIn order to create, manipulate and analyse networks in R, we will use the igraph package, which we imported in Section 5.2. We start by creating a very simple network with the code below. The network contains five nodes and five edges and it is undirected, so the edges do not have orientations. The nodes and edges could represent, respectively, a set of cities and the presence of migration flows between these cities in two consecutive years.\n\n# Create an undirected network with 5 nodes and 5 edges\n# The number of nodes is given by argument n\n# In this case, the node labels or IDs are represented by numbers 1 to 5\n# The edges are specified as a list of pairs of nodes\ng1 &lt;- graph( edges=c(1,2, 1,4, 2,3, 2,4, 4,5), n=5, directed=F ) \n\n# A simple plot of the network allows us to visualise it\nplot(g1) \n\n\n\n\n\n\n\n\nIf the connections between the nodes of a network are non-reciprocal, the network is called directed. For example, this could correspond to a situation where there are people moving from city 1 to city 2, but nobody moving from city 2 to city 1. Note that in the code below we have not only added directions to the edges, but we have also added a few additional parameters to the plot function in order to customise the diagram.\n\n# Creates a directed network with 7 nodes and 6 edges \n# Note that we now have edge 1,4 and edge 4,1 and that 2 of the nodes are isolated\ng2 &lt;- graph( edges=c(1,2, 1,4, 2,3, 4,1, 4,2, 4,5), n=7, directed=T ) \n\n# A simple plot of the network with a few extra features\nplot(g2, vertex.frame.color=\"red\",  vertex.label.color=\"black\",\nvertex.label.cex=0.9, vertex.label.dist=2.3, edge.curved=0.3, edge.arrow.size=.5, edge.color = \"blue\", vertex.color=\"yellow\", vertex.size=15) \n\n\n\n\n\n\n\n\nThe network can also be defined as a list containing pairs of named nodes. Then, it is not necessary to specify the number of nodes but the isolated nodes have to be included. The following code generates a network which is equivalent to the one above.\n\ng3 &lt;- graph( c(\"City 1\",\"City 2\", \"City 2\",\"City 3\", \"City 1\",\"City 4\",  \"City 4\",\"City 1\",  \"City 4\",\"City 2\", \"City 4\",\"City 5\"), isolates=c(\"City 6\", \"City 7\") ) \nplot(g3, vertex.frame.color=\"red\",  vertex.label.color=\"black\",\nvertex.label.cex=0.9, vertex.label.dist=2.3, edge.curved=0.3, edge.arrow.size=.5, edge.color = \"blue\", vertex.color=\"yellow\", vertex.size=15) \n\n\n\n\n\n\n\n\n\n\n5.3.2 Adding attributes\nIn R, we can add attributes to the nodes, edges and the network. To add attributes to the nodes, we first need to access them via the following command:\n\nV(g3)\n\n+ 7/7 vertices, named, from 4fa653c:\n[1] City 1 City 2 City 3 City 4 City 5 City 6 City 7\n\n\nThe node attribute name is automatically generated from the node labels that we manually assigned before.\n\nV(g3)$name\n\n[1] \"City 1\" \"City 2\" \"City 3\" \"City 4\" \"City 5\" \"City 6\" \"City 7\"\n\n\nBut other node attributes could be added. For example, the current population of the cities represented by the nodes:\n\nV(g3)$population &lt;- c(134000, 92000, 549000, 1786000, 74000, 8000, 21000)\n\nSimilarly, we can access the edges:\n\nE(g3)\n\n+ 6/6 edges from 4fa653c (vertex names):\n[1] City 1-&gt;City 2 City 2-&gt;City 3 City 1-&gt;City 4 City 4-&gt;City 1 City 4-&gt;City 2\n[6] City 4-&gt;City 5\n\n\nand add edge attributes, such as the number of people moving from an origin to a destination city in two consecutive years. We call this attribute the weight of the edge, since if there is a lot of people going from one city to another, the connection between these cities has more importance or “weight” in the network.\n\nE(g3)$weight &lt;- c(2000, 3000, 5000, 1000, 1000, 4000)\n\nWe can examine the adjacency matrix of the network, which represents the presence of edges between different pairs of nodes. In this case, each row corresponds to an origin city and each column to a destination:\n\ng3[] #The adjacency matrix of network g3\n\n7 x 7 sparse Matrix of class \"dgCMatrix\"\n       City 1 City 2 City 3 City 4 City 5 City 6 City 7\nCity 1      .   2000      .   5000      .      .      .\nCity 2      .      .   3000      .      .      .      .\nCity 3      .      .      .      .      .      .      .\nCity 4   1000   1000      .      .   4000      .      .\nCity 5      .      .      .      .      .      .      .\nCity 6      .      .      .      .      .      .      .\nCity 7      .      .      .      .      .      .      .\n\n\nWe can also look at the existing node and edge attributes.\n\nvertex_attr(g3) #Node attributes of g3. Use edge_attr() to access the edge attributes\n\n$name\n[1] \"City 1\" \"City 2\" \"City 3\" \"City 4\" \"City 5\" \"City 6\" \"City 7\"\n\n$population\n[1]  134000   92000  549000 1786000   74000    8000   21000\n\n\nFinally, it is possible to add network attributes\n\ng3$title &lt;- \"Network of migration between cities\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "network.html#sec-sec_reading",
    "href": "network.html#sec-sec_reading",
    "title": "5  Network Analysis",
    "section": "5.4 Reading networks from data files",
    "text": "5.4 Reading networks from data files\n\n5.4.1 Preparing the data to create an igraph object\nAt the beginning of the chapter, we defined a data frame called df based on some imported data from movements between different UK local authority districts. This is a data frame containing 4,271 rows, but how can we turn this data frame into a network similar to the ones that we generated in Section 5.3. The igraph function graph_from_data_frame() can do this for us. To find out more about this function, we can run the following command:\n\nhelp(\"graph_from_data_frame\")\n\nAs we can see, the input data for graph_from_data_frame() needs to be in a certain format which is different from our migration data frame. In particular, the function requires three arguments: 1) d, which is a data frame containing an edge list in the first two columns and any additional columns are considered as edge attributes; 2) vertices, which is either NULL or a data frame with vertex metadata (i.e. vertex attributes); and 3) directed, which is a boolean argument indicating whether the network is directed or not. Our next task is therefore to obtain 1) and 2) from the migration data frame called df.\nLet us start with argument 1). Each row in df will correspond to an edge in the migration network since it contains information about a pair of origin and destination cities for two consecutive years. The names of the origin and destination LAs are given by the columns in df called origin_LAD and destiantion_LAD. In addition, the column called flow gives the number of people moving between the origin and the destination cities, so this will be the weight attribute of each edge in the migration network. Hence, we can define a data frame of edges which we will call df_edges that conforms with the format required by the argument 1) as follows:\n\n#The pipe operator used below and denoted by %&gt;% is a feature of the magrittr package, it takes the output of one function and passes it into another function as an argument\n\n# Creates the df_edges data frame with data from df and renames the columns as \"origin\", \"destination\" and \"weight\"\ndf_edges &lt;- data.frame(df$origin_LAD, df$destination_LAD, df$flow) %&gt;%\n  rename(origin = df.origin_LAD, destination = df.destination_LAD, weight = df.flow) \n\nFor argument 2) we can define a data frame of nodes which we will call df_nodes, where each row will correspond to a unique node or city. To obtain all the unique local authorities from df, we can firstly obtain a data frame of unique origin LAs, then a data frame of unique LAs, and finally, apply the full_join() function to these two data frames to obtain their union, which will be df_nodes. The name of the unique cities in df_nodes is in the column called label, the other columns can be seen as the nodes metadata.\n\ndf_unique_origins &lt;- df_edges %&gt;% \n  distinct(origin) %&gt;%\n  rename(name = origin) \n\ndf_unique_destinations &lt;- df_edges %&gt;%\n  distinct(destination) %&gt;%\n  rename(name = destination)\n\ndf_nodes &lt;- full_join(df_unique_origins, df_unique_destinations, by = \"name\")\n\nFinally, a directed migration network can be obtained with the following line of code. It should contain 373 nodes and 4,271 edges. You can test this yourself with the functions that you learnt in Section 5.3.\n\ng &lt;- graph_from_data_frame(d = df_edges,\n                                       vertices = df_nodes,\n                                       directed = TRUE)\n\nIf we try to plot the network g containing the migratory movements between UK Local Authorities with the plot() function as we did before, we obtain a result which is rather undesirable…\n\nplot(g)\n\n\n\n\n\n\n\n\n\n\n5.4.2 Filtering the data to create a subgraph\nWe will dedicate the entirety of next section to explore tools that can help us improve the visualisation of networks, since it is one of the most important aspects of network analysis. To facilitate the visualisation in the examples shown in Section 5.5, we will work with a subset of the full network g. A way to create a subnetwork is to filter the original data frame. In particular, we will filter df to only include cities from a particular region, in this case, the North West of England. To filter, we use the filter() function. In this case, we are filtering the dataset so that it only contains data corresponding to local authorities in Greater Manchester.\n\nManchester_LAs &lt;- c('Bolton', 'Bury', 'Manchester', 'Oldham', 'Rochdale', 'Salford', 'Stockport', 'Tameside', 'Trafford', 'Wigan')\n\nThen, we can prepare the data as we did before to create g. But, instead of basing the network on df, we will generate it from df_sub.\n\n# Create a new data frame containing the columns for the origin, destination, and weight from df_sub\n# Rename the columns to origin, destination, and weight respectively\ndf_sub_edges &lt;- df_edges %&gt;% filter(origin %in% Manchester_LAs) %&gt;% filter(destination %in% Manchester_LAs)\n\ndf_sub_unique_origins &lt;- df_sub_edges %&gt;% \n  distinct(origin) %&gt;%\n  rename(name = origin) \n\ndf_sub_unique_destinations &lt;- df_sub_edges %&gt;%\n  distinct(destination) %&gt;%\n  rename(name = destination)\n\ndf_sub_nodes &lt;- full_join(df_sub_unique_origins, df_sub_unique_destinations, by = \"name\")\n\n# Create a graph object from the edges and nodes data frames\ng_sub &lt;- graph_from_data_frame(d = df_sub_edges,\n                                       vertices = df_sub_nodes,\n                                       directed = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "network.html#sec-sec_visualise",
    "href": "network.html#sec-sec_visualise",
    "title": "5  Network Analysis",
    "section": "5.5 Network visualisation",
    "text": "5.5 Network visualisation\n\n5.5.1 Visualisation with igraph\nLet us start by generating the most basic visualisation of g_sub.\n\nplot(g_sub)\n\n\n\n\n\n\n\n\nThis plot can be improved by changing adding a few additional arguments to the plot() function. For example, by just changing the color and size of the labels, the color and size of the nodes and the arrow size of the edges, we can already see some improvements.\n\nplot(g_sub, vertex.size=10, edge.arrow.size=.2, edge.curved=0.1,\nvertex.color=\"gold\", vertex.frame.color=\"black\",\nvertex.label=V(g_sub)$name, vertex.label.color=\"black\",\nvertex.label.cex=.65)\n\n\n\n\n\n\n\n\nBut there are few more things we can do not only to improve the look of the diagram, but also to include more information about the network. For example, we can set the size of the nodes so that it reflects the total number of people that the corresponding cities receive. We can do this by adding a new node attribute, inflow, which is obtained as the sum of the rows of the adjacency matrix of g_sub.\n\nV(g_sub)$inflow &lt;- rowSums(as.matrix(g_sub[]))\n\nBelow we set the node size based on the inflow attribute. Note the formula \\(2\\times(V(g\\_sub)\\$inflow)^{0.5}\\), where the power of 0.5 is chosen to scale the size of the nodes in such a way that the largest ones do not get excessively large and the smallest ones do not get excessively small. We also set the edge width based on its weight, which is the total number of people migrating from the origin and destination cities that it connects.\n\n# Set node size based on inflow of migrants:\nV(g_sub)$size &lt;- 2*((V(g_sub)$inflow)^0.5)\n# Set edge width based on weight:\nE(g_sub)$width &lt;- E(g_sub)$weight/6\n\nRun the code below to discover how the aspect of the network has significantly improved with the modifications that we have introduced above.\n\nplot(g_sub, vertex.size=V(g_sub)$size, edge.arrow.size=.1, edge.arrow.width=9, edge.curved=0.1, edge.width=E(g_sub)$width, edge.color =\"gray80\",\nvertex.color=\"gold\", vertex.frame.color=\"gray90\",\nvertex.label=V(g_sub)$name, vertex.label.color=\"black\",\nvertex.label.cex=.65)\n\n\n\n\n\n\n\n\n\n\n5.5.2 Visualisation of spatial networks\nFirstly, we will import geographical data for the local authority districts in the whole of the UK, using the sf package. Here, we are only interested in the Local Authorities in Greater Manchester so we will filter the data frame LAs to keep only the metropolitan areas, i.e. those entries where the value in column LAD222NM is a local authority from Greater Manchester\n\n# Import LA areas https://geoportal.statistics.gov.uk/\nLAs &lt;- st_read(\"./data/networks/Local_Authority_Districts_(December_2022)_Boundaries_UK_BFC/LAD_DEC_2022_UK_BFC.shp\")\n\nReading layer `LAD_DEC_2022_UK_BFC' from data source \n  `/Users/carmen/Documents/GitHub/r4ps/data/networks/Local_Authority_Districts_(December_2022)_Boundaries_UK_BFC/LAD_DEC_2022_UK_BFC.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 374 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -116.1928 ymin: 5336.966 xmax: 655653.8 ymax: 1220302\nProjected CRS: OSGB36 / British National Grid\n\n\nSince we are focusing on Greater Manchester, let us filter LAs so that it only includes data from Greater Manchester:\n\nLAs_sub &lt;- LAs %&gt;% filter(LAD22NM %in% Manchester_LAs )\n\nWe will now find the centroid of each LA polygon and add columns to LAs_sub for the longitude and latitude of each centroid.\n\n# Add longitude and latitude corresponding to centroid of each LA polygon\nLAs_sub$lon_centroid &lt;- st_coordinates(st_centroid(LAs_sub$geometry))[,\"X\"]\nLAs_sub$lat_centroid &lt;- st_coordinates(st_centroid(LAs_sub$geometry))[,\"Y\"]\n\nWe can now plot the polygons for the LAs belonging to Greater Manchester as well as the centroids:\n\nplot(st_geometry(LAs_sub))\nplot(st_centroid(LAs_sub$geometry), add=TRUE, col=\"red\", cex=0.5, pch=20)\n\n\n\n\n\n\n\n\nHowever, we still need to link this geographic data to the network data that we obtained before. In order to incorporate the geographic information to the nodes of the migration subnetwork, we can join data from two data frames: LAs_sub, which contains the geographic data, and df_sub_nodes, which contains the names of the nodes. To do this, we can use the function left_join() and then, select only the columns of interest. For more information on this magical function, check this link.\n\n# Join the data frame of nodes df_sub_nodes with the geographic information of the centroid of each LA\ndf_sub_spatial_nodes &lt;- df_sub_nodes %&gt;% left_join(LAs_sub, by = c(\"name\" = \"LAD22NM\")) %&gt;% select(c(\"name\", \"lon_centroid\", \"lat_centroid\"))\n\nNow, instead of using igraph to plot the graph, we will be using ggraph. This interacts nicely with other packages to plot data and maps, such as ggplot2 and ggmap. Before we proceed with plotting the graph, let’s specify the coordinates of the nodes by defining a custom layout according to the information in df_sub_spatial_nodes.\n\ncustom_layout &lt;- data.frame(\n  name = df_sub_spatial_nodes$name,  # Node names from the graph\n  x = df_sub_spatial_nodes$lon_centroid,    # Custom x-coordinates\n  y = df_sub_spatial_nodes$lat_centroid     # Custom y-coordinates\n)\n\nWe are now ready to plot:\n\n# Plot the graph 'g_sub' with specific visual attributes\nplot &lt;- ggraph(as_tbl_graph(g_sub), custom_layout) + # basic graph plot with custom layout\n  geom_edge_link(color = \"gray80\", alpha=0.9, aes(width = E(g_sub)$weight)) + # custom edges\n  scale_edge_width(range = c(.5, 3)) + # scale edge size\n  geom_node_point(aes(color = \"gold\", size = V(g_sub)$size)) + # custom nodes\n  scale_size_continuous(range = c(1, 5)) + # scale node size\n  geom_node_text(aes(label = name), size=2.5,  repel = TRUE) + # custom node labels\n  scale_color_identity() + # scale node color (not relevant for this plot, but could be for others)\n  theme(legend.position = \"none\", panel.background=element_rect(fill = NA, colour = NA)) + # map legend and background color\n  geom_sf(data = LAs_sub, fill = NA, color = \"black\") + # basic map plot\n  labs(title = \" \") + # title for plot\n  ggspatial::annotation_scale(location = 'tl') + # scale bar\n  ggspatial::annotation_north_arrow(location = 'br') # north arrow\n\nplot\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\n\n\n\n\n5.5.3 Alternative visualisations\nIn this session we have based our visualisations on igraph, however, there exist a variety of packages that would also allow us to generate nice plots of networks.\nFor example, migration networks are particularly well-suited to be represented as a chord diagram. If you want to explore this type of visualisation, you can find further information on the official R documentation and also, for example, on this other link link.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "network.html#sec-sec_metrics",
    "href": "network.html#sec-sec_metrics",
    "title": "5  Network Analysis",
    "section": "5.6 Network metrics",
    "text": "5.6 Network metrics\nHere we define some of the most important metrics that help us quantify different characteristics of a network. We will use the migration network for the whole of the UK again, g. It has more nodes and edges than g_sub and consequently, its behaviour is richer and helps us illustrate better the concepts that we introduce in this section.\n\n5.6.1 Density\nThe network density is defined as the proportion of existing edges out of all the possible edges. In a network with \\(n\\) nodes, the total number of possible edges is \\(n\\times(n-1)\\), i.e. the number of edges if each node was connected to all the other nodes. A density equal to \\(1\\) corresponds to a situation where \\(n\\times(n-1)\\) edges are present. A network with no edges at all would have density equal to \\(0\\). The line of code below tells us that the density of g is approximately 0.03, meaning that about 3% of all the possible edges are present, or in other words, that there are migratory movements between almost half of every pair of cities.\n\nedge_density(g, loops=FALSE)\n\n[1] 0.03078065\n\n\n\n\n5.6.2 Reciprocity\nThe reciprocity in a directed network is the proportion of reciprocated connections between nodes (i.e. number of pairs of nodes with edges in both directions) from all the existing edges.\n\nreciprocity(g)\n\n[1] 0.2701943\n\n\nFrom this result, we conclude that about 27% of the pairs of nodes that are connected have edges in both directions.\n\n\n5.6.3 Degree\nThe total degree of a node refers to the number of edges that emerge from or point at that node. The in-degree of a node in a directed network is the number of edges that point at it whereas the out-degree is the number of edges that emerge from it. The degree() functions, allows us to compute the degree of one or more nodes and allows us to specify if we are interested in the total degree, the in-degree or the out-degree.\n\n# Compute degree of the nodes given by v belonging to graph g_US, in this case the in-degree\ndeg &lt;- degree(g, v=V(g), mode=\"in\")\n\n# Produces histogram of the frequency of nodes with a certain in-degree\nhist(deg, breaks = 50, main=\"Histogram of node in-degree\")\n\n\n\n\n\n\n\n\nAs we can see in the histogram, most LAs receive immigrants from 4-6 LAs. Very few LAs receive immigrants from 60 or more LAs. We can check which is the LA with the maximum in-degree.\n\nV(g)$name[degree(g, mode=\"in\")==max(degree(g, mode=\"in\"))]\n\n[1] \"Liverpool\"\n\n\nThe LA with the highest in-degree is Liverpool. The in-degree is 87 as we can see below.\n\ndegree(g, v=c(\"Liverpool\"), mode=\"in\")\n\nLiverpool \n       87 \n\n\nNote that the fact that these two cities have the largest in-degree does not necessarily mean that they are the ones receiving the largest number of migrants.\n\n\n5.6.4 Distances\nA path in a network between node \\(A\\) and node \\(B\\) is a sequence of edges which joins a sequence of distinct nodes, starting at node \\(A\\) and terminating at node \\(B\\). In a directed path there is an added restriction: the edges must be all directed in the same direction.\nThe length of a path between nodes \\(A\\) and \\(B\\) is normally defined as the number of edges that form the path. The shortest path is the minimum number of edges that need to be traversed to travel from \\(A\\) to \\(B\\).\nThe length of a path can also be defined in other ways. For example, if the edges are weighted, it can be defined as the sum of the weights of the edges that form the path.\nIn R, we can use the function shortest_paths() to find the shortest path between a given pair of nodes and its length. For example, below we can see that the shortest path between Kensington and Chelsea and Liverpool is one, meaning that there are people migrating between these two locations.\n\nshortest_paths(g, \nfrom = V(g)$name==\"Kensington and Chelsea\",\nto = V(g)$name==\"Liverpool\",\nweights=NA, #If weights=NULL and the graph has a weight edge attribute, then the weigth attribute is used. If this is NA then no weights are used (even if the graph has a weight attribute)\noutput = \"both\") # outputs both path nodes and edges\n\n$vpath\n$vpath[[1]]\n+ 2/373 vertices, named, from ab90db0:\n[1] Kensington and Chelsea Liverpool             \n\n\n$epath\n$epath[[1]]\n+ 1/4271 edge from ab90db0 (vertex names):\n[1] Kensington and Chelsea-&gt;Liverpool\n\n\n$predecessors\nNULL\n\n$inbound_edges\nNULL\n\n\nOf all shortest paths in a network, the length of the longest one is defined as the diameter of the network. In this case, the diameter is 6 meaning that the longest of all shortest paths in g has 6 edges.\n\ndiameter(g, directed=TRUE, weights=NA)\n\n[1] 6\n\n\nThe mean distance is defined as the average length of all shortest paths in the network. The mean distance will always be smaller or equal than the diameter.\n\nmean_distance(g, directed=TRUE, weights=NA)\n\n[1] 2.671381\n\n\n\n\n5.6.5 Centrality\nCentrality metrics assign scores to nodes (and sometimes also edges) according to their position within a network. These metrics can be used to identify the most influential nodes.\nWe have already explored some concepts which can be regarded as centrality metrics, for example, the degree of a node or the weighted degree of a node, also known as the strength of a node, which is the sum of edge weights that link to adjacent nodes or, in other words, the in-flow or out-flow associated with each node. As we can see from the code below, many nodes in g have an in-flow of less than 100 immigrants. Note that we have set mode to c(“in”) for inflow and we have set the weights parameter to NULL since we want to know the sum of weights for the incoming edges and not just the total number of incoming edges.\n\n# Compute strength of the nodes belonging to graph g, in this case the in-flow\nstrength_g &lt;- strength(g, #The input graph\n  vids = V(g), #    The vertices for which the strength will be calculated.\n  mode = c(\"in\"), #“in” for in-degree\n  loops = FALSE, #whether the loop edges are also counted\n  weights = NULL #If the graph has a weight edge attribute, then this is used by default when weights=NULL. If the graph does not have a weight edge attribute and this argument is NULL, then a warning is given and degree is called.\n)\n  \n#Produce histogram of the frequency of nodes with a certain strength\nhist(strength_g, breaks = 50, main=\"Histogram of node strength\")\n\n\n\n\n\n\n\n\nWe can check which is the LA with the maximum strength:\n\nV(g)$name[strength(g, vids = V(g), mode = c(\"in\"), loops = FALSE, weights = NULL)==max(strength(g, vids = V(g), mode = c(\"in\"), loops = FALSE, weights = NULL))]\n\n[1] \"Liverpool\"\n\n\nWe will look at another two important centrality metrics that are based on the structure of the network. Firstly, closeness centrality which is a measure of the length of the shortest path between a node and all the other nodes. For a given node, it is computed as the inverse of the average shortest paths between that node and every other node in the network. So, if a node has closeness centrality close to \\(1\\), it means that on average, it is very close to the other nodes in the network. A closeness centrality of exactly \\(0\\) corresponds to an isolated node.\n\nclose_centr &lt;- closeness(g, mode=\"in\", weights=NA) #using unweighted edges\nhist(close_centr, breaks = 50, main=\"Histogram of closeness centrality\")\n\n\n\n\n\n\n\n\nThe other metric is known as betweenness centrality. For a given node, it is a measure of the number of shortest paths that go through that node. Therefore, nodes with high values of betweenness centrality are those that play a very important role in the connectivity of the network. Betweenness can also be computed for edges.\n\nbetween_centr &lt;- betweenness(g, v = V(g), directed = TRUE, weights = NA)\nhist(between_centr, breaks = 50, main=\"Histogram of betweenness centrality\")\n\n\n\n\n\n\n\n\n\n\n5.6.6 Hubs and authorities\nWe call hubs or authorities those nodes with a higher-than-average degree. Normally, the name hub is reserved to nodes with high out-degree whereas authority is reserved to nodes with high in-degree. An algorithm to detect hubs and authorities was developed by Jon Kleinberg, although it was initially used to examine web pages. Like we did for other network metrics, we can compute the hub score and then plot a histogram to see how this metric is distributed across the nodes of the network.\n\nhs &lt;- hub_score(g, weights=NULL)$vector #In this case, we use the weighted edges\nhist(hs, breaks = 50, main=\"Histogram of hub score\")\n\n\n\n\n\n\n\n\nSimilarly, we can explore the authority score for each node:\n\nas &lt;- authority_score(g, weights=NULL)$vector\nhist(as, breaks = 50, main=\"Histogram of authority score\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "network.html#questions",
    "href": "network.html#questions",
    "title": "5  Network Analysis",
    "section": "5.7 Questions",
    "text": "5.7 Questions\nIn this set of questions, as well as analysing population movements between April 2019 and May 2019, we will also explore population movements between April 2020 and May 2020, in the middle of COVID-19\n\ndf_covid &lt;- read.csv(\"./data/networks/internal_migration_uk.csv\")\ndf_covid &lt;- df_covid %&gt;% filter(month_start=='2020-04') \n\nWith this dataset, generate a network called g_covid analogous to g, but for the April-May 2020 time period.\nKnowing that the inner London local authorities are given by:\n\ninner_LND_LAs &lt;- c('Camden', 'City of London', 'Greenwich', 'Hackney',  'Hammersmith and Fulham', 'Islington', 'Kensington and Chelsea', 'Lambeth', 'Lewisham', 'Southwark', 'Tower Hamlets', 'Wandsworth', 'Westminster')\n\nprovide answers to the following essay questions:\n\nCreate a graph visualisation of the population movements between inner London local authorities that displays the changes in the period April-May 2020 with respect to the period April-May 2019. You can be as creative as you want. For example, you might create a visualisation of a directed graph on top of a map of inner London, where the nodes are the centroids of the local authorities and the edges between LAs are coloured according to the extent to which the flow of people has increased or decreased from one year to the next. You may find a hint on how to do this for example here. Other ideas are welcome. You may need to do a few Internet searches in order to be able to complete this question, however, you should be able to achieve the fundamentals with the libraries loaded for this session (you can always use more if you want!). Your output will be graded for clarity, aesthetics and the correctness of the displayed data. Do not forget to add clear legends, scale bars and North arrows.\nFurthermore, briefly comment on your findings. Do the patterns you observe agree with your intuition? You may suggest reasons for the observed patterns to arise.\nConsider the movements between Camden and the rest of local authorities in the country. Using the strength of a node, compute the net flow of people moving between Camden and the rest of the country, both in April-May 2019 and 2020. Comment on the difference in net flow between these two years.\n\n\n\n\n\nCabrera-Arnau, Carmen, Chen Zhong, Michael Batty, Ricardo Silva, and Soong Moon Kang. 2022. “Inferring Urban Polycentricity from the Variability in Human Mobility Patterns.” arXiv. https://doi.org/10.48550/ARXIV.2212.03973.\n\n\nNewman, Mark. 2018. Networks / Mark Newman. Second edition. Oxford: Oxford University Press.\n\n\nOgnyanova, K. 2016. “Network Analysis with r and Igraph: NetSci x Tutorial.” www.kateto.net/networks-r-igraph.\n\n\nPrieto Curiel, Rafael, Carmen Cabrera-Arnau, and Steven Richard Bishop. 2022. “Scaling Beyond Cities.” Frontiers in Physics 10. https://doi.org/10.3389/fphy.2022.858307.\n\n\nWang, Yikang, Chen Zhong, Qili Gao, and Carmen Cabrera-Arnau. 2022. “Understanding Internal Migration in the UK Before and During the COVID-19 Pandemic Using Twitter Data.” Urban Informatics 1 (1): 15.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "sentiment-analysis.html#dependencies",
    "href": "sentiment-analysis.html#dependencies",
    "title": "6  Sentiment Analysis",
    "section": "6.1 Dependencies",
    "text": "6.1 Dependencies\n\n# text data manipulation\nlibrary(tidytext) # text data tidy approach\nlibrary(tm) # creating data corpus\nlibrary(SnowballC) # stemming text\nlibrary(tidyverse) # manipulate data\n\n# sentiment analysis\nlibrary(vader)\n\n# download twitter data (no used)\nlibrary(rtweet)\n\n# design plots\nlibrary(patchwork)"
  },
  {
    "objectID": "sentiment-analysis.html#data",
    "href": "sentiment-analysis.html#data",
    "title": "6  Sentiment Analysis",
    "section": "6.2 Data",
    "text": "6.2 Data\nWe will use a sample of Twitter data on public opinion about migration originated in the United States during December 1st 2019 and May 1st 2020. They data were collected by Rowe, Mahony, Graells-Garrido, et al. (2021) to analyse changes in public opinion related to migration during the early stages of the COVID-19 pandemic. During this period, a rising number of anti-immigration sentiment incidents were reported across the world (“Stop the Coronavirus Stigma Now” 2020). Acts and displays of intolerance, discrimination, racism, xenophobia and violent extremism emerged linking individuals of Asian descendent and appearance to COVID-19 Coates (2020). In the United States, President Donald Trump repeatedly used the terms “Chinese Virus,” “China Virus,” and “Fung Flu” in reference to COVID-19. Fear mongering and racial stereotyping spread on social media and rapidly spilled onto the streets (Cowper 2020). In the United Kingdom, the government reported a 21% increase in hate crime incidents against Asian communities between January and March, and Chinese businesses reported a notorious reduction in footfall during Chinese celebrations (Home Affairs Committee 2020).\nData were collected via an application programming interface (API) using random sampling strategy. A set of key search terms were defined, including words, Twitter accounts and hashtags to collect the data. These search terms were developed in collaboration with the United Nations’ International Organization for Migration. See Rowe, Mahony, Graells-Garrido, et al. (2021) for details on the search strategy. For this chapter, Twitter users’ handles have been anonymised.\n\ntweets_df &lt;- readRDS(\"./data/sentiment-analysis/usa_tweets_01122019_01052020.rds\")\nglimpse(tweets_df)\n\nRows: 224,611\nColumns: 4\n$ id         &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ created_at &lt;dttm&gt; 2019-12-01 00:00:18, 2019-12-01 00:00:23, 2019-12-01 00:00…\n$ status_id  &lt;dbl&gt; 1.200928e+18, 1.200928e+18, 1.200928e+18, 1.200928e+18, 1.2…\n$ text       &lt;chr&gt; \"Another example @anonymous are the TRUE protectors of sanc…\n\n\n\n6.2.1 Text data structures\nDifferent approaches to storing and manipulating text data exist. General formats include:\nString: Text can be stored as strings, i.e., character vectors, within R, and often text data is first read into memory in this form.\nCorpus: These types of objects typically contain raw strings annotated with additional metadata and details.\nDocument-term matrix: This is a sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count.\nTidyText: This is a novel approach developed by Sielge and Robinson (2022). The tidytext approach defines a representation in the form of a table with one-token-per-row. A token is a meaningful unit of text for analysis, such as a word, a sentence or a tweet. Tokenisation is the process of splitting text into tokens. Tidy data has a specific structure. Each variable is a column. Each observation is a row and each type of observational unit is a table, The R tidytext package provides full functionality to implement the tidytext approach and manipulate text with the standard tidyverse ecosystem.\n\n# converting to a tidytext data representation\ntidy_tweets_df &lt;- tweets_df %&gt;%\n    select(created_at, text) %&gt;%\n    unnest_tokens(\"word\", text)\n\nIn numeric data analysis, we normally look at the distribution of variables to gain a first understanding of the data. In text analysis, we explore the distribution of words. Typically we analyse the top words.\n\n# ranking words\ntidy_tweets_df %&gt;% \n  count(word) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 129,554 × 2\n   word             n\n   &lt;chr&gt;        &lt;int&gt;\n 1 anonymous   449611\n 2 the         215293\n 3 to          163434\n 4 http        143377\n 5 url_removed 136774\n 6 and         112320\n 7 of          106244\n 8 in           93216\n 9 a            92763\n10 is           75237\n# ℹ 129,544 more rows\n\n\n\n\n6.2.2 Basic text data principles\nWorking with text data is complex. There are various important concepts and procedures that we need to introduce to get you familiar with, before we can get you rolling with text data mining. In this section, we introduce key concepts using example to illustrate the main ideas and basic code.\nCharacter encoding\nCharacter encoding is the numeric representation of graphical characters that we use to represent human language. Character encoding, such as UTF-8, ASCII and -ISO-8859-1 enables characters to be stored, transmitted and transformed using digital computers. For example, we use the English alphabet and understand differences between lower, upper case letters, numerals and punctuation. Computers encode and understand these characters as binary numeric combinations. There is no unique system of character representation.\nVarious systems exist and vary according to the type of information, when it was stored and geographic context. Additionally, different character encoding representations are used with varying levels of popularity according to the operating system, language and software, and this level of popularity changes over time. Currently UTF-8 is one of the most popular character encoding systems used on the web according to Google.\nSometimes we may need to standardise text data before they can be combined based on different character encoding systems. Generally R recognises and read different character encoding representations. But, if you notice an error of invalid string or an unusual character, this may mean you are using a dataset based on a character encoding representation which has not been globally integrated in R. This tends to occur with characters in various languages and emojis.\nThere is not quick way to standardise two different encoding systems, to our knowledge. Two handy functions in R that would help you with this task is the iconv and encoding. Both functions require you to know the character encoding representation of the data.\nRegular expressions\nRegular expressions are patterns that occur in a group of strings. Often when you work with text data, you are likely to find unusual character expressions, particularly if you are working with data scrapped from a website. Strings such as tab and return are normally represented by \\t and \\r so you may want to remove these expressions. To deal with regular patterns, we can use the grep base R library. In our dataset, we do have various occurrences of the expression &gt which stands for &gt; - see example below. Let’s assume we want to remove this expression from our tweet sample. We first could use grepl from the grep library to identify tweets containing this expression. The grepl function asks if the first expression before the comma is present in the data object after the comma, and returns Boolean result i.e. TRUE or FALSE.\n\n\n\n\n\n\nNote\n\n\n\nYou can also use the function str_detect from the stringr package to identify regular patterns or expressions in text\n\n\n\ngrepl(\"&gt\", tweets_df$text[40:50])\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n\n\nLet’s see one of those tweets:\n\ntweets_df$text[48]\n\n[1] \"Job Lead! ---&gt; LIFE Program Assistant, International Institute of Los Angeles http://url_removed#nonprofit #jobs #Immigration #multitasking\"\n\n\nWe may now want to remove the regular expression &gt. We can do this by using gsub from the grep library. In gsub, you add the pattern you want to replace (i.e. ---&gt), followed by the expression you want to use to replace this pattern with (nothing), and the data object (the tweet:tweets_df$text[48]).\n\ngsub(\"---&gt\", \"\", tweets_df$text[48])\n\n[1] \"Job Lead! ; LIFE Program Assistant, International Institute of Los Angeles http://url_removed#nonprofit #jobs #Immigration #multitasking\"\n\n\ngrep functions can also be very helpful in identifying a word and prefix in a string of words. We recommend consulting the Cheat Sheet for basic regular expressions in R put together by Ian Kopacka, to get familiar with the various character expressions and functions.\nUnit of analysis\nA key component in text data mining is the unit of analysis. We could focus our analysis on single words, individual sentences, paragraphs, sections, chapters or a larger corpus of text. The concept of here is relevant. The process of splitting text into units is known as tokenisation. Tokens are the resulting units of analysis. In the context of text data mining, n-grams is a popular tokenisation concept. This refers to a sequence of words of length n. A unigram is one word (e.g. migration). A bigram is a sequence of two words (migration can). A trigram is a sequence of three words (migration can have) and so on. n-grams can be useful when you want to capture the meaning of a sequence of words; for example, identifying “The United Kingdom” in a sequence of words. In R, we can use tidytext to organise the data according to n-grams.\n\n\n\n\n\n\nNote\n\n\n\nNote that the size of the chunk of text that we use to add up unigram sentiment scores can have an effect on an analysis. A text the size of many paragraphs can often have positive and negative sentiment averaged out to about zero, while sentence-sized or paragraph-sized text often works better. Similarly, small sections of text may not contain enough words to accurately estimate sentiment, while sentiment in very large sections may be difficult to identify.\n\n\n\ntweets_df[1:50,] %&gt;%\n    select(created_at, text) %&gt;%\n    unnest_tokens(ngram, text, token = \"ngrams\", n = 2)\n\n# A tibble: 1,296 × 2\n   created_at          ngram                \n   &lt;dttm&gt;              &lt;chr&gt;                \n 1 2019-12-01 00:00:18 another example      \n 2 2019-12-01 00:00:18 example anonymous    \n 3 2019-12-01 00:00:18 anonymous are        \n 4 2019-12-01 00:00:18 are the              \n 5 2019-12-01 00:00:18 the true             \n 6 2019-12-01 00:00:18 true protectors      \n 7 2019-12-01 00:00:18 protectors of        \n 8 2019-12-01 00:00:18 of sanctuary         \n 9 2019-12-01 00:00:18 sanctuary communities\n10 2019-12-01 00:00:18 communities when     \n# ℹ 1,286 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nTask\nTry changing n to 4. What changes do you observe?\n\n\nText pre-processing\nWe also need to think about the words we want to include in our analysis. Normally we focus on a selection of words conveying a particular conceptual representation. Some words may not convey much meaning, enrich our analysis or may distort the meanings we want to capture. So carefully thinking of the words we want to include in our analysis is important. Below we go through key character concepts which are often considered in text data mining and natural language processing. These concepts imply the removal of certain characters such as stop words, punctuation and numbers. The need to remove these characters will vary on the aim of the study, context and algorithm used to analyse the data.\nStop words\nStop words are commonly used words in a language. They tend to comprise articles, prepositions, pronouns and conjunctions. Examples of stop words in English are “a”, “the”, “is” and “are”. Stop words are essential to communicate in every day language. Yet, in the text data mining and NLP, stop words are often removed as they are considered to carry limited useful information. Stop words differ across languages and different lists of words exist to remove stop words from analysis. We use the tidytext approach to remove stop words by running:\n\n# remove stop words\ndata(\"stop_words\") # call stop word list\n tidy_tweets_df &lt;- tidy_tweets_df %&gt;% \n   anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\n\nLet’s see what are the top words now:\n\ntidy_tweets_df[1:10,] %&gt;% \n  count(word) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 9 × 2\n  word            n\n  &lt;chr&gt;       &lt;int&gt;\n1 trust           2\n2 anonymous       1\n3 citizens        1\n4 communities     1\n5 ice             1\n6 police          1\n7 protectors      1\n8 sanctuary       1\n9 true            1\n\n\nWe can see words, such as “t.co” and “https” which may not add much to our analysis and we may consider to remove them using grep functions. We will illustrate this below.\nPunctuation\nWe may also want to remove punctuation. Again, while punctuation may be very important in written language to communicate and understand the meaning of text. Punctuation by itself does not convey helpful meaning in the context of text analysis as we often take the text out of sequential order. Punctuation removal is another reason to prefer tidytext as punctuation marks are removed automatically.\nNumbers\nWe may want to remove numbers. Sometimes numbers, such as 9/11 or 2016 may provide very relevant meaning in the terrorist attacks in the US context or Brexit referendum in the UK. However, they generally do not add much meaning. We can use grep to remove numbers. The \"\\\\b\\\\d+\\\\b\" text tells R to remove all numeric digits. d stands for digits. The - sign tells grep to exclude these digits.\n\n# remove numbers\ntidy_tweets_df &lt;- tidy_tweets_df[ -grep(\"\\\\b\\\\d+\\\\b\", \n                                        tidy_tweets_df$word),]\n\nWord case\nAn additional element to consider is word case. Often all text is forced into lower case in quantitative text analysis as we do not want words starting with an upper case word such as “Migration” to be counted as a different word than “migration”. In this example, the difference between using lower or upper case words may not matter. Semantically, on other occasions (e.g. in a sentiment analysis context), this distinction may make all the difference. Consider “HAPPY” or “happy”. The former may emphasise that a person is much happier than in the latter case and we may want to capture the intensity of this emotion in our analysis. In such cases, we may be better off preserving the original text.\nThe unnest_tokens in the tidytext package automatically forces all words into lower case. To preserve upper case words, you will need to change the default options for to_lower to FALSE.\n\ntweets_df[1:10,] %&gt;%\n    select(created_at, text) %&gt;%\n    unnest_tokens(\"word\", \n                  text, \n                  to_lower = FALSE) # preserve upper case\n\n# A tibble: 282 × 2\n   created_at          word       \n   &lt;dttm&gt;              &lt;chr&gt;      \n 1 2019-12-01 00:00:18 Another    \n 2 2019-12-01 00:00:18 example    \n 3 2019-12-01 00:00:18 anonymous  \n 4 2019-12-01 00:00:18 are        \n 5 2019-12-01 00:00:18 the        \n 6 2019-12-01 00:00:18 TRUE       \n 7 2019-12-01 00:00:18 protectors \n 8 2019-12-01 00:00:18 of         \n 9 2019-12-01 00:00:18 sanctuary  \n10 2019-12-01 00:00:18 communities\n# ℹ 272 more rows\n\n\nWhite spaces\nWhite spaces can also be concern. Often white spaces may also be considered as words. In tidytext language, white spaces can be removed using the gsub function identifying white spaces with s+.\n\ngsub(\"\\\\s+\",\n     \"\",\n     tidy_tweets_df$word[1:20])\n\n [1] \"anonymous\"   \"true\"        \"protectors\"  \"sanctuary\"   \"communities\"\n [6] \"citizens\"    \"trust\"       \"police\"      \"trust\"       \"ice\"        \n[11] \"politics\"    \"ahead\"       \"public\"      \"safety\"      \"http\"       \n[16] \"url_removed\" \"218th\"       \"illegal\"     \"immigrant\"   \"child\"      \n\n\nStemming\nA common step in text-preprocessing is stemming. Stemming is the processing of lowering inflection in words to their root forms. For example, the stem of the word “monitoring” is “monitor”. This step aids in the pre-processing of text, words, and documents for text normalisation. Stemming is common practice because we do not want the words, such as “monitoring” and “monitor” to convey different meanings to algorithms, as such topic modelling algorithms, that we use to extract latent themes from unstructured texts.\nFor stemming words, we can use the function wordStem from the SnowballC package.\n\ntidy_tweets_df[1:20,] %&gt;%\n      mutate_at(\"word\", \n                funs(wordStem((.), \n                              language=\"en\")))\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\n# A tibble: 20 × 2\n   created_at          word     \n   &lt;dttm&gt;              &lt;chr&gt;    \n 1 2019-12-01 00:00:18 anonym   \n 2 2019-12-01 00:00:18 true     \n 3 2019-12-01 00:00:18 protector\n 4 2019-12-01 00:00:18 sanctuari\n 5 2019-12-01 00:00:18 communiti\n 6 2019-12-01 00:00:18 citizen  \n 7 2019-12-01 00:00:18 trust    \n 8 2019-12-01 00:00:18 polic    \n 9 2019-12-01 00:00:18 trust    \n10 2019-12-01 00:00:18 ice      \n11 2019-12-01 00:00:18 polit    \n12 2019-12-01 00:00:18 ahead    \n13 2019-12-01 00:00:18 public   \n14 2019-12-01 00:00:18 safeti   \n15 2019-12-01 00:00:18 http     \n16 2019-12-01 00:00:18 url_remov\n17 2019-12-01 00:00:23 218th    \n18 2019-12-01 00:00:23 illeg    \n19 2019-12-01 00:00:23 immigr   \n20 2019-12-01 00:00:23 child    \n\n\nCoding characters\nTypically we also want to remove coding characters, including links to websites. Words such as “https”, “t.co” and “amp” are common in webscrapped or social media text. The strings “https” and “t.co” appear as the top two more frequent words in our data. Generally such words do not convey relevant meaning for the analysis so they are removed. To do this, we can use grep.\n\ntidy_tweets_df[-grep(\"https|t.co|amp\",\n                              tidy_tweets_df$word),] %&gt;% \n  count(word) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 123,886 × 2\n   word             n\n   &lt;chr&gt;        &lt;int&gt;\n 1 anonymous   449611\n 2 http        143377\n 3 url_removed 136774\n 4 immigration  51480\n 5 people       19523\n 6 ice          19397\n 7 immigrant    18898\n 8 trump        17861\n 9 virus        16162\n10 illegals     14392\n# ℹ 123,876 more rows\n\n\nWe could also use str_detect from the stringr package.\n\ncoding_words &lt;- c(\"https|http|t.co|amp|anonymous|url_removed|http://url_removed\")\n\ntidy_tweets_df &lt;- tidy_tweets_df %&gt;%\n  filter(!str_detect(word, coding_words)) \n\ntidy_tweets_df %&gt;% \n  count(word) %&gt;% \n  arrange(desc(n))\n\n# A tibble: 123,191 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 immigration 51480\n 2 people      19523\n 3 ice         19397\n 4 immigrant   18898\n 5 trump       17861\n 6 virus       16162\n 7 illegals    14392\n 8 china       12750\n 9 coronavirus 12318\n10 chinese     11258\n# ℹ 123,181 more rows\n\n\nAnalysing word frequencies is often the first stop in text analysis. We can easily do this using ggplot. Let’s visualise the 20 most common words used on Twitter to express public opinions about migration-related topics.\n\ntidy_tweets_df %&gt;% \n  count(word) %&gt;% \n  arrange(desc(n)) %&gt;% \n  slice(1:20) %&gt;%\n  ggplot( aes(x= reorder(word, n), y= n/1000, fill = n/1000)) +\n  geom_bar( position=\"stack\", \n            stat = \"identity\"\n            ) +\n  theme_tufte2() +\n  scale_fill_gradient(low = \"white\", \n                      high = \"darkblue\") +\n  theme(axis.text.x = element_text(angle = 90, \n                                   hjust = 1)) +\n  ylab(\"Number of occurrences ('000)\") +\n  xlab(\"\") +\n  labs(fill = \"Word occurrences\") +\n  coord_flip()"
  },
  {
    "objectID": "sentiment-analysis.html#sentiment-analysis",
    "href": "sentiment-analysis.html#sentiment-analysis",
    "title": "6  Sentiment Analysis",
    "section": "6.3 Sentiment Analysis",
    "text": "6.3 Sentiment Analysis\nAfter pre-processing our text, we can focus on the key of this chapter; that is, measuring migration sentiment. We do this by using sentiment analysis, which as described in the introduction of this chapter, enables identifying, measuring and analysing emotional states and subjective information. It computationally infers the polarity of text, that is, whether the underpinning semantics of an opinion is positive, negative or neutral. A variety of methods and dictionaries for evaluating the opinion or emotion in text exists. We will explore four different lexicon-based approaches: AFFIN, bing, nrc and VADER.\n\n6.3.1 Dictionary-based methods\nAFFIN, bing and nrc are dictionary- or lexicon-based approaches. Sentiment lexicons include key words which are typically used to express emotions or feelings, and are assigned a numeric score for positive and negative sentiment. They may also contain scores for emotions, such as joy, anger and sadness. Sentiment lexicons can thus be used to measure the valence of a given text by searching for words that describe affect or opinion. Dictionaries can be created by examining text-based evaluations of products in online forums to ratings systems from a variety of sources. They can also be created via systematic observations about different emotions in the field of psychology or related fields.\nThe package tidytext provides access to all three lexicons. The nrc lexicon classifies words in a binary way into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise and trust. nrc constructed via Amazon Mechanical Turk (i.e. people manually labelling the emotional valence of words). The bing lexicon classifies words in a binary classification of positive and negative, and is based on words identified on online forums. The AFINN lexicon assigns words with a score ranging between -5 and 5 to capture the intensity of negative and positive sentiment. AFINN includes a list of sentiment-laden words used during discussions about climate change on Twitter.\n\n\n\n\n\n\nNote\n\n\n\nNote that not all words are in the lexicons and they only contain words in the English language.\n\n\nIn R, we can browse the content of each lexicon using the get_sentiment function from tidytext.\n\n\n\n\n\n\nNote\n\n\n\nNote that you may need to authorise the download of the lexicons on your console\n\n\n\nget_sentiments(\"nrc\")\n\n# A tibble: 13,872 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# ℹ 13,862 more rows\n\n\n\nget_sentiments(\"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n\n\n\nget_sentiments(\"afinn\") \n\n# A tibble: 2,477 × 2\n   word       value\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# ℹ 2,467 more rows\n\n\nWe can easily employ sentiment lexicons using the get_sentiment function from tidytext. Let’s first create a date variable to analyse fluctuations in sentiment by day and compute sentiment scores.\n\ntidy_tweets_df &lt;- tidy_tweets_df %&gt;%\n  mutate(\n    date = as.Date(substr(as.character(created_at),\n                          1,\n                          10))\n  )\n\n\nnrc_scores &lt;- tidy_tweets_df %&gt;%\n  inner_join(get_sentiments(\"nrc\") %&gt;% \n                 filter(sentiment %in% c(\"positive\", \n                                         \"negative\"))\n             ) %&gt;%\n  mutate(method = \"nrc\")\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., get_sentiments(\"nrc\") %&gt;% filter(sentiment %in% : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 102 of `x` matches multiple rows in `y`.\nℹ Row 812 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nnrc_scores %&gt;% head()\n\n# A tibble: 6 × 5\n  created_at          word      date       sentiment method\n  &lt;dttm&gt;              &lt;chr&gt;     &lt;date&gt;     &lt;chr&gt;     &lt;chr&gt; \n1 2019-12-01 00:00:18 true      2019-12-01 positive  nrc   \n2 2019-12-01 00:00:18 sanctuary 2019-12-01 positive  nrc   \n3 2019-12-01 00:00:18 police    2019-12-01 positive  nrc   \n4 2019-12-01 00:00:18 ahead     2019-12-01 positive  nrc   \n5 2019-12-01 00:00:18 public    2019-12-01 positive  nrc   \n6 2019-12-01 00:00:23 illegal   2019-12-01 negative  nrc   \n\n\n\nbing_scores &lt;- tidy_tweets_df %&gt;%\n  inner_join(get_sentiments(\"bing\")) %&gt;%\n    mutate(method = \"bing\")\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., get_sentiments(\"bing\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 106476 of `x` matches multiple rows in `y`.\nℹ Row 6177 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nbing_scores %&gt;% head()\n\n# A tibble: 6 × 5\n  created_at          word    date       sentiment method\n  &lt;dttm&gt;              &lt;chr&gt;   &lt;date&gt;     &lt;chr&gt;     &lt;chr&gt; \n1 2019-12-01 00:00:18 trust   2019-12-01 positive  bing  \n2 2019-12-01 00:00:18 trust   2019-12-01 positive  bing  \n3 2019-12-01 00:00:23 illegal 2019-12-01 negative  bing  \n4 2019-12-01 00:00:23 abuse   2019-12-01 negative  bing  \n5 2019-12-01 00:00:41 threats 2019-12-01 negative  bing  \n6 2019-12-01 00:00:41 protect 2019-12-01 positive  bing  \n\n\n\nafinn_scores &lt;- tidy_tweets_df %&gt;%\n  inner_join(get_sentiments(\"afinn\")) %&gt;%\n  mutate(method = \"afinn\")\n\nJoining with `by = join_by(word)`\n\nafinn_scores %&gt;% head()\n\n# A tibble: 6 × 5\n  created_at          word    date       value method\n  &lt;dttm&gt;              &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt; &lt;chr&gt; \n1 2019-12-01 00:00:18 true    2019-12-01     2 afinn \n2 2019-12-01 00:00:18 trust   2019-12-01     1 afinn \n3 2019-12-01 00:00:18 trust   2019-12-01     1 afinn \n4 2019-12-01 00:00:18 safety  2019-12-01     1 afinn \n5 2019-12-01 00:00:23 illegal 2019-12-01    -3 afinn \n6 2019-12-01 00:00:23 arrests 2019-12-01    -2 afinn \n\n\nAs you can see, the output of the various algorithm differs. nrc and bing provides sentiment scores classified into positive and negative. afinn returns a value from -5 to 5. An important feature of the three approaches explored so far is that they are based on unigrams; that is, single words. As a result, these methods do not take into account qualifiers before a word, such as in “no good” or “not true”. Additionally, these methods cannot appropriately handle negations, contractions, slang, emoticons, emojis, initialisms, acronyms, punctuation and word-shape (e.g., capitalization) as a signal of sentiment polarity and intensity (Hutto and Gilbert 2014) . Most commonly, lexicon-based approaches only capture differences in sentiment polarity (i.e., positive or negative) but do not identify differences in sentiment intensity (strongly positive vs. moderately positive) or contrasting statements. We note that accurate identification and scoring of sarcastic statements remain a key challenge in natural language processing.\nWe could subtracting positive and negative score to obtain an estimate of sentiment for each day based on three lexicon approaches. The resulting data frames could be binned and used to visualise how the predominant pattern of migration sentiment changes over time. We recalculate the sentiment scores by reusing the code above and adding lines for counting, pivoting, summing and subtracting.\n\nnrc_scores &lt;- tidy_tweets_df %&gt;%\n  inner_join(get_sentiments(\"nrc\") %&gt;% \n                 filter(sentiment %in% c(\"positive\", \n                                         \"negative\"))\n             ) %&gt;%\n  count(date, sentiment) %&gt;% \n  pivot_wider(names_from = sentiment,\n              values_from = n,\n              values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative) %&gt;% \n  mutate(method = \"nrc\") %&gt;% \n  select(date, sentiment, method)\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., get_sentiments(\"nrc\") %&gt;% filter(sentiment %in% : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 102 of `x` matches multiple rows in `y`.\nℹ Row 812 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nbing_scores &lt;- tidy_tweets_df %&gt;%\n  inner_join(get_sentiments(\"bing\")) %&gt;%\n  count(date, sentiment) %&gt;% \n   pivot_wider(names_from = sentiment,\n              values_from = n,\n              values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative) %&gt;% \n    mutate(method = \"bing\") %&gt;% \n  select(date, sentiment, method)\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., get_sentiments(\"bing\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 106476 of `x` matches multiple rows in `y`.\nℹ Row 6177 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nafinn_scores &lt;- tidy_tweets_df %&gt;%\n  inner_join(get_sentiments(\"afinn\")) %&gt;%\n  group_by(date) %&gt;% \n  summarise(sentiment = sum(value)) %&gt;% \n  mutate(method = \"afinn\")\n\nJoining with `by = join_by(word)`\n\n\nOnce we have daily sentiment scores, we bin them together and visualise them.\n\nbind_rows(nrc_scores,\n          bing_scores,\n          afinn_scores) %&gt;%\n  ggplot( aes(x = date, y = sentiment, fill = method)) +\n  geom_col(show.legend = FALSE) +\n  theme_tufte2() +\n  facet_wrap(~method, ncol = 1, scales = \"free_y\")\n\n\n\n\nAll there lexicons display the same predominant pattern of increasing negative migration sentiment between March and April. They differ in the representation they provide during earlier months. While afinn and bing lexicons concur in suggesting that a negative sentiment was the prevalent pattern of sentiment towards migration during these months, nrc paints a different picture of a predominantly positive sentiment.\n\n\n6.3.2 VADER\nWe move on to explore VADER. VADER is a lexicon and rule-based sentiment analysis tool which is tailored to the analysis of sentiments expressed in social media, and stands for Valence Aware Dictionary and sEntiment Reasoner (Hutto and Gilbert 2014). VADER has been shown to perform better than 11 typical state-of-practice sentiment algorithms at identifying the polarity expressed in tweets (Hutto and Gilbert 2014), and has remained one of the most widely used sentiment analysis methods for social media data (e.g. Elbagir and Yang 2020) ). See Ghani et al. (2019) and Rosa et al. (2019) for recent comprehensive reviews of social media analytics.\nVADER overcomes limitations of existing approaches (Hutto and Gilbert 2014). It also captures differences in sentiment intensity, contrasting statements, and can handle complex sentences, including typical negations (e.g. “not good”), contractions (e.g. “wasn’t very good”), conventional use of punctuation to signal increased sentiment intensity (e.g. “Good!!!”), use of word-shape to signal emphasis (e.g. using ALL CAPS), using degree modifiers to alter sentiment intensity (e.g. intensity boosters (e.g. “very”) and intensity dampeners (e.g.”kind of”), sentiment-laden slang (e.g. ‘sux’), slang words as modifiers (e.g. ‘uber’ or ‘friggin’ or ‘kinda’), emoticons (:) and :D), translating utf-8 encoded emojis (💘, 💋 and 😁), initialisms and acronyms (e.g. ‘lol’). VADER can also handle entire sentences or ngrams, rather than only unigrams. See some examples below.\n\nvader_df(\"wasn't very good\")\n\n              text      word_scores compound pos   neu   neg but_count\n1 wasn't very good {0, 0, -1.62282}   -0.386   0 0.433 0.567         0\n\nvader_df(\"not good\")\n\n      text word_scores compound pos   neu   neg but_count\n1 not good {0, -1.406}   -0.341   0 0.294 0.706         0\n\nvader_df(\"good\")\n\n  text word_scores compound pos neu neg but_count\n1 good       {1.9}     0.44   1   0   0         0\n\nvader_df(\"Good!!!\")\n\n     text word_scores compound pos neu neg but_count\n1 Good!!!       {1.9}    0.583   1   0   0         0\n\nvader_df(\"VERY good!!!\")\n\n          text word_scores compound   pos   neu neg but_count\n1 VERY good!!!  {0, 2.926}    0.701 0.828 0.172   0         0\n\nvader_df(\"wasn't bad but very good\")\n\n                      text              word_scores compound   pos   neu neg\n1 wasn't bad but very good {0, 0.925, 0, 0, 3.2895}    0.736 0.674 0.326   0\n  but_count\n1         1\n\n\nThe output is a vector with the following entries: * word_scores: a string that contains an ordered list with the matched scores for each of the words in the text. For the first example, you can see three scores i.e. a 0 score for “wasn’t” and “very” and a negative score for “good” reflecting the meaning of “good” in the text. * compound: the resulting valence compound of VADER for the entire text after applying modifiers and aggregation rules. * pos, neg, and neu: the parts of the compound for positive, negative, and neutral content. These take into account modifiers and are combined when calculating the compound score * but_count: an additional count of “but” since it can complicate the calculation of sentiment.\nLet’s think about the results from the examples above, what is the piece of text with the most negative and positive sentiment score? Why? How do modifiers, amplifiers and negators influence the meaning of text?\nNow we will use VADER to explore the sentiment towards migration during the wake of the COVID-19 pandemic. To reduce computational requirements, we will work with a sub-sample of our data to obtain sentiment scores at the tweet level. This is unlike our previous analysis which returned word-level scores. Obtaining sentiment scores via VADER may take some time. So do not panic, relax and wait. If this is taking too long, you can use the `tweet_vader_scores.rds` in the data folder for this chapter.\nNote that for this example we will use the tweet text as VADER can handle various of the issues that would be a problem using the previous three approaches (as we have described above). Nonetheless, for your our work you may want to explore the influence of regular expressions.\n\n# tweet level \ntweet_vader_scores &lt;- vader_df(tweets_df$text)\n\n\n# combine vader scores,tweet ids and dates\ntweets_scores_df &lt;- cbind(tweets_df$id, tweets_df$created_at, tweet_vader_scores) \n\n# rename vars and extract day var\ntweets_scores_df &lt;- tweets_scores_df %&gt;% \n  rename(\n    id = \"tweets_df$id\",\n    created_at = \"tweets_df$created_at\"\n  ) %&gt;%  \n  dplyr::mutate(\n    date = as.Date(substr(as.character(created_at),\n                          1,\n                          10))\n  )\n\nConcentration\nWe have done the hard work of computing sentiment scores. We can now start analysing the results. As any exploratory analysis, a first feature you may want to analyse if the overall distribution of sentiment scores. Applied to public opinion data, such analysis may give you an idea of how socially polarised is a discussion on social media. To this end, we can create a histogram.\n\np1 &lt;- ggplot(data = tweets_scores_df) +\n  geom_histogram(aes(x = compound, \n                 binwidth = 0.05), \n                 fill = \"#440154FF\",\n                 color=\"#440154FF\") +\n  theme_tufte2() + \n  labs(x= \"Tweet sentiment score\",\n       y = \"Density\")\n\np2 &lt;- ggplot(tweets_scores_df, aes(compound)) + \n  stat_ecdf(geom = \"step\",\n            size = 2,\n            colour = \"#440154FF\") +\n  theme_tufte2() + \n  labs(x= \"Tweet sentiment score\",\n       y = \"Cumulative density\")\n\np1 | p2\n\n\n\n\nWe produce two plots exploring the frequency and cumulative distribution of migration sentiment scores. The results indicate that a concentration around zero and also at both extremes i.e. below -0.5 and over 0.5, suggesting that migration is very polarising social issue.\nTemporal evolution\nWe can also explore the temporal evolution of sentiment towards migration over time. The results indicate that migration sentiment remained slighly negative but stable during March to April 2020.\n\n# plot sentiment scores by day\np3 &lt;- ggplot(tweets_scores_df, \n             aes(x = date, y = compound)) +\n geom_point(colour = \"gray\", alpha = 0.3, size = 1, shape=\".\") + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", size = .3) +\n  geom_smooth(method = \"loess\", se = FALSE, size=2, span = 0.3, color=\"#440154FF\") +\n  theme_tufte2() +\n  labs(x= \"Day\",\n       y = \"Tweet sentiment score\")  +\n  scale_y_continuous(limits = c(-1, 1))\n\np3\n\n\n\n\nComposition\nAnalysing the coumpound conceals the composition of sentiment in tweets, particularly potential rises in strongly negative sentiment. We then analyse the composition of tweets classifying our sentiment score into “Strongly Negative”, “Negative”, “Neutral”, “Positive” and “Strongly Positive” as shown below. The results indicate that the composition remained largely stable over time with 50% of all tweets being negative, of which around 25% were strongly negative.In contrast, less than 20% of all tweets were strongly positive. These results suggest that anti-migration sentiment tends to use a stronger rethoric than pro-migration sentiment.\n\n# sentiment categories\ntweets_scores_df &lt;- tweets_scores_df %&gt;% \n  mutate(stance_group =\n           case_when(\n           compound &gt;= -.05 & compound &lt;= .05 ~ 3,\n           compound &lt; -.5 ~ 1,\n           compound &lt; -.05 & compound &gt;= -.5 ~ 2,\n           compound &gt; .05 & compound &lt;= .5 ~ 4,\n           compound &gt; .5 ~ 5,\n           )\n         )\n\n# count in each sentiment category by day\ncomposition_tab &lt;- tweets_scores_df %&gt;% \n  group_by(date) %&gt;% \n  dplyr::count(date, stance_group) %&gt;% \n  spread(stance_group, n)\n\n# percentage in each sentiment category by day\ncomposition_percent_tab &lt;- cbind(composition_tab[,1], (composition_tab[,2:6] / rowSums(composition_tab[,2:6]) * 100)) %&gt;% \n  .[, c(1, 3, 4, 2, 5, 6)] %&gt;% \n  gather(stance, percent, -date)\n\n# composition of sentiment score by day\np4 &lt;- ggplot(composition_percent_tab , \n             aes(fill = stance, y = percent, x = date)) + \n    geom_bar(position=\"stack\", stat=\"identity\") + \n    theme_tufte2() + \n    theme(legend.position = \"bottom\") +\n    scale_fill_manual(values = c(\"darkred\",\"#d7191c\", \"#f7f7f7\", \"#2c7bb6\", \"darkblue\"),\n                      labels = c(\"Strongly Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Strongly Positive\")) +\n  labs(x= \"Day\",\n       y = \"Percent\")\n\np4\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNatural language processing is a rapidly evolving field and various new sentiment analysis algorithms emerge over the last five years. They are often tailored to address specific tasks and their performance can vary widely across datasets. So before deciding on a particular algorithm, consult the literature on what has been previously used to identify standard approaches their limitations and strengths."
  },
  {
    "objectID": "sentiment-analysis.html#questions",
    "href": "sentiment-analysis.html#questions",
    "title": "6  Sentiment Analysis",
    "section": "6.4 Questions",
    "text": "6.4 Questions\nFor the second assignment, we will focus on the United Kingdom as our geographical area of analysis. We will use a dataset of tweets about migration posted by users in the United Kingdom during February 24th 2021 to July 1st 2022. This period coincides with the start of the war in Ukraine and is expected to capture changes in migration sentiment. The dataset contains the following information:\n\ntweet_id: unique tweet identifier\ncreated_at: date tweet were posted\nplace_name: name of place linked to tweet\nlat: latitude\nlong: longitude\ntext: text content of tweet\n\n\ntweets_qdf &lt;- readRDS(\"./data/sentiment-analysis/uk_tweets_24022021_01072022.rds\")\nglimpse(tweets_qdf)\n\nRows: 34,490\nColumns: 6\n$ tweet_id   &lt;dbl&gt; 1.364707e+18, 1.364694e+18, 1.364692e+18, 1.364684e+18, 1.3…\n$ created_at &lt;dttm&gt; 2021-02-24 22:40:21, 2021-02-24 21:47:44, 2021-02-24 21:42…\n$ place_name &lt;chr&gt; \"Westhumble\", \"Rushden\", \"Birmingham\", \"Cardiff\", \"Alexandr…\n$ lat        &lt;dbl&gt; -0.3302450, -0.6038262, -1.8906405, -3.1797998, -4.5748715,…\n$ long       &lt;dbl&gt; 51.25447, 52.28951, 52.49397, 51.49700, 55.98702, 53.64739,…\n$ text       &lt;chr&gt; \"@post_liberal Voting for Griffin was part of this working …\n\n\nUsing VADER:\n\nObtain sentiment scores and create plots to visualise the overall distribution of sentiment scores;\nCreate plots to analyse the temporal evolution of sentiment over time.\nVisualise the geographical patterns of sentiment scores - see the spatial autocorrelation section on Rowe (2022) to map sentiment scores using ggplot.\n\nAnalyse and discuss: a) the extent of anti-immigration sentiment in the United Kingdom; b) how it has changes over time in intensity and composition; and, c) the degree of spatial concentration in anti-immigration sentiment in the country.\n\n\n\n\nBail, Christopher A., Lisa P. Argyle, Taylor W. Brown, John P. Bumpus, Haohan Chen, M. B. Fallin Hunzaker, Jaemin Lee, Marcus Mann, Friedolin Merhout, and Alexander Volfovsky. 2018. “Exposure to Opposing Views on Social Media Can Increase Political Polarization.” Proceedings of the National Academy of Sciences 115 (37): 9216–21. https://doi.org/10.1073/pnas.1804840115.\n\n\nCheong, Pauline Hope, Rosalind Edwards, Harry Goulbourne, and John Solomos. 2007. “Immigration, Social Cohesion and Social Capital: A Critical Review.” Critical Social Policy 27 (1): 24–49. https://doi.org/10.1177/0261018307072206.\n\n\nCoates, Melanie. 2020. “Covid-19 and the Rise of Racism.” BMJ, April, m1384. https://doi.org/10.1136/bmj.m1384.\n\n\nCowper, Andy. 2020. “Covid-19: Are We Getting the Communications Right?” BMJ, March, m919. https://doi.org/10.1136/bmj.m919.\n\n\nElbagir, Shihab, and Jing Yang. 2020. “Sentiment Analysis on Twitter with Python’s Natural Language Toolkit and VADER Sentiment Analyzer.” IAENG Transactions on Engineering Sciences, January. https://doi.org/10.1142/9789811215094_0005.\n\n\nEuropean Commision. 2019. “10 Trends Shaping Migration.” https://op.europa.eu/en/publication-detail/-/publication/aa25fb8f-10cc-11ea-8c1f-01aa75ed71a1.\n\n\nGhani, Norjihan Abdul, Suraya Hamid, Ibrahim Abaker Targio Hashem, and Ejaz Ahmed. 2019. “Social Media Big Data Analytics: A Survey.” Computers in Human Behavior 101 (December): 417–28. https://doi.org/10.1016/j.chb.2018.08.039.\n\n\nHome Affairs Committee. 2020. “Oral evidence: Home Office preparedness for Covid-19 (Coronavirus), HC 232.” London: House of Commons. https://committees.parliament.uk/oralevidence/359/default/.\n\n\nHutto, C., and Eric Gilbert. 2014. “VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text.” Proceedings of the International AAAI Conference on Web and Social Media 8 (1): 216–25. https://doi.org/10.1609/icwsm.v8i1.14550.\n\n\nRosa, H., N. Pereira, R. Ribeiro, P. C. Ferreira, J. P. Carvalho, S. Oliveira, L. Coheur, P. Paulino, A. M. Veiga Simão, and I. Trancoso. 2019. “Automatic Cyberbullying Detection: A Systematic Review.” Computers in Human Behavior 93 (April): 333–45. https://doi.org/10.1016/j.chb.2018.12.021.\n\n\nRowe, Francisco. 2021. “Using Twitter Data to Monitor Immigration Sentiment.” http://dx.doi.org/10.31219/osf.io/sf7u4.\n\n\n———. 2022. “Introduction to Geographic Data Science.” Open Science Framework, August. https://doi.org/10.17605/OSF.IO/VHY2P.\n\n\nRowe, Francisco, Michael Mahony, Eduardo Graells-Garrido, Marzia Rango, and Niklas Sievers. 2021. “Using Twitter to Track Immigration Sentiment During Early Stages of the COVID-19 Pandemic.” Data & Policy 3. https://doi.org/10.1017/dap.2021.38.\n\n\nRowe, Francisco, Michael Mahony, Niklas Sievers, Marzia Rango, and Eduardo Graells-Garrido. 2021. “Sentiment towards Migration during COVID-19. What Twitter Data Can Tell Us.” IOM Publications.\n\n\nSielge, Jullia, and David Robinson. 2022. Welcome to Text Mining with r. O’Reilly. https://www.tidytextmining.com.\n\n\n“Stop the Coronavirus Stigma Now.” 2020. Nature 580 (7802): 165–65. https://doi.org/10.1038/d41586-020-01009-0."
  },
  {
    "objectID": "topic-modelling.html#dependencies",
    "href": "topic-modelling.html#dependencies",
    "title": "7  Topic Modelling",
    "section": "7.1 Dependencies",
    "text": "7.1 Dependencies\nAs shown in the Figure below by we can use tidy text principles to approach topic modeling with the same set of tidy tools used for other data analysis in R. In this chapter, we’ll learn to work with LDA objects from the topicmodels package, tidying such models so that they can be analysed with the help of ggplot2 and dplyr.\n\n\n\nSilge & Robinson 2022: A flowchart of a text analysis that incorporates topic modeling. The topicmodels package takes a Document-Term Matrix as input and produces a model that can be tided by tidytext, such that it can be manipulated and visualized with dplyr and ggplot2.\n\n\nWe use the libraries below.\n\n#Topic models package that allows tidying such models with ggplot2 and dplyr\nlibrary(topicmodels)\n#A framework for text mining applications within R.\nlibrary(tm)\n#The Life-Changing Magic of Tidying Text\nlibrary(tidytext)\n# Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library\nlibrary(SnowballC)\n# Data manipulation\nlibrary(tidyverse)\n#Create Elegant Data Visualisations Using the Grammar of Graphics\nlibrary(ggplot2)\nlibrary(ggthemes)\n# Reddit Data Extraction Toolkit\nlibrary(RedditExtractoR)\n# Flexibly Reshape Data\nlibrary(reshape2)\n# Estimation of Structural Topic Models\nlibrary (stm)"
  },
  {
    "objectID": "topic-modelling.html#data",
    "href": "topic-modelling.html#data",
    "title": "7  Topic Modelling",
    "section": "7.2 Data",
    "text": "7.2 Data\nReddit is a social news website where you can find posts about almost anything. Reddit has a huge user base and is increasingly used One of the most interesting aspects of Reddit is the comments that accompany posts. Redditors are known for their brutal honesty and often provide interesting opinions that you wouldn’t otherwise find. Reddit also has a plug-n-play R package which makes is very easy to get Reddit data via API.\nThe key is to find URLs to Reddit threads of interest. There are 2 available search strategies: by keywords and by home page. Using a set of keywords can help you narrow down your search to a topic of interest that crosses multiple subreddits whereas searching by home page can help you find, for example, top posts within a specific subreddit.\nIf you want to source your own Reddit data, comment out the code below and change keywords.\n\n#This function takes a collection of URLs and returns a list with 2 data frames: 1. a data frame containing meta data describing each thread 2. a data frame with comments found in all threads\n#urls &lt;- find_thread_urls(keywords = \"fertility\", sort_by = \"top\", subreddit = NA, period = \"year\" )\n\n#This function GETs the data. \n#fertility_data &lt;- get_thread_content(urls$url)\n\n# The below code simply creates data frames for the threads and the comments and saves them as csvs for later use. \n\n#threads &lt;- pandemic_babies_data$threads\n#comments &lt;- pandemic_babies_data$comments\n#write.csv(threads, \"data/topic-modelling/threads.csv\", row.names = FALSE)\n#write.csv(comments, \"data/topic-modelling/comments.csv\", row.names = FALSE)\n\nFirst we import the data we will be working with. You can either use the same data as used below or find other data sourced from reddit here.\n\ncomments &lt;- read.csv(\"data/topic-modelling/comments.csv\", header = TRUE)\n\nhead(comments)\n\n                                                                             url\n1 https://www.reddit.com/r/PrequelMemes/comments/zjcuzy/begun_the_clone_war_has/\n2 https://www.reddit.com/r/PrequelMemes/comments/zjcuzy/begun_the_clone_war_has/\n3 https://www.reddit.com/r/PrequelMemes/comments/zjcuzy/begun_the_clone_war_has/\n4 https://www.reddit.com/r/PrequelMemes/comments/zjcuzy/begun_the_clone_war_has/\n5 https://www.reddit.com/r/PrequelMemes/comments/zjcuzy/begun_the_clone_war_has/\n6 https://www.reddit.com/r/PrequelMemes/comments/zjcuzy/begun_the_clone_war_has/\n             author       date  timestamp score upvotes downvotes golds\n1    LaLiLuLeLo9001 2022-12-11 1670802198   223     223         0     0\n2 Obiwan-Kenobi-Bot 2022-12-11 1670802222   113     113         0     0\n3    LaLiLuLeLo9001 2022-12-11 1670802288    75      75         0     0\n4 Obiwan-Kenobi-Bot 2022-12-11 1670802305    63      63         0     0\n5         Chung_Soy 2022-12-12 1670821362    37      37         0     0\n6 Obiwan-Kenobi-Bot 2022-12-12 1670821387    35      35         0     0\n                                                                                                                                                                                                                                             comment\n1                                                                                                                                                   I'm sorry, are you implying Obi-Wan is just okay looking? That's pure blasphemy, he's beautiful.\n2                                                                          That is kind of you to say. But, beauty is an opinion, and I cannot agree with yours. Thank you for your kind thoughts, though.\\n\\n^This ^Response ^Generated ^by ^OpenAI\n3                                                                                                                                                                                                   Damn, and I thought *I*  had self esteem issues.\n4  We all have difficult times in our lives. It is not easy to maintain a positive self-image but, with patience and focus, it is possible to find inner peace and contentment. May the Force be with you.\\n\\n^This ^Response ^Generated ^by ^OpenAI\n5                                                                                                                                                                                                         Damn Obi-wan, you going through some shit?\n6                                                         Difficult times are part of life, but it is how we respond to them that truly reveals our strength. May the Force guide you through these times.\\n\\n^This ^Response ^Generated ^by ^OpenAI\n   comment_id\n1           1\n2         1_1\n3       1_1_1\n4     1_1_1_1\n5   1_1_1_1_1\n6 1_1_1_1_1_1\n\n\nWe now have a data frame with authors, dates and comments.\n\n7.2.1 Text data structures\nHowever, we need to create a Corpus style object to preserve both both the full text of our Reddit comments and the metadata to eventually move to a Document-Term matrix used Topic Modelling. We are going to be using the package tidytext.\n\ntidy_fertility_reddit &lt;- comments %&gt;% # Takes comments dataframe\n  select(timestamp, comment) %&gt;% # Breaks out the timestamp (like a unique idenified) and the text variables \n  unnest_tokens(\"word\", comment) # Passes the \"word\" token and the name of the variable which is 'comment'\n\nhead(tidy_fertility_reddit) # Checks out the first 5 words and the dataframe format\n\n   timestamp     word\n1 1670802198      i'm\n2 1670802198    sorry\n3 1670802198      are\n4 1670802198      you\n5 1670802198 implying\n6 1670802198      obi\n\n\nThe tidytext format is very useful because once the text has been tidy-ed, regular R functions can be used to analyze it instead of the specialized functions. For example, to count the most popular words in our Reddit, we can can un-comment the following:\n\n#tidy_fertility_reddit %&gt;%\n#  count(word) %&gt;%\n#  arrange(desc(n))\n\n\n\n7.2.2 Basic text data principles\nBefore we can run any type of analysis, we first need to decide precisely which type of text should be included in our analyses. For example, as the code above showed, common words such as “the”, “and” and “that” are most likely not very informative. Usually, words such as “the” will not be informative for our quantitative text analysis, but how many times reddit comments use the word “abortion” might be very relevant to an analysis about pro-choice discourses.\nStopwords\n\ndata(\"stop_words\") # Stopwords in tidytext package \ntidy_fertility_reddit_clean &lt;- tidy_fertility_reddit %&gt;%\n  anti_join(stop_words) #using anti-join to remove words\n\nJoining with `by = join_by(word)`\n\n\nPunctuation and numbers\nAn advantage of tidytext is that it removes punctuation automatically. There is also very easy in tidytext to remove all numeric digits. We can use basic grep commands (note the “\\b\\d+\\b” text here tells R to remove all numeric digits and the ‘-’ sign means grep excludes them rather than includes them). Grep (Global Regular Expression print) commands are used in searching and matching text files contained in the regular expressions.\n\ntidy_fertility_reddit_clean&lt;-tidy_fertility_reddit_clean[-grep(\"\\\\b\\\\d+\\\\b\", tidy_fertility_reddit_clean$word),]\n\n# Eliminate some specific words\ntidy_fertility_reddit_clean &lt;- tidy_fertility_reddit_clean %&gt;% \n  filter(!(word %in% c(\"https\", \"www.reddit.com\", \"comments\", \"gt\", \"don\", \"roe\", \"post\", \"didn\", \"oop\", \"ve\", \"x200b\", \"op\", \"nta\", \"fuck\", \"yeah\"))) \n\n# Replace some words with others (manual cleaning)\ntidy_fertility_reddit_clean &lt;- tidy_fertility_reddit_clean %&gt;%\n  mutate(word = if_else(word == \"children\", \"child\", word)) %&gt;%\n  mutate(word = if_else(word == \"kids\", \"child\", word)) %&gt;%\n  mutate(word = if_else(word == \"pregnancy\", \"pregnant\", word))\n\nWe could always do more cleaning.\nStemming\nStemming reduces words to most basic forms. A final common step in text-pre processing is stemming. Stemming a word refers to replacing it with its most basic conjugate form. For example the stem of the word “typing” is “type.” Stemming is common practice because we don’t want the words “type” and “typing” to convey different meanings to algorithms that we will soon use to extract latent themes from unstructured texts. Tidytext includes the wordStem function:\n\n  tidy_fertility_reddit_clean&lt;-tidy_fertility_reddit_clean %&gt;%\n      mutate_at(\"word\", ~wordStem(., language = \"en\"))\n\nAnalysing word frequencies is often the first stop in text analysis. We can easily do this using ggplot. Like in sentiment analysis, let’s visualise the 20 most common words used on reddit regarding fertility-related topics.\n\ntidy_fertility_reddit_clean %&gt;% \n  count(word) %&gt;% \n  arrange(desc(n)) %&gt;% \n  slice(1:20) %&gt;%\n  ggplot( aes(x= reorder(word, n), y= n/1000, fill = n/1000)) +\n  geom_bar( position=\"stack\", \n            stat = \"identity\"\n            ) +\n  theme_tufte() +\n  scale_fill_gradient(low = \"white\", \n                      high = \"darkblue\") +\n  theme(axis.text.x = element_text(angle = 90, \n                                   hjust = 1)) +\n  ylab(\"Number of occurrences ('000)\") +\n  xlab(\"\") +\n  labs(fill = \"Word occurrences\") +\n  coord_flip()\n\n\n\n\nThe Document-Term Matrix (DTM)\nFinally, we transform our data into a document-term matrix which is the format we will be needing for quantitative text analysis. This is a matrix where each word is a row and each column is a document. The number within each cell describes the number of times the word appears in the document. Many of the most popular forms of text analysis, such as topic models, require a document-term matrix.\nTo create a DTM in tidytext we can use the following code:\n\ntidy_fertility_DTM&lt;-\n  tidy_fertility_reddit_clean %&gt;%\n  count(timestamp, word) %&gt;%\n  cast_dtm(timestamp, word, n)\n\ninspect(tidy_fertility_DTM[1:5,3:8])\n\n&lt;&lt;DocumentTermMatrix (documents: 5, terms: 6)&gt;&gt;\nNon-/sparse entries: 6/24\nSparsity           : 80%\nMaximal term length: 7\nWeighting          : term frequency (tf)\nSample             :\n            Terms\nDocs         act aren awkward dis normal wish\n  1645984812   0    0       0   0      0    0\n  1645984815   0    0       0   1      0    0\n  1645984839   1    1       0   0      1    1\n  1645984869   0    0       1   0      0    0\n  1645984938   0    0       0   0      0    0"
  },
  {
    "objectID": "topic-modelling.html#topic-modelling",
    "href": "topic-modelling.html#topic-modelling",
    "title": "7  Topic Modelling",
    "section": "7.3 Topic Modelling",
    "text": "7.3 Topic Modelling\nAfter pre-processing out text, we can focus on the key of this chapter: discussions around fertility and the pandemic’s influence on fertility rates. We do this by using topic modelling.\nTo start, we will use the DTM we created from the reddit data and the LDA() function from the topicmodels package, setting k = 5, to create a five-topic LDA model. Almost any topic model in practice will use a larger k, but we will soon see that this analysis approach extends to a larger number of topics.\nThis function returns an object containing the full details of the model fit, such as how words are associated with topics and how topics are associated with documents.\n\n# set a seed so that the output of the model is predictable\n\nReddit_topic_model &lt;- LDA(tidy_fertility_DTM,\n              k = 5, # number of presumed topics\n              control = list(seed = 541)) # important if you want this to be reproducible, (321)\n\nReddit_topic_model\n\nA LDA_VEM topic model with 5 topics.\n\n\nFitting the model was the “easy part”: the rest of the analysis will involve exploring and interpreting the model using tidy functions from the tidytext package.\n\n7.3.1 Word-topic probabilities\nWe can use the tidy() function, originally from the broom package Robinson 2017, for tidying model objects. The tidytext package provides this method for extracting the per-topic-per-word probabilities, called β (“beta”), from the model.\n\nap_topics &lt;- tidy(Reddit_topic_model, matrix = \"beta\")\nap_topics\n\n# A tibble: 162,320 × 3\n   topic term       beta\n   &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1     1 carri 0.000567 \n 2     2 carri 0.00115  \n 3     3 carri 0.0000683\n 4     4 carri 0.000402 \n 5     5 carri 0.00117  \n 6     1 deal  0.000830 \n 7     2 deal  0.00164  \n 8     3 deal  0.00203  \n 9     4 deal  0.000326 \n10     5 deal  0.000681 \n# ℹ 162,310 more rows\n\n\nThis has turned the model into a one-topic-per-term-per-row format. For each combination, the model computes the probability of that term being generated from that topic. CHANGE: For example, the term “aaron” has a 1.686917 × 10−12 probability of being generated from topic 1, but a 3.8959408 × 10−5 probability of being generated from topic 2.\nThen there are different options. We could use dplyr’s slice_max() to find the 10 terms that are most common within each topic. As a tidy data frame, this lends itself well to a ggplot2 visualization.\n\nap_top_terms &lt;- ap_topics %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nap_top_terms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered() +\n  theme_tufte()\n\n\n\n\nHave we defined too many topics? Do we need to increase the number of words per topic. We can see that the Topic 1 focuses on “pregancy” and “adoption”, while Topic 3 is probably addressing “legal” questions around fertility. We would need to clean the data further to identify better patterns.\n\n\n7.3.2 Greatest differences in \\(\\beta\\)\nWe can consider the terms that had the greatest difference in β between Topic 1 and Topic 3. This can be estimated based on the log ratio of the two: \\(\\log_2(\\frac{\\beta_2}{\\beta_1})\\) (a log ratio is useful because it makes the difference symmetrical: \\(\\beta_2\\) being twice as large leads to a log ratio of 1, while \\(\\beta_1\\) being twice as large results in -1). To make sure we pick up relevant words, we can filter for relatively common words, such as those that have a \\(\\beta\\) greater than 1/1000 in at least one topic.\n\nap_topics &lt;- ap_topics %&gt;%\n  filter(topic == 1 | topic == 5) # Keeping just the topics of interest\n\nbeta_wide &lt;- ap_topics %&gt;%\n  mutate(topic = paste0(\"topic\", topic)) %&gt;%\n  pivot_wider(names_from = topic, values_from = beta) %&gt;% \n  filter(topic1 &gt; .001 | topic5 &gt; .001) %&gt;% # Beta greater than 1/1000 in at least one topic\n  mutate(log_ratio = log2(topic5 / topic1)) # Calculate Log ratio\n\nbeta_wide\n\n# A tibble: 275 × 4\n   term     topic1    topic5 log_ratio\n   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 carri  0.000567 0.00117       1.05 \n 2 lot    0.00574  0.000850     -2.76 \n 3 run    0.00134  0.0000847    -3.98 \n 4 child  0.0206   0.00159      -3.70 \n 5 enjoy  0.000929 0.00102       0.131\n 6 fertil 0.00430  0.00362      -0.249\n 7 grow   0.00193  0.00236       0.289\n 8 hous   0.00166  0.000110     -3.92 \n 9 life   0.00923  0.00209      -2.14 \n10 mean   0.000899 0.00118       0.396\n# ℹ 265 more rows\n\n\nThe words with the greatest differences between the Topic 1 and Topic 3:\n\nbeta_wide %&gt;%\n  group_by(direction = log_ratio &gt; 0) %&gt;%\n  slice_max(abs(log_ratio), n = 10) %&gt;% \n  ungroup() %&gt;%\n  mutate(term = reorder(term, log_ratio)) %&gt;%\n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 5 / topic 1\", y = NULL) +\n  theme_tufte()\n\n\n\n\nWe can see that the words more common in topic 3 include words such as “sperm”, “embryo” and “response” suggesting we may be picking up medical discussion around fertility. Whereas Topic 1 is more centred around “pregnancy”, “parents” and “divorce” suggesting socio-economic …. More exploration would be warranted here…\n\n\n7.3.3 Structural Topic Modelling\nThe stm package has some text pre-processing functions integrated in it. Similar to the steps we did manually in the previous section. The textProcessor function automatically removes a) punctuation; b) stop words; c) numbers, and d) stems each word. The function requires us to specify the part of the dataframe where the documents we want to analyze are documents), and requires us to name the dataset where the rest of the meta data live (pandemic_threads). Notice what happens in your console while function textProcessor.\n\npandemic_threads &lt;- read.csv(\"data/topic-modelling/pandemicthreads.csv\", header = TRUE)\n\nhead(pandemic_threads)\n\n                                                                                                       url\n1                              https://www.reddit.com/r/entitledparents/comments/v2kalc/am_i_overreacting/\n2        https://www.reddit.com/r/BBBY/comments/1144ioj/today_21623_sue_gove_said_bbby_has_a_new_supplier/\n3                    https://www.reddit.com/r/JUSTNOMIL/comments/uizf4z/the_other_son_is_the_golden_child/\n4 https://www.reddit.com/r/hiphopheads/comments/10c2n82/album_of_the_year_25_kendrick_lamar_mr_morale_the/\n5                https://www.reddit.com/r/collapse/comments/y4mqrd/last_week_in_collapse_october_814_2022/\n6     https://www.reddit.com/r/bridezillas/comments/yyda0z/slavedriver_bridezilla_starved_my_mother_while/\n                author       date  timestamp\n1     user_not_found01 2022-06-01 1654099230\n2      DroppingVittles 2023-02-16 1676590184\n3 Fair_Personality_122 2022-05-05 1651762956\n4   freshsupreme_acist 2023-01-14 1673735518\n5   LastWeekInCollapse 2022-10-15 1665837036\n6           Azazeru921 2022-11-18 1668753411\n                                                                         title\n1                                                           Am I overreacting?\n2  Today (2/16/23) Sue Gove said BBBY has a new \"supplier promise\" and more...\n3                                            The other son is the golden child\n4    Album of the Year #25 : Kendrick Lamar - Mr Morale &amp; The Big Steppers\n5                                    Last Week in Collapse: October 8-14, 2022\n6 Slavedriver Bridezilla, starved my mother while she was staying at her place\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       text\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\\\*Sorry for the long/rant post\\\\*\\n\\n&amp;#x200B;\\n\\nSo, back story I had my son at 17 with a boy who was not ready to be a father. Fine. I lived with my son's father's grandparents (my son's great grandparents) for a time because my parents were divorcing, and they had space for us. It was all well and good until my son was about one, and I met my now fianc\\xe9. We started to do things as a family and the great grandparents started to get mad that the baby wasn't with them as much. They about lost it when I moved out on my own. My son was 4, hadn't heard from his dad in over 3 years, and I got served papers from his father asking for full custody of my son. The paperwork was all filled out in the great grandmother's handwriting \\\\*eye roll\\\\*. They even went so far as to write a letter to the judge detailing every time they agreed to watch my son, and twisted it seem as though I was a flighty, irresponsible mother. Furthest thing from the truth.\\n\\nShow up for court and in the end they got every other weekend and a day during the week, and a week in the summer. I say they because the father would quite literally just drop my son off with the great grandparents and leave. He is an alcoholic and would often not show up because of that reason, even lost his license for a time and was still driving my son around. ugh. They would try to manipulate my son (who was 4/5 years old at the time) into thinking I was the bad guy because they would wait until the last second before pick up and get him involved in an art project, or bring out a big new shiny toy, and I would have to tell my son we cant stay and play, or take it with us because we had a small apartment, he would cry and they would comfort him. This went on for a some years until the pandemic hit (my son was then 9/10 years old) and my son's father up and fucked off, again. The great grandmother herself suggested my son not be around his father at that time because his father was/is not taking precautions to be safe. He hasn't even texted me asking about my son since May 2020.\\n\\nHowever, the great grandparents have wanted to see my son, who is now 13, and I was allowing it for a time, but these people just show over and over their complete lack of respect for me as a parent. We would establish a time for the visit and I would ask my son come home at a certain time. They would wait till the last possible moment and call me to say they started a movie and would be late, or ask if he could spend the night, and if I said no they would be like \"your mom said you cant stay for more fun\". She would text him and make these plans and then not ask me until the last minute and when we had plans already it would be the same deal. I asked them to ask me first, not my son with plans. They then talked about a birthday party for a cousin in front of my son so that it was my son asking me about it, then again they wait till the last minute to make the plans. I tell them we as a family with a child too young to be vaccinated (my daughter) are avoiding large crowds, they take my son to the mall at peak Christmas time. I tell them my son has a fever he cant attend a basketball game, they call him and tell him to \"have his vaccine card ready\" because they are going to try to convince me to let him go any way, then they tell him all the fun things he is going to do there (of course I didn't let him go he was sick). This woman SHOWS UP AT MY HOUSE with out telling me and my sister was home with my daughter, scared the crap out of her, and when I told her not to just show up at my house she said \"well I didn't see any cars in the drive way so I thought it was fine\".  They get on my case all the time saying they \"don't know what my problem is\", but they are my problem and the lack of respect they have for me. The last time they picked my son up they told me they were going to take him to a place near me, but then I find out they took him about 45 min to an hour away and didn't think that that was something they should have clued me in on.\\n\\nI really don't want to send my son with them any more, its not like he is asking me to go there or see them. Am I overreacting?\\n\\n&amp;#x200B;\\n\\n\\\\*\\\\*\\\\* EDIT to clarify, we are no longer following the every other weekend court order, haven't since my son's father walked out again. The my son seeing his great grandparents is purely voluntary, because I have no legal obligation to them. Thankfully.\n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Take from this as you will...\\n\\nSource: [https://businessofhome.com/articles/bed-bath-beyond-bought-some-time-will-it-be-enough](https://businessofhome.com/articles/bed-bath-beyond-bought-some-time-will-it-be-enough)\\n\\nFull article: (boldface is mine)\\n\\nLet\\031s go with the assumption\\024as much of a stretch as it may be\\024that Bed Bath &amp; Beyond manages to pull off its Houdini-like magic trick and gets the financing to continue to stay in business (and out of bankruptcy). Then what?\\n\\nIf you consider the company\\031s original store format in the 1970s and \\03180s\\024back then it was just \\034Bed &amp; Bath\\035\\024as 1.0, and its 1987 expansion into the \\034Beyond\\035 hard goods categories as 2.0, then the private brand era of the past two years was 3.0. On tap is the next version: BBB 4.0.\\n\\nThe beleaguered retailer\\024which has secured new funds with the expectation that more are on the way\\024has sketched out a new strategy that is long on optimism but short on specifics. In a statement last week, CEO Sue Gove focused on the big picture: \\034This transformative transaction will provide runway to execute our turnaround plan. We continue to put our customers at the center of every decision, positioning Bed Bath &amp; Beyond to meet and exceed their expectations, while resetting our foundation for near- and long-term success.\\035\\n\\n**On Thursday afternoon, BOH was able to access two calls during which BBB executives provided further details to its suppliers\\024one call was with conventional vendors, and the other one was with direct-to-consumer brands. Most important, they indicated BBB would begin to pay for merchandise \\034in advance or COD\\035 depending on the vendor\\031s preference. They also said DTC suppliers would start getting paid immediately after BBB was paid by shoppers for orders that are fulfilled directly by the vendor.**\\n\\n**Executives also said they would be moving to net pricing terms, which would eliminate many of the special charges the store has used in the past to reduce its payments to its suppliers. It\\031s all part of what Gove called the company\\031s new \\034supplier promise.\\035 New interim CFO Holly Etlin was blunter when she said BBB was taking the next few months \\034to clean up its act.\\035**\\n\\nBeyond new payment terms, here\\031s what we know so far about what management has in mind going forward:\\n\\nStores: From its high of some 1,500 stores just a few years ago, Bed Bath &amp; Beyond is radically slashing its fleet and says it will end up with about 360 stores, plus another 120 BuyBuy Baby locations. All stores in Canada\\024about 65 between the two nameplates\\024are closing as the company exits the country entirely. (Curiously, it has said nothing about its joint venture in Mexico, where it has a handful of locations.) The new U.S. footprint will represent a drastic reduction, with broad swaths of the country having few if any stores left.\\n\\nE-commerce: The company has not shared what percentage of its overall business is done online, but the last time it did, in the first quarter of 2021, it was 38 percent, albeit during the pandemic conditions when everyone was shopping online. All Bed Bath &amp; Beyond will say now is that \\034the digital channel is expected to rise to a higher proportion of sales with improved channel profitability.\\035 Yet none of its statements on how the new funding will be used make mention of investment in its subpar digital operation.\\n\\nMerchandising: Here\\031s where it gets really vague. \\034We are prioritizing availability of leading national and emerging direct-to-consumer brands our customers know and love,\\035 said Gove in a press release, which probably means the company will continue to de-emphasize private label merchandise and bring in more DTC brands. But these are the same national brands Bed Bath &amp; Beyond previously carried in its 2.0 era\\024the same period of decline that led to the pivot toward private label goods. On the DTC front, the retailer already carries Casper and has started to bring in some other digitally native players, but all of these brands are pursuing physical store distribution on their own as well, and it will be hard for the company to stand out.\\n\\nPhysical format and fulfillment: This one is intriguing. **\\034The Company will also be pursuing asset-light inventory management strategies to drive growth, including vendor-direct-to-consumer, marketplace and the potential for innovative collaborations,\\035** said the release. The vendor-direct strategy for online orders is something BBB has been doing for years, as have most other retailers. (Wayfair, in particular, takes very little ownership of goods, relying on its suppliers to handle fulfillment.) But what does that approach mean for stores? Is the company suggesting leased or consignment departments that vendors run and staff? While that is a component of department store retailing for luxury brands and beauty, there\\031s very little of that in the home space. Only ABC Carpet &amp; Home in New York pursued that strategy in its heyday, and it has never been scaled up to any degree anywhere. **\\034Innovative collaborations\\035 could mean just about anything\\024or, frankly, nothing.**\\n\\nOperations: Bed Bath &amp; Beyond says it expects to achieve significant cost savings as it drastically reduces the size of the company. \\034Supply chain, technology, expense structure and business processes will continue to be streamlined as the company realigns its operational foundation\\035 was how Gove put it. **That process has already begun with serious layoffs at corporate headquarters** in Union, New Jersey, and with hundreds of store-closing sales underway.\\n\\nWill all of this be enough? And will it be in time? The new financial lifeline investors have extended is not all that long, and in the meantime, current sales have likely been severely diminished\\024at least one independent source reported that many stores are managing inventory levels off 40 percent from optimum.\\n\\nAll of those bankruptcy headlines can\\031t have helped customer enthusiasm\\024or worker morale, for that matter. Bed Bath &amp; Beyond 4.0 is most certainly a work in progress. Grove has to hope it will indeed result in progress.\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               MIL has always been a pain in the butt. When my kids were babies, she was the MIL who refused to honor my wishes re: no kissing the baby, no \"fake biting\" the baby. Yes, that was her thing. She wanted the right to fake bite my baby's arm. It was her way to bond. She declined an invitation to attend a birthday party in the park, because celebrating outdoors is beneath her.\\n\\nFast forward to the pandemic. As soon as it hit, MIL was isolated due to the fear of covid. She is healthy but age is definitely a factor. So, we respected her wishes and we didn't see her for 18 months, until she was fully vaccinated. Then, she only wanted to see us outdoors! And her excuse was that our children weren't vaccinated yet (at that time, vaccines weren't offered to children).\\n\\n We saw her that one time on our deck, where she sat and ate and barely interacted with the kids. \\n\\nSince then, my kids got their full vaccines but MIL changed her tune and said the risk to see them was too high.\\n\\nA few weeks ago, my husband's brother's father-in-law passed away. There was a memorial service held in a tiny hotel reception area. Quite obviously indoors. Packed with people Food was served and masks were only worn when people weren't helping themselves to the buffet. Yes, there was a buffet. But my covid-conscious MIL attended. And she took off her mask to help herself to the buffet. We attended as well. She saw my husband and squeezed his cheeks with her dirty hands, pointed out his bald spot, and asked if the suit he was wearing is the same one from high school (20 years ago. It wasn't). She ran around quacking about her now she's fully vaccinated, times 4, she needs to move on with her life. She's done everything she could.\\n\\nIn the 2+ years since covid hit, my husband had surgery  and had to be off his feet for months. I worked from home, took care of the kids' virtual schooling, and did ALL of the errands that my husband used to help out with. No help from MIL. No concern.\\n\\nSo, my kids have a cello recital. They both worked had. They've won awards in the past. This is important to them. We invited MIL and she declined. It is a much less covid-hazardous event. But my MIL has seen her other son in person, has been inside his home, has attended his father-in-law memorial service. My kids get nothing. \\n\\nI am from Ukraine. As an immigrant, though not recent, I have no extended family. My parents died when I was a teen. This is it for grandparents for my kids.\\n\\nThat's it. I just wanted to vent. Give it to me straight. Maybe I'm just delusional.\n4 Artist : Kendrick Lamar\\n\\nAlbum : \\034Mr. Morale &amp; The Big Steppers\\n\\nApple download link :\\n\\n[https://music.apple.com/us/album/mr-morale-the-big-steppers/1626195790](https://music.apple.com/us/album/mr-morale-the-big-steppers/1626195790)\\n\\nSpotify link : [https://open.spotify.com/album/3OqPkYVDzHKistrI9exrjR?si=ha-Ln-\\\\_wSleWQa1Lo9MgMQ](https://open.spotify.com/album/3OqPkYVDzHKistrI9exrjR?si=ha-Ln-_wSleWQa1Lo9MgMQ)\\n\\nYouTube : [YouTubehttps://m.youtube.com : playlistMr. Morale &amp; The Big Steppers](https://m.youtube.com/playlist?list=PLjB_8hSS2lEMY-ap4zdPv0-mbTwxtN7KW)\\n\\nWho is **Kendrick Lamar**????? I can\\031t imagine this is worth answering in 2022, but he\\031s a good kid from a MAAD city (a.k.a Section 80) and he taught us how to pimp a butterfly while the whole world said DAMN to even his most untitled and unmastered hits. Do ya googles\\n\\nAs I get a little older (literally today is my birthday) I realize life is about perspective. That one word, changes how you and i give and receive information.\\n\\nSo my perspective is that i heard of Kendrick early. I can\\031t say exactly when. It was the infamous mixtape era for sure. But that time is honestly just a blur of datpiff cover arts and cigarillos minimally filled with weed hoping for a maximum high. I do remember downloading both of his mixtapes. Not on purpose hilariously enough, no i was a music junkie so anything that looked dope got downloaded in mass. It was the *Kendrick Lamar Ep* and *Overly Dedicated.* It just didn\\031t get played. When i finally did, to add more weirdness, i didn\\031t care for overly dedicated. I really don\\031t have to explain, i do think theres too much of that going on these days, but in this case i want to. I vaguely remember why i didn\\031t care for it, but it was mainly that the songs like Michael Jordan etc that i just wasn\\031t as into at the time. Not bad music by any stretch, but at that time i was super backpack. Im talking Ghostface Killah, Mos Def, Lupe, Mood Musik times. So it wasn\\031t often that i played more up tempo or even braggadocios music. I went to college somewhat knowing who he was, but still not really playing his music. I don\\031t think i ever played the Kendrick Lamar Ep, which truly makes me think things happen for a reason because if i had, he would have been one of my favorites from them.\\n\\nIn college i made one really good friend early. He was from California and we were in Kentucky where i was actually born. Somehow we just got each other more than most. The kind of friendship i never ask you do you have weed, only do you wanna smoke when i do. He actually brought up Kendrick again, BUT, and maybe I\\031m the only one that this has happened to, but he played some of the O.D mixtape in a different light and i loved it. Im not sure how but by the time i doubled back to the intro, i was in awe. Im not sure how i missed it, as the display of lyricism and content was enough for me to fully start to appreciate his music. In this same year span, he not only was featured on Mac Miller (another person who i didn\\031t like at first, but once he started talking about drugs and the life i lived i loved it), but Section 80 came out. So, perspective. Section 80 is my good kid MAAD city, if i had to vote for a perfect kendrick album it would be TPAB, and my favorite album to listen to by Kendrick as of now is Mr. Morale and the Big Steppers. Ill explain\\n\\n***United in Grief*** is an interesting opener. While this is one of my favorite albums of the year, this record in particular may be my least favorite. When I FIRST heard the album I listened to every record no skips. There is no bad music on this album. But this album does introduce an interesting concept. Should music be forever? Pineapple Express is one of my favorite movies, I\\031ve seen it a thousand times and I can quote the entire movie if I wanted to. So by that definition is an amazing movie. But on the other hand, seven pounds changed my life. I have only seen it maybe 6 or 7 times because I love tragedy. But with such a deep impact, it\\031s also great. That is what Kendrick is introducing to music. He knew we would all tune into track one at least. So he said all of the things he felt he needed to in a run on sentence type fashion. A line that sticks out to me is \\034**I grieve different**\\035. I never really related to Kendrick. Loved his albums, have been a fan since around 2010, but I didn\\031t relate. This felt like one of those universal lines. Some people take off work and grieve. I bought more weed and cried on my way to work about my grandmother for a week. We ALL grieve different. While he was \\034quiet\\035 through the pandemic, he like all of us learned something. In this case, this is the first time I\\031ve heard Kendrick talk about the other side of being who he is. So the statement hits extremely hard because you know Kendrick watched everything that happened, and one of the first thoughts he wanted to share with us when he came back is \\034**I grieve different**\\035.\\n\\n***N95*** is my album opener. When I pull up this album, I\\031m clicking on this to start my run. I don\\031t listen to the entire album anymore, as I respect it as a piece of art that doesn\\031t need replay value. Yet another example of Kendrick being ahead of the curve. Art doesn\\031t have a singular definition, yet sometimes rap makes it seem like it does. Albums like Testing are crucified for straying too far away from the artist\\031s discography. I lived through Yeezus and let me tell you, nobody liked it originally. Don\\031t even get me started on 808s, I specifically recall being ridiculed for liking that \\034weird shit\\035. So in a world where most people are putting on so many different things to fit in, here Kendrick is saying take it off. Not one to waste a moment, the moment he\\031s told you to take off every single thing that you\\031ve put on without it really being you, he tells you \\034**ughhh, you ugly as fuck**\\035. Clearly he\\031s had enough time to analyze himself as well as the world 4 times over. It could easily be one of the deepest lines of the record if you think about it. Yes you\\031re ugly. So am I. That\\031s what makes us beautiful. You could surely take that line at face value and say the cool kid from the mad city is back on his bragging, but I disagree. This record may have more of the traditional Kendrick markers from his albums pre hiatus, but it seems like he\\031s bridging us to something new. Even with all his success in the first half of his career, he still changes and pushes boundaries. \\034**Ohh you worried about a critic, that ain\\031t protocol**\\035 says it best. With lines like that, it\\031s clear to see that if Nas isn\\031t relevant, the kids are being raised wrong.\\n\\nI told you that I didn\\031t really relate to Kendrick. He was always a cool kid to me, not just because of his music, but the way he\\031s been perceived as well. Ever single person from that era has had at least one negative narrative except Kendrick. I\\031ve spent hours arguing this exact thing with friends. J Cole was quote unquote boring. Big Sean was called corny. Childish was seen as soft. Not saying any of these are true, just proving my point. I\\031ve always debated Kendrick is a mastermind that watched the complaints his classmates received and did extra curricular work to be sure he was never the victim. I was unpopular so I related more to Childish. But when Kendrick said \\034 **I don\\031t know how to feel, like the first time I fucked a white bitch**\\035 on ***Worldwide Steppers*** I was instantly transported back to that dorm room when i didn\\031t know how to feel. In was in college, which was the first time i had ever really interacted with white girls. Here i was, asking myself if this was worth the perception that would follow, if i was actually wrong for doing it, could i still be a black leader once people found out all crossed my mind before i even had the condom on. Most people will hear that and not think much of it. But to me, especially with the fact that kendrick had a exact recollection of each time he crossed that line, i believe he too sees it as somewhat of a traumatic event. Not the act itself, but the self questions and self blaming that comes with that for a black man isn\\031t something a wise man can just shrug off.\\n\\n***Die Hard*** would be the record that i originally started at after several, and i do mean plus 15 listens of the full album from the day prior to release on. Once digested, i started to pick my favorites and Die Hard just captures everything i didn\\031t know that i wanted from Kendrick. This is the first song I knew I\\031d be playing for the foreseeable future due to it containing mantras like \\034**I hope I\\031m not too late, to set my demons straight/I know I made you wait, but how much can you take**?\\035. For a person who has admittedly not achieved as much as I want in life, words like this are golden. Who can\\031t relate to wanting just a bit more time to get everything right? I\\031m 31 and I\\031m learning more from adopting two huskies at once than I can recall in most of my years of school. It\\031s becoming clear he has tapped into what could be seen as universal truths, whereas before you could argue Kendrick was speaking to a specific audience with each album, regardless of how well the album did. It\\031s like Jay-Z\\031s album Reasonable Doubt. If you judge by the lingo and the content on the album, you could argue it was only for high level players and very established drug dealers. Kind of like a memo among a company. But with this album, and maybe for the rest of his career, Kendrick isn\\031t just talking to Compton. Or to black people. Or to the youth of the lost age. He\\031s talking to the world, which to me, is another mark of his genius. \\034**I hope you see the god in me, I hope you can see, and if it\\031s up stay down for me**\\035 isn\\031t a complex scheme, but as well all know you can be looked down upon for things you said or done. Here Kendrick is pleading for you to see the divine root in him despite human actions that take place daily. \\034**I get emotional about life/the lost ones keeping me up at night/the world be reminding me it\\031s danger/I still risk it all for a stranger/if I told you who I was would you use it against me?/right or wrong, no stone, just love to send me**\\035 is extremely profound. I will not use extreme examples here as that could be too controversial and ultimately not worth the metaphor, but let\\031s take Dave Chappelle. As a member of his Reddit for a while I was proud to see how people didn\\031t let media narratives dictate who he was to them. Then the Elon musk incident happened. There were quite a few people who were quick to bash and denounce him because it \\034was cool to do\\035 over a single joke he made they didn\\031t care for. Kendrick\\031s lyrics make me ask the same question again, is there no kind of system where we can properly account of the all the good deeds someone has done when we finally find a bad one? Will smith was safe. Like for a black guy safe lol. Up until that slap not one public incident or outburst. I think we can all recall how many \\034he should be in jail\\035 phrases were thrown around. What is the point of telling people who you are if they\\031ll use it against you? When something happens most people quickly say how they never cared for you anyways. I\\031m not saying that you should blindly agree with things you don\\031t, and neither is Kendrick. No singular action makes you good, nor evil. It is the sum of all actions multiplied by your intentions. But even then, should we as humans be able to definitively say either way? Slink Johnson said it best, let ye WITHOUT sin cast the first stone. \\034**I wonder when I lost my way/been waiting on your call all day/tell me you\\031re in my corner right now/when I fall short I\\031m leaning on you to cry out**\\035 further pushes this point. Maybe we all are just one phone call from a fan or loved one away from finding our path again.\\n\\nWith the opening of ***Father Time***, I find myself chuckling. I\\031ve just never really known much of anything about Kendrick. More of an enigma. But for him to say what I\\031ve said to my (ex) girlfriends countless times was hilarious. It serves as yet another glimpse into situations he\\031s been in. \\034I **got daddy issues, that\\031s on me**\\035 is a perfect example of the accountability that\\031s present all over the album. In reality, most of us have some issues with how our fathers did, or didn\\031t, raise us. Even though this is an old concept, if people like Kendrick don\\031t reiterate it, it\\031s usually lost from one generation to the next, even as far as him identifying to still have those problems at his age is on him. One of my absolute favorite lyrics from the album is \\034**When Kanye got back with Drake, I was slightly confused/guess I\\031m not mature as I think, got some healing to do**\\035 and it\\031s for good reason. This was something I had only talked about with close homies and hadn\\031t heard too much publicly about. If my understand of that beef and the events that took place (like Drake allegedly sleeping with Ye\\031s mother in law as well as Ye saying he found Kim in the bed with Chris Paul and there being videos of Drake wearing his jersey shooting shots) I can\\031t understand why he would become \\034cool\\035 with a guy like that. Drake literally said on his new album, \\034**linking with the ops, bitch I did that shit for j prince/I did it for the mob ties/feels like seventeen, two Percs, frog eyes/and I\\031ve never been the one to go apologize/me I\\031d rather hit \\030em up one more time**\\035 so I was just as confused that anyone would allow someone who\\031s openly duplicitous in their friend circle. Maybe I\\031m not as mature as I should be either because after a beef like that, we don\\031t gotta be enemies. It\\031s enough people dying as it is. But we surely don\\031t have to be friends. And for Kendrick to touch on that it was a huge moment, the kind where you realize even in your most outlandish takes, you\\031re not alone.\\n\\nI have not listened to Kodak Black, ever. If I\\031m not mistaken when he came out I wasn\\031t as musically accepting, as I was a backpack kid. As open as I am with music, I still have not downloaded a single album of his. So for someone like Kendrick to put his record ***Rich (interlude)*** on his platform, it\\031s major. I sat and listened to every bar of Kodaks at least 15 times, which is much more than I would have when left to my own devices. I think tracks like this are important, not just to expand horizons but also build more brotherhood in a business that we invented but do not control. It didn\\031t make me a fan, but it did make me realize he definitely has something to say, which I\\031d argue is more valuable. Fans have expectations but if I think you have something to say I may give you multiple chances to do that.\\n\\n***Rich Spirit*** is another example of what Kendrick brings this offering. A super catchy hook, with the new mind state courtesy of everything he\\031s been through as well as talks with his therapist. \\034**Rich n\\\\*\\\\*\\\\*a, broke phone/tryna keep the balance, im staying strong**\\035 sounds like the words that got him thru the last 5 years of not releasing any music. It shouldn\\031t be undervalued to be the type of artist that can make music other artists will gravitate towards. Kendrick floats effortlessly all over this track, which is absolutely typical. I feel like some things are worth mentioning and some aren\\031t. In 2022, if you\\031re not aware of the typical things that Kendrick does, this review probably isn\\031t for you. I\\031m of the beliefs two type of people are coming to this review. People who want to see their favorite album of the year celebrated, or people who didn\\031t like it and want to see why others celebrated it. By the end I hope to answer both. This is the Kendrick that I will take with me for the rest of my life. Realistically this album covers a lot more themes and issues I\\031ll be dealing with moving forward, where as songs like Backseat Freestyle don\\031t hold as much weight as they did when I was in college. This is inevitable. Huge Jay Z fan growing up, but I play only 3-5 of his albums now. In my opinion there should be mutual growth. Human nature is to evolve. Rich spirit showcases some of what we can expect from Kendrick at this stage in his life, and content wise I believe this is as good as it gets. Personal confessions over minimal production will always have a warm place in my heart anyways so maybe I\\031m biased, but when someone is vulnerable and can show their flaws as well as their triumphs, I don\\031t think there\\031s anything better when it comes to music. The original purpose of music was to convey a message, so its great to see thats not lost here\\n\\n***We Cry Together*** has to be in the top 10 of all time for hip hop love songs. Yes, I said it. Let me be clear. Is this a toxic relationship on display? Absolutely. It\\031s hard to argue that saying \\034fuck you\\035 constantly back and forth isn\\031t toxic. BUT. Who is really able to fully pass judgment? Romeo and Juliet is an amazing love story. But how many times have you seen a guy/girl willing to die rather than live without their love? Now how many love stories have you heard of like We Cry Together? Much more common. It reminds me of a 2pac line \\034**they say Jesus is a kind man, but we should understand times in this crime land**\\035. To me, there should always be a variable when you add up any equation that should represent how things change. So if you take away all the judgment, this song is beautiful. It\\031s on the list of songs that I don\\031t often play anymore, which throws back to my original point that Kendrick is introducing two different types of music here. The kind you\\031ll keep with you in your pocket, as well as a song you could hear once and it will change your life (Kim anyone?). The dialogue here is also TOP NOTCH. He didn\\031t just do a toxic love song, he did THE toxic love song that anybody could play and get a good glimpse of the gender relations of today. It\\031s even more appropriate that their \\034f**uck yous**\\035 turn to \\034**fuck** **mes**\\035 once the couple has vented all their negative emotions. Even with the title you have to respect his intelligence to take the typical phrase \\034we love together\\035 or \\034we pray together\\035 to one of its natural opposites. That\\031s the truth of life that not every artist touches on. To throw back to an earlier record, he\\031s just \\034**tryna keep the balance**\\035 and i couldn\\031t appreciate it more\\n\\nIm just gonna say this and leave it here. \\034**Shut the fuck up when you hear love talking**\\035 is one of the best lines on the album. Hell, in the last 10 years of rap its one of the best rap lines. Why? Everybody remembers, \\034**i am the beast, feed me rappers or feed me beats**\\035 right? To me, this line has an equal amount of staying power. Lil wayne was capturing a whole mixtape era with two sentences. So with Kendrick\\031s line, i feel like post pandemic, this is equally as important. Personally i think thats why subreddits like r/wholesome or r/brosbeingbros are flourishing. People want to sit silently and watch moments of humanity being as great as we can, because we have all watched the ugly side of humanity for at least the last two years. When i heard this song it instantly put me in such a good mood. Kendrick and summer and two vocals i wouldn\\031t have requested to be overlapped before, but now hearing their chemistry i can\\031t be the only one hoping summer doesn\\031t show up again, especially considering the days when Kendrick, or most of TDE showing up with Jhene or Sza are over. Ghostface showing up is everything to me. He delivers a very potent verse, but its more important to me that a kid from Compton recognizes one of the Wu Tang greats. While the first half of the CD definitely had some moments for me that made it worth the wait for Kendrick, in all honesty i was a little let down my first listen. Not because its bad music, not at all. I just by the highs. *Die Hard*, *N95*, *Rich Spirit*, and ***Purple Hearts*** are such high highs that in comparison every track didn\\031t match up to me. I know what you\\031re thinking, how on earth could this be one of his favorite albums of the year if he\\031s said he only really likes 40% of the first disc? Well&\\n\\nIn case you didn\\031t know, ***Count Me Out*** is one of the best intros in years. Not only that, but this is that record that your favorite, and your favorite, and his favorite all recognize as a kind of \\034hey, this is how well i rap\\035 record. Its just top tier lyricism. The way the choir in the background just kind of ushers you down the aisle as Kendrick spills in-between claps feels like an opening to a Devil May Cry or an Elden Ring type game (if they knew how to pick rap music that is). This is one of the rare occasions where, even for Kendrick Lamar, someone who has always had a good flow on any song, showcases one of his best flows ever. @ somebody you want to debate that with as long as it isn\\031t me. Maybe its the way the bass hits underneath Kendrick saying \\034**fuck** **it** **up**\\035. Maybe its the song structure that sounds more like a two verses with a million bridges in-between. \\034**i made a decision, never give you my feelings, fuck with you from..fuck with you from a distance**\\035 is a prime example. The way it feels, its like watching the million man march form person by person as Martin Luther King speaks slowly rising in tone until the entire town can hear him. But I\\031m a pot smoking imagineer don\\031t mind me lol. There is no wasted bar here period. \\034**When you was at your lowest tell me where the hoes was at/when you was at your lowest tell me where the bros was at**\\035 is definitely not the typical statement from a rapper on top. This kind of self awareness is present all over the album, but not in the way that you feel you\\031ve attended a college lecture, but more like you met a wino who had 4:42 minutes of game from all the mistakes he\\031s made to give you. If you can play this record for day ones and \\034just heard him todays\\035 then it his the universal chord. The same reason Micheal Jackson could reach the whole world with just his voice. Rich or poor, i doubt theres a single person who can\\031t relate to \\034**you said id feel better if i just worked hard without lifting my head up/that left me fed up/you made me worry/i wanted my best version but you ignored me/then changed the story/then changed the story/**\\034. Thats the beauty of a talent like Kendrick. Even i made the mistake of counting out Kendrick when my friend told me whenever he comes back the world will listen. This record made me realize i couldn\\031t have been more wrong.\\n\\nOne thing I\\031ve always been curious about, is how Kendrick sees himself. I know how the world does. Ive been a huge J Cole fan long enough to have had one or two conversations about who i think is doing the best from that class. Kendrick was more often than not peoples answer. Ive always had an affinity for risk takers and people who will do an entire project in such an artist fashion that people who don\\031t love music can\\031t enjoy it as much as someone like me. 4 Your Eyez Only comes to mind. You\\031ve heard it a million times. If \\\\_\\\\_\\\\_\\\\_ did that, it would have been the biggest thing the world. It seems like a digression, but its not, because this is why I\\031ve always wondered what does Kendrick think. Sure he did control. But does he really think he\\031s the best out? Or does he hear bars like Big Krits verse on \\0341 Train\\035 and wish he could have did that? ***Crown*** kind of answers that. For better or worse, he acknowledges he wears a crown while saying things like \\034**i can\\031t please everybody**\\035. One of the more shocking statements from an artist who, to me, seems to have done exactly that thus far. Sure, there are people who don\\031t like this album or that album. But everybody has at least one Kendrick album they enjoy, unless they don\\031t enjoy him at all. They are in the minority. \\034**They idolize and praise your name across the nation/tap they feet and nod their head for confirmation/promise that you\\031ll keep the music in rotation/thats what i call love**\\035 could be taken as a brag, but to me it sounds like he\\031s aware of the double standard. When you\\031re of a certain caliber, people love you. But their love comes with the condition that you produce in the same fashion that you have. Don\\031t believe me? Check out r/FrankOcean when you get a chance. Love without expectations isn\\031t something most people have ever received, let alone given. \\034**Heavy is the head that wears the crown/to whom its given, much is required now**\\035 further cements my point. \\034**One thing I\\031ve learned, love can change with the seasons**\\035 drills home that he overstands how love works, but he doesn\\031t sound fazed. One thing is for sure from this record, Kendrick sees himself as a king of the game, even if its a reluctant title\\n\\n***Silent Hill*** reminds me of his earlier works. Mainly the hook reminds me of the love hate relationship i have with similar hooks (Hood Politics was corny at first, but then i came to love it after so many listens). This one, however, i jumped straight to loving. The meticulously placed hard hitting bass might have helped, and maybe its Kendrick finding his pocket constantly in such an esthetically pleasing way, but somehow \\034**pushing them all off me like hhuhhhhh**\\035 translates instantly even with the extreme animation. Honestly, even with how i initially receive records like this from, i still think its dope that he does it. This is the one glimpse of kendrick we have constantly gotten, and it goes further back than you\\031d think, Cut You Off being one of the earliest examples i can think of. The animated type hooks should be a signature of his and thats dope because its something he enjoys, not something he\\031s doing to make the most pop record ever. THIS Kodak verse did it for me, definitely one of my favorites. If it wasn\\031t for Baby Keems performance, this would have been my favorite feature even over the Ghostface because it came from someone i didn\\031t expect it. Speaking of Keen&\\n\\n***Savior (Interlude)*** is my favorite interlude. But its not fair at all, because *The Melodic Blue* was one of my favorite projects that year. So when i heard him going in, i was on board. Its lines like \\034**the engineer dead if the drive don\\031t back up**\\035 and \\034**my uncle had told me the shit in the movies could only be magic/this year i did 43 shows, and took it all home, to buy him a casket**\\035 that show exactly what i mean when i say I\\031m a fan of his. The former being a reference to the Atlanta episode (this isn\\031t confirmed but i don\\031t care lol it fits too well) when they were in the studio with the Clark county guy? His name escapes me but he made the yoohooo record. They sat as he recorded and as he did a freestyle type verse, the engineer said the computer crashed. Clark then proceeds to say \\034don\\031t crash it again\\035 \\034if it crashes again, imma crash my foot in your ass\\035. Mind you, its definitely a technical issue, but thats the hilarity. The program ends up crashing again, where Clark takes a walk and his two goons say \\034y\\031all should go home\\035 to Darius and PaperBoi which is the universal sign from one black person to another that some illegal shit is about to go down and y\\031all not gonna want to witness it. The second line is what makes Keem great in his own right, he has a different type of awareness thats just as potent as the one Kendrick has found on this album. The idea that a family member would basically tell you that all the stuff in the movies is make believe, only for you to go out on a world tour that would have made him a believer had he lived to see it. Profound perspective delivered in an entertaining way is Keem.\\n\\n***Savior*** is another high high undeniably. \\034**Are you happy for me**\\035 became one of the questions everybody was asking post this album\\031s release, and rightfully so. He said it best, \\034**Kendrick made you think about it, but he is not you savior**\\035 If you didn\\031t notice the cover art, he has a crown of thorns with is a well known reference to the story of Jesus\\031s crucifixion. \\034**One protest for you, 365 for me**\\035 is the perfect way to say that if he stands up for something, thats his view now. Whereas a person can protest two opposite ideas and nobody says anything. \\034**Smile in my face, but are you happy for me? I\\031m out the way, are you happy for me**\\035 sounds like the response to people who said Kendrick should have been front and center when the world \\034needed him\\035. \\034**see the christians say the vaccine mark of the beast/then he caught COVID and prayed to pfizer for relief/then i caught COVID and started to question Kyrie/will i stay organic or hurt in this bed for two weeks?**\\035 is suchhhh a potent line. This is the duality of a lot of americans. I refuse to get political. So many people claimed it was this or that. But when their life was in need, their prayers are altered then and only then. Personally i don\\031t believe in this \\034opinion culture\\035 where you\\031re asking basketball players about health policies and rappers about socialism as if thats the source of the best info. 9 times out of 10 you get the same answer you\\031d get if you asked a 40 year long teacher of math only about what happened april 1921 in oklahoma. A good guess, but its not his expertise. \\034**The cat is out the bag, i am not your savior/i find it just as difficult to love thy neighbors**\\035 needs no explanation. \\034**the struggle for the right side of history/independent thought is like an eternal enemy/capitalist posing as compassions be offending me/yeah suck my dick with authenticity**\\035 captures what i don\\031t think a lot of people understand. We are in a herd mentality time, where if you don\\031t agree with the herd, youre wrong. You\\031re either good or bad, no in-between. The second half of that line directly made me think back to the pandemic year, when crimes by the cops against minorities were at a high, and every company came out and said \\034we stand with you\\035. Mind you, they\\031re still supporting the people in power. Mind you, they aren\\031t actually throwing their weight around to ensure that black lives matter. No, its easier to say these things because you\\031re selling a product and you want to SEEM progressive. Seem being the operative word as most American companies either have blood or racism all over their hands, or both. Its just hard to trust statements from these companies knowing full well when things die down its back to business as usual. My favorite line from this song is \\034**and they like to wonder where I\\031ve been/protecting my soul, in the valley of silence**\\035\\n\\n***Auntie Diaries*** is probably one of the most divisive records on the album, if not the most. In the times we are in, everybody is sensitive and nothing can be said. I don\\031t really see how we can ever progress as humanity, if every time somebody does a singular thing we don\\031t like, we shut down and ignore them completely. There was a Dave Chappelle joke about the LBGTQ movement on one of his specials where he compared it to people on a car ride. I won\\031t butcher the joke, but it was brilliant. It even spoke to what some may see as tension with african americans and the LGBTQ movement, but again I\\031m paraphrasing \\034We aren\\031t mad at your movement, we admire it because you\\031ve gotten so much further than we have in a short time\\035. It basically highlights that whatever movement white people are apart of, gets more traction while we are still fighting for equal treatment across the board. I brought this up to hopefully highlight, even with the best intentions, people can still get offended. This is what I\\031ve found to be the case on this record, as i heard many people shaming him for \\034mixing pronouns\\035 or the past tense way he spoke, or even phrases like **\\034my auntie is a man now**\\035, while clearly rooted in him explaining he understands the problems the community faces, as well as taking accountability. While its not pleasing for everybody to hear, there was a period where saying somebody was \\034gay\\035 was an insult and people used \\034f\\\\*\\\\*\\\\*\\\\*\\\\*\\035 constantly for a number of different reasons. This happened. As much as american loves the \\034my hands are clean now\\035 approach, the fact still remains a lot of those things were not that long ago. I loved this record because it was an honest discussion on how we saw it from his point of view, and how he\\031s changed. But the world sees records like this as tone deaf. Makes me think back to a coment I made recently. If we censor the Dave Chappelles, what can we expect from further generations but more Kevin Hart?\\n\\n***Mr. Morale*** contains my second favorite flow for the entire album. The records where Kendrick\\031s rapping ability overshadows what he talked about are almost non existent on this album, which while thats refreshing, i also appreciate a silent hill or mr morale existing on such a heavy album. \\034shit on my mind and its heavy/tell you in pieces cuz its way too heavy\\035 captures that idea exactly. When you add in lines like \\034transformation, i must had a thousand lives and like 3 thousand wives\\035 the picture is clear. Honestly, that could have been the tag line for the entire album, because as this is still the Kendrick we have come to know from a distance over the years, this is the first up close view of his life. It would seem that in the years in-between he went through some hyperbolic time chamber to exponentially increase all his stats in a small amount of time. While he does touch on subjects that are lesser known, the next track is a much more concise journal of those\\n\\n***Mother I Sober*** is probably the one track that captures everything that Kendrick has been through both before fame, during, and after DAMN. I would basically have to paste every lyric to really capture the depth here, but lines like \\034y**ou haven\\031t felt grief until you felt it sober**\\035 and then the flip \\034**you haven\\031t felt guilt until you felt it sober**\\035 punctuate the verses they come from. Truly not a song to be spoiled until you hear the way Kendrick delivers it, it\\031s the ultimate example of a song changing your life and perspective even if it\\031s just heard once. I, on the other hand gravitate to this album because I\\031ve had so many similar experiences, this song is like a silent last course for an album completely based in being food for thought. \\034**I wish I was somebody, anybody but myself**\\035 could not resonate more with me. Kendrick did what he hasn\\031t before with this album to me, and after I discuss mirror I\\031ll explain\\n\\n***Mirror*** is my favorite record. It\\031s the first record I ever wanted to replay, from literally my first listen I doubled back a few times before hearing the album. \\034I choose me\\035 is undeniably one of the most powerful lyrics. I can only speak from experience, and after battling depression and dealing with never truly wanting to be who I was, I finally learned that I had to choose me. I had to make an effort to maintain my sanity above all else. You can be super giving, but you have to know when you\\031re on 5% and don\\031t have any battery to share. The hook is the most simple of the entire album outside of Silent Hill, yet for the purpose of a hook being something you\\031d want to repeat, \\034**I choose me I\\031m sorry**\\035 is the highest on the list for me. I found myself grooving in traffic multiple times like I was at a jazz concert with the green incense burning. With some of the final bars of the album, Kendrick doesn\\031t miss saying \\034**I can\\031t live in the matrix/rather fall short of your graces/this time I won\\031t trade places/not about who\\031s right who\\031s wrong/evolve the only known/ask me when I\\031m coming home/blink twice and then I\\031m gone**\\035. This hits hard for me because to me Kendrick has always did music from someone else\\031s perspective. He\\031s aware of it, and saying this time I can\\031t do that. I choose me. \\034**You won\\031t grow old waiting on me**\\035 is another direct response to people demanding his time and presence. Not one to miss the moment, he even has choice words for hip hop herself when he says \\034**She told me she need me the most, I didn't believe her/she even called me names on the post, the world can see it/jokes and gaslightin'/mad at me 'cause she didn't get my vote, she say I'm trifflin\\031/disregardin' the way that I cope with my own vices/maybe, it's time to break it off/runaway from the culture to follow my heart**.\\035 It\\031s especially profound that someone would come off a hiatus, to put people back on notice that you just might not be as into it as you were. He\\031s not saying flat out he\\031s gonna stop, and most likely he has a couple more albums in him before he stops, but it\\031s one of the most interesting things said on on the album. It makes you wonder was Kendrick on a break because of x, y or z, or was he testing the waters with stopping all together?\\n\\nSo one of my original points was that I related to this album more than any other Kendrick album. I touched on why randomly here and there, but the main point is that before, all of his albums felt like they were about something or for specific people. You never truly got a view into Kendrick\\031s inner workings outside of a bar here or there, or songs like \\034u\\035, so he always had this mystique. If I met Kendrick then, I\\031d probably have referred to him as k. dot or something informal. That\\031s not to say I didn\\031t like his albums, quite the contrary. TPAB is perfection. But, for me, when I hear an artist being vulnerable and baring all, that\\031s what I relate to the most. Not Jay Z \\034Dirt Off Your Shoulders\\035, but \\034Song Cry\\035 or \\034Regrets\\035. I don\\031t have to agree with your every take. For me it\\031s just like when you meet somebody. I\\031m not that invested. But once you tell me you\\031re into Star Wars we can have a whole conversation before I realize it\\031s been a couple hours. So to me, that makes this the number one album in his discography. If I saw him at the airport, I\\031ll probably say yoooo Kendrick you killed that album! Lol because it feels like he let me into his Tuesday and Thursday therapy session. Which means Kendrick took the pressure, and still found a way to add more layers without disappointing. Which is honestly all we can ask after a long hiatus right? Growth\\n\\nQuestions\\n\\n1. **Where is this on Kendrick\\031s discography for you?**\\n2. **How much of the album do you still play at this point? All or select few? And why?**\\n3. **Regardless of the quality of music, what did you want from Kendrick\\031s return? Were you satisfied?**\\n4. **Do you agree with people like Kendrick and Chappelle for trying to bridge the gap or do you think we should all just never mention anything that doesn\\031t directly concern us?**\\n5. **Who\\031s the best rapper in the game right now?**\\n6. **(Bonus) What are your 5 favorite albums from Kendrick\\031s freshmen class (Kendrick, Meek, Mac Miller, Big KRIT, Yelawolf, Cyhi, Lil Twist, Lil B, Diggy Simmons, Fred The Godson, and YG were all in this photo but for the purposes of this question add in J Cole, Big Sean, Wale, and Drake)**\n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\nWill it be Collapse by a thousand cuts\\024or one big gash?\\n\\nThis is *Last Week in Collapse*, a weekly self-post, bringing together some of the most important, timely, ironic, useful, demoralizing, stunning, or otherwise must-see moments in Collapse. \\n\\nThis is the 42nd edition. You can find the October 1-7 edition [here](https://old.reddit.com/r/collapse/comments/xyrca2/last_week_in_collapse_october_17_2022/) if you didn\\031t catch it last week. If you don\\031t want to miss an edition, you can [sign up for the SubStack](https://substack.com/profile/18092228-last-week-in-collapse), and upvote this so more people see it. \\n\\nThe soldier who took power in Burkina Faso\\031s latest *coup d\\031\\xe9tat* was allegedly going to step down and [be replaced](https://www.france24.com/en/africa/20221008-burkina-faso-to-pick-a-transitional-president-ahead-of-elections) by a transitional president in advance of a 2023 election\\024but then the Council [chose him for the job](https://globeecho.com/news/africa/burkina-faso-captain-ibrahim-traore-appointed-transitional-president-until-the-2024-election/), and pushed \\034elections\\035 to 2024. Each successive power struggle divides the military and the common people, eroding trust and legitimacy that is increasingly exploited by jihadist groups wresting power from the government.\\n\\nHaiti has become so unstable that [it is **asking for foreign military aid**](https://www.bbc.com/news/world-latin-america-63181481) to establish humanitarian corridors and intimidate gangsters into receding into the background. The last UN Peacekeepers left 5 years ago.\\n\\nThousands of [Pakistanis are protesting](https://www.aljazeera.com/news/2022/10/11/thousands-protest-rising-violence-in-pakistans-swat-valley) in the Swat Valley over rising violence in the region.\\n\\nThe Tigray War in Ethiopia, and its associated supply blockade, is [leading to the deaths](https://www.bbc.com/news/world-africa-63166044) of many medical patients who are not receiving medicine and hospital supplies. Not to mention the ~500,000 already estimated [dead from violence &amp; famine](https://en.wikipedia.org/wiki/Casualties_of_the_Tigray_War) (so far) in the 23-month old Civil War.\\n\\nAmerican officials are [trying to reduce **fears of nuclear \\034armageddon\\035**](https://www.theguardian.com/world/2022/oct/09/biden-armageddon-russia-nuclear-threat-pentagon) as a result of the Ukraine War. It must be a coincidence that [Putin appointed a new commander](https://www.bbc.com/news/world-europe-63217467) nicknamed \\034General Armageddon.\\035 [Power was restored](https://www.reuters.com/world/europe/power-supply-restored-zaporizhzhia-nuclear-plant-energoatom-2022-10-09) to the nuclear power plant (Europe\\031s largest) at Zaporizhzhia, so a meltdown seems unlikely for the time being. The plant\\031s emergency diesel generators (which are not currently active) have enough fuel now for 8 days.\\n\\nFrance is [running out of petrol](https://news.sky.com/story/third-of-petrol-stations-out-of-fuel-in-france-as-strikes-continue-12717708), too. **Fuel refinery worker strikes have entered their third week**, and lines have grown long across the nation. France\\031s refined **oil supply has dropped by more than 60%**, and [more than 25%](https://www.thesun.co.uk/travel/20071679/france-holiday-warning-petrol-shortage/) of gas stations have closed. Some motorists are being turned away because they are judged to have enough petrol in their tanks already, and drivers with \\034priority occupations\\035 are given preference. Is this approach a model for how governments manage our future famine(s)? [This weekly observation](https://old.reddit.com/r/collapse/comments/y0arpk/weekly_observations_what_signs_of_collapse_do_you/is2wqoy/) explains a little more what\\031s going on in the Paris area.\\n\\nThe [perpetually hungry](https://www.aspistrategist.org.au/north-korea-could-be-headed-back-towards-famine/) nation of North Korea, after testing several missiles two weeks ago, says [they were simulations for a nuclear attack on South Korea](https://www.bbc.com/news/world-asia-63196618), ahead of an **expected nuclear test** coming within a few weeks.\\n\\nRussia [struck Kyiv with a flurry of missiles](https://www.theguardian.com/world/2022/oct/10/explosions-kyiv-ukraine-war-russia-crimea-putin-bridge) on Monday morning, hitting Zelenskyy\\031s office (he was not there), as well [as a German consulate](https://euroweeklynews.com/2022/10/10/breaking-german-embassy-in-kyiv-hit-by-russian-strikes/) building. The strike was reportedly retaliation for the Ukrainian ~~(?)~~ sabotage of the Crimean/Kerch bridge a few days earlier. President Putin [claims there will be no more \\034massive strikes\\035 in Ukraine](https://www.bbc.com/news/world-europe-63255617) for the time being, and claimed that he will have mobilized 300,000 soldiers by the month\\031s end; 220,000 have allegedly been mobilized already.\\n\\n**Belarus** [**is positioning its troops** near the Ukraine border](https://www.telegraph.co.uk/world-news/2022/10/10/fears-new-invasion-putin-lukashenko-form-joint-task-force-ukraine/), a signal that they are likely to ~~join~~ be dragged into this conflict by Russia, deployed against Ukraine\\031s northwest oblasts. [Moldova](https://www.aljazeera.com/news/2022/10/10/moldova-says-russian-missiles-that-hit-kyiv-crossed-its-airspace) may be drawn into this War in the not-too-distant future, too.\\n\\nSome people are talking about the potential for ~~worldwide annihilation~~ [nuclear war to counterbalance the effects of global warming](https://www.newsweek.com/fact-check-nuclear-war-climate-change-global-warming-1750274). Methinks maybe they misunderstood what made the Cold War cold.\\n\\nDr. Fauci [is warning](https://www.cnbc.com/2022/10/07/dr-fauci-new-more-dangerous-covid-variant-could-emerge-this-winter.html) that **an even more dangerous COVID variant could emerge this winter**. &gt;!Are people still reporting their positive cases to their governments, or have we all moved past that? I\\024and [many others](https://old.reddit.com/r/collapse/comments/y2j1pa/the_data_is_clear_long_covid_is_devastating/is3ws1h/)\\024can\\031t tell who has Long COVID these days, and who just has late-stage [*Weltschmerz*](https://en.wikipedia.org/wiki/Weltschmerz). No doubt the two afflictions are often related.!&lt;\\n\\nIndia is at the forefront [of the growing **superbug crisis**](https://www.bbc.com/news/world-asia-india-63059585), wherein various bacteria have grown resistant to many conventional antibiotics. The antibiotic-resistance problem already leads to the deaths of over 60,000 newborn babies every year worldwide. Seems like we already slipped into dystopia.\\n\\nWildlife populations [are in freefall](https://archive.ph/l77fK), particularly in South America, where they have reportedly lost 94% of wildlife population in the last 50 years\\024among 32,000 monitored species. Worldwide, a **loss of 69% of biodiversity** has been ~~lost~~ sacrificed, based on the 60-page [WWF 2022 Living Planet Report](https://wwflpr.awsassets.panda.org/downloads/lpr_2022_full_report.pdf). Freshwater populations are down 83% worldwide.\\n\\nTo highlight one example, penguins in Antarctica [are dying off](https://www.theguardian.com/world/2022/oct/12/australian-scientists-observe-rapid-decline-in-adelie-penguin-numbers-off-antarctic-coast); their population has dropped 43% in the last decade. Researchers blame overfishing (which depletes their food supply) and climate change.\\n\\nBrazil [broke a September record](https://www.rte.ie/news/world/2022/1007/1327812-brazil-deforestation/) for how much of the Amazon rainforest they deforested. 1,455 km\\xb2 (562 mi\\xb2) of the Amazon was lost last month; equivalent to slightly larger than the size of the Greek island of [Rhodes](https://en.wikipedia.org/wiki/Rhodes). \\n\\nFlooding in Venezuela [killed 39+](https://www.rte.ie/news/world/2022/1011/1328427-venezuela-landslide/) with dozens more missing. Rising floodwaters [are swallowing homes\\024and humans\\024in Nigeria](https://edition.cnn.com/2022/10/10/africa/casualties-latest-nigeria-flood-intl/index.html). Flooding in Nepal [killed at least 33](https://www.bbc.com/news/world-asia-63224454) last week.\\n\\nCanada [is talking about relocating people](https://www.cbc.ca/news/politics/fiona-climate-change-relocation-maritimes-1.6614604?cmp=rss) away from regions likely to be hit by natural disasters. Canada\\031s [disaster budget is already overspent](https://www.cbc.ca/news/politics/disaster-adaptation-fund-money-1.6613203), and the government is reportedly releasing a revised plan later this year.\\n\\nAmerican rivers [are drying up](https://www.wbrz.com/news/drought-conditions-drop-mississippi-river-waters-exposing-19th-century-shipwreck/), revealing old sunken ships\\024and new riverfront real estate. The mighty Mississippi River has gotten so low that [shipping barges, ferries, and recreational boats](https://eu.usatoday.com/story/news/nation/2022/10/09/mississippi-river-low-water-halt-key-commerce-travel/8229047001/) cannot safely traverse parts of the river. All the world\\031s [crises are overlapping](https://www.smh.com.au/national/fires-floods-pandemic-the-age-of-overlapping-crises-20221011-p5bort.html) now, and sifting out cause-and-effect is an impossible task.\\n\\nEswatini [broke an October heat record](https://twitter.com/extremetemps/status/1579383733387304960) last week, at 46 \\xb0C (115 \\xb0F). T\\xfcrkiye and Iran [also broke October records](https://twitter.com/extremetemps/status/1578827597961900033) for heat. The **Atlantic Ocean surface temperature** [**is 0.2 \\xb0C warmer**](https://twitter.com/LeonSimons8/status/1579923715868934145) than it was in 2021&\\n\\nAll of Scotland [is now snow-free](https://www.bbc.com/news/uk-scotland-highlands-islands-63184780), an occasion thought to have happened just 9 times in the last 300 years. 7 of those snow-free times occurred within the last 30 years.\\n\\nRwanda [is trying to reforest land](https://allafrica.com/stories/202210100411.html) with drought-resistant trees, in preparation for a hotter, drier future. Forest cover dropped about 2/3rds in the last 80 years, something the Forestry Authority blames on \\034different anthropogenic activities and resettlement of refugees.\\035\\n\\n[Hungary is recruiting](https://archive.ph/bBCTe) a few thousand of its citizens to become **migrant \\034hunters,\\035** mostly on the Hungary-Serbia border, but also patrolling the fenced border to Croatia (which is in the EU but not yet the Schengen Area). Latvia has also [been accused](https://www.bbc.com/news/world-63237670) of mistreating migrants weaponized by neighboring Belarus.\\n\\nThe global economy continues [its slow plunge](https://archive.ph/SGybg). U.S. inflation \\024 the CPI, which is &gt;!a price index for \\034food and beverages, housing, apparel, transportation, medical care, recreation, education and communication, and other goods and services\\035!&lt; \\024 reached its [highest level in 40 years](https://www.straitstimes.com/business/economy/us-inflation-rises-to-40-year-high-paving-way-for-another-big-fed-hike). Growth possibilities are limited because [the global oil supply is strained](https://finance.yahoo.com/news/world-worried-saudi-aramco-world-160000039.html) and **energy prices have made operating many businesses unprofitable**. \\n\\nUnsurprisingly, [LNG tanker rates are are all-time highs](https://oilprice.com/Latest-Energy-News/World-News/LNG-Freight-Rates-Hit-Record-High-As-Europe-Races-To-Secure-Gas.html). The price to hire one of these massive, bulbous ships has increased more than 5x this year, and LNG demand is up 65% this year.\\n\\n**Food prices continue to soar** across the world; in [Tunisia](https://abcnews.go.com/International/wireStory/empty-shelves-unaffordable-food-tunisias-crisis-deepens-91320714), tremors of violence lurk behind economic unrest. [In India](https://www.channelnewsasia.com/asia/india-rising-inflation-hit-poor-fuel-food-prices-3001896), food &amp; fuel prices are pushing people back into poverty; [in Europe](https://euobserver.com/health-and-society/156260), pension funds are betting on food indices; [in Saudi Arabia](https://www.zawya.com/en/economy/gcc/saudi-inflation-up-31-in-september-as-food-prices-soar-xu2ix3bs); food price increases are slowly worsening inflation; [in Argentina](https://www.euronews.com/2022/10/14/argentina-inflation-poverty), people are scavenging landfills for things to trade for money/food& [In Australia](https://www.cnbc.com/2022/10/05/australia-inflation-rising-food-prices-are-hurting-restaurants-diners.html), produce costs have skyrocketed; [in Sudan and Somalia](https://finance.yahoo.com/news/global-survey-reveals-food-prices-202200209.html), food scarcity is triggering a slow famine. [Economic projections for next year](https://old.reddit.com/r/collapse/comments/y3fyo8/bofa_warns_that_the_us_economy_will_start_to_lose/) aren\\031t exactly sanguine, either&\\n\\nIranian police [have been machine-gunning fleeing protestors](https://www.bbc.com/news/world-middle-east-63253724) at some of the protests, which have spread to more than 100 cities and could possibly result in the collapse of the regime. It is said that nations collapse slowly\\024and then all at once; this is the opposite pattern for the collapse of an insurgency movement. Whether these [repressive measures](https://www.bbc.com/news/world-middle-east-63218963) are just another milestone in the slow-motion Collapse of Iran remains to be seen.\\n\\n&gt;\"We used to be afraid of the {Iranian} regime, but now **the wall of fear has collapsed**. Nobody is frightened anymore&the regime won't collapse. It can't be changed. They are strong and they keep killing people. **We will never stop, and so they will keep killing us**&It's crazy, and it's corrupt. Nobody cares about us. The outside world says it supports Iran, but nobody does. We are being tortured and killed every day.\" -quotes from a questionable [BBC article](https://www.bbc.com/news/world-middle-east-63218963) \\n\\nMexico [is keeping its soldiers on the streets](https://www.bbc.com/news/world-latin-america-63241024) for the next 6 years, the government overwhelmingly decided. The military is a large part of Mexico\\031s failing counterinsurgency practices to combat drug trafficking and the spread of gang violence which threatens the legitimacy/sovereignty of the state.\\n\\n[Tensions have been rising in Israel/Palestine](https://apnews.com/article/middle-east-jerusalem-israel-arrests-d518e1186fd89a7c4325ef8d5d696cf5) in the days after [a mystery shooter killed an IDF soldier](https://www.timesofisrael.com/palestinians-in-east-jerusalem-nablus-hold-strike-amid-manhunt-for-shuafat-shooter/) outside a refugee camp. The camp was put into lockdown so security forces could track the killer, but Palestinians began a general strike in East Jerusalem a couple days in. Another IDF soldier was killed days after the first, escalating violence in [an already violent year](https://www.middleeastmonitor.com/20221010-un-100-palestinians-killed-in-2022/). These **cycles of fear and vengeance** may prove impossible to escape.\\n\\nThe son of the 36-year-long (\\034freedom fighter\\035) President of Uganda bizarrely [claimed](https://www.capitalfm.co.ke/news/2022/10/musevenis-son-muhoozi-asks-president-ruto-to-forgive-him-following-kenya-invasion-tweets/) that the Ugandan military would invade and conquer Nairobi in two weeks. The boast was met with widespread ridicule and condemnation by the Ugandan government. Who will take over when the 78-year-old Museveni dies? \\n\\nThe [**Ebola outbreak in Uganda**](https://archive.ph/4JNmJ) is the country\\031s worst in 20+ years; 39 are confirmed dead from it, and at least 54 infected. [**Cholera has reappeared in Syria**](https://reliefweb.int/report/syrian-arab-republic/cholera-spreads-across-syria-putting-vulnerable-people-serious-risk) for the first time since 2007, linked to contaminated Euphrates River water. Worldwide, [monkeypox numbers continue to decline](https://www.nature.com/articles/d41586-022-03204-7). The [risk of bird flu crossing over](https://www.theguardian.com/environment/2022/oct/06/bird-flu-an-urgent-warning-to-move-away-from-factory-farming) continues to hang over humanity&What would we do without our chicken?\\n\\n*Things to watch for next week include:*\\n\\n\\xa0 China\\031s President, Xi Jinping, is widely [expected to secure a third 5-year term](https://www.hrw.org/news/2022/10/10/china-third-term-xi-threatens-rights) at the CCP Congress next week. Nobody doubts that he will accomplish this; the question is rather: how will China\\031s behavior change once this is achieved? China is still [maintaining its zero-COVID policy](https://www.aljazeera.com/economy/2022/10/10/xis-bid-to-extend-rule-dampens-hopes-for-zero-covid-exit), struggling with [a real estate/economic collapse](https://archive.ph/mCwb3), and alienating itself from other countries (even its old friend Russia).\\n\\nTo make matters more interesting, a daring, mystery protestor in Beijing [hung a large banner opposing Xi Jinping and China\\031s zero-COVID policy](https://www.bbc.com/news/world-asia-china-63252559), and supposedly shared a manifesto online, calling for acts of civil disobedience. In an age of totalitarianism, can a single, dramatic act of defiance be considered an act of War?\\n\\n*Select comments/threads from the subreddit last week suggest:*\\n\\n-Looks like wood\\031s back on the menu, according to [this post](https://old.reddit.com/r/collapse/comments/xz1zyi/firewood_demand_is_surging_as_europeans_return_to/) about the skyrocketing demand for firewood across Europe. One of my colleagues is from Germany, and he told me that his father is storing a tonne (1,000 kg, or 2,205 lbs) of coal in their basement, and another neighbor has already stockpiled a similar amount of firewood. Tell me again about our ~~soylent~~ green future?\\n\\n-Legendary r/collapse poster u/Myth_of_Progress linked [a small observation](https://old.reddit.com/r/collapse/comments/y0arpk/weekly_observations_what_signs_of_collapse_do_you/is5odpk/) citing the bonkers climate situation in Canada\\031s southwest. Extreme heat, extreme cold, lengthy rain, and lengthy drought. The New Normal\" is here.\\n\\n-There\\031s a reason why so many reforestation programs are failing (many reasons, actually), and they are explored [in this thread](https://old.reddit.com/r/collapse/comments/xzlbju/phantom_forests_why_ambitious_tree_planting/) and its comments. \\n\\n-Healthcare systems around the world are overwhelmed by respiratory illnesses, [based on the link and comments to this thread](https://old.reddit.com/r/collapse/comments/y0xkcb/the_healthcare_system_is_under_stress_from/). The WHO [is still warning about **Long COVID**](https://www.theguardian.com/society/2022/oct/12/long-covid-who-tedros-adhanom-ghebreyesus), which they claim **1 in 3 women get; 1 in 5 men get Long COVID**, too, reportedly. This is not only a mass disabling event/process, but a mass psychologically destabilizing phenomenon, too. Can you feel your sense of reality under siege? Can you hold the line?\\n\\nHave any feedback, questions, comments, articles, news, death threats, home-heating advice, doomy prognostications, etc.? If you can\\031t remember to check r/collapse every Saturday, you can join the [***Last Week in Collapse* SubStack**](https://substack.com/profile/18092228-last-week-in-collapse) and get this roundup sent to your email inbox every weekend. I always forget something; what did I miss this week this time?\n6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\n\\nMy brother married this bridezilla last year.\\n\\nThis is a bit long but please bear with me. I will need to start the story with the events that happened 2 years ago.\\n\\nThe wedding should've been 2 years earlier and would have been held in our country but the pandemic happened so they had to reschedule it.\\n\\nTo make this easier let's call the bridezilla, \"Anna\".\\n\\nAnna was nice to us in the beginning. I only met her once and have been on video calls with her since I am always busy with my job.\\n\\nWhen she got comfortable with us, she started to ask us to do some errands for her (to be able to 'help' her and my brother).\\n\\nThe first thing she did was ask my sister to look for wedding decorations and produce wedding invitations and souvenirs according to her desired 'plans'. I am not that aware of what she wants but my sister told me that the materials she wants are very expensive. My sis gave her an estimate and she told her to look for 'cheaper' alternatives. This will go on for weeks and she\\031s not even giving my sister some allowance for petrol. She also wants everything done as soon as possible (like we have no other stuff to do but serve her).\\n\\nMy sister informed me that Anna is kind of a slavedriver. After some time, she asked me to make blueprints for her \\030dream house\\031. I am a busy person that works for a minimum of 12 hrs a day (because of deadlines). I told her this and politely refused the offer to build plans for her. I recommended a trusted friend that can help her with it and was willing to give a discount on the plans. He quoted about $2000 for the plans which are cheaper by $200 (we are from a 3rd world country so it might be too cheap for others) compared to others. She said that it was too much and tried to look for someone else that can do it cheaper.\\n\\nAfter that, she rarely communicates with me. After a few months, I heard from my sister that Anna and her architect friend (who 'made' the plans) is talking shit behind my back. Saying that I am \"worthless\" and \"wasted\" my degree in engineering just because I wasn't able to help her with the plans. She probably thought I can give her plans for free too since she's my brother's fianc\\xe9. \\n\\nThey ended up asking my cousin to work on the plans but my cousin discovered that the architectural plans Anna's friend provided were a copy of someone else's work (they literally just changed the title block) so my cousin ended up giving up on that job too.\\n\\nAfter a few months, the wedding was rescheduled and since my brother is busy with his work, he will only be able to have 2 weeks off for the wedding. Anna got livid! She forced my brother to go on an extended leave without the employer's approval (meaning he could lose his job). Since she wants to be able to go visit other countries for their honeymoon.\\n\\nThey decided to get married in South Africa. Since Anna isn't earning much, my brother probably ended up paying for everything. But since they recently bought properties, his money is not enough to bring the family with them. Instead, they will only be able to bring the parents and Anna's child. \\n\\nAnna wasn't able to provide the necessary documents to be able to get her child's passport though so only my mom and her mom were able to attend the wedding.\\n\\nMy brother provided my mom $2000 allowance so she can buy the stuff she wanted when they arrive. My mom was with them for 2-3 months (mostly at Anna's place). \\n\\nAfter a few weeks, I found out that my mom is going home by herself and that they took her allowance so Anna's sister and her child can go visit other countries too. \\n\\nI am okay with my mom going early though since we missed her and she did tell me once that she wants to go home early but she didn't spend much of the money since she was saving it for a few renovations in the house. \\n\\nEverything is fine until I found out from my sister that Anna was not feeding my mom properly during their stay at her place (my brother had to leave sometimes since he need to work to be able to recover from the wedding expenses). So while my mom was staying with Anna and her own mother, she would never ask my mom to eat. My mom is very shy and wouldn't get food on her own unless offered. This is a common thing in our country (Anna is aware of this). So then, my mom will only be able to eat when Anna\\031s mother offers her food. \\n\\nMy mom wasn't able to buy her own food too since they took away her allowance as mentioned earlier. Also, Anna has a vibe that she doesn't like people going through her stuff so my mother doesn't look for food in the fridge or pantry.\\n\\nWhen they eat out, Anna would ask her mom what she wants and buys it for her. As for my mom, she would pick the cheapest food they have and would not even ask my mother's preference (my mom is not that adventurous when it comes to food and doesn\\031t eat something that\\031s spicy). It's a good thing that we have relatives in that country so there are times that my mother would visit them so she can eat properly.\\n\\nThe bitch also doesn\\031t eat the same food twice a day. If there\\031s leftover food (even if it\\031s still good/edible) she would throw it in the trash. Not to mention, she always goes fine dining even if she doesn\\031t earn that much.  \\nShe has also a fear of missing out. Our family loves dogs so she got one too. Poor baby was kept inside a small cage for almost 24hrs a day. She lives alone with the dog so when she\\031s at work and the dog is inside the cage, the dog got so stressed that it started to eat its own sh\\\\*t. Looking at her photos with the dog inside the cage in the background made me think she doesn\\031t care about the poor thing at all. She then gave the dog away (which is a good thing since I heard the dog is getting along well with the new fur parents). It\\031s sad that the dog had to go through a lot of stress though.\\n\\nMy brother and Anna have been married for a year now. I am surprised that he was able to survive. Anyways, I think he\\031s at fault too since he spoiled her so much. He can\\031t say \\030no\\031 to her demands. On top of the real estate properties they purchased, he even got her a ride. He told us that Anna \\030paid for it\\031 but I doubt that she can afford that on her own (considering the lavish lifestyle she currently has that doesn\\031t match her income).  \\nMy family treat her right every time she comes over to stay at our house for days. Mom cooks her whatever she likes. I really don\\031t get why she had to treat my mom and sister like that.\n        subreddit score upvotes downvotes up_ratio total_awards_received golds\n1 entitledparents   528     528         0     0.99                     1     0\n2            BBBY   527     527         0     0.97                     0     0\n3       JUSTNOMIL   520     520         0     0.97                     0     0\n4     hiphopheads   515     515         0     0.84                     0     0\n5        collapse   515     515         0     0.98                    10     0\n6     bridezillas   514     514         0     0.96                     0     0\n  cross_posts comments\n1           0       42\n2           0       48\n3           0       53\n4           1      293\n5           0       36\n6           0       48\n\nprocessed &lt;- textProcessor(pandemic_threads$text, metadata = pandemic_threads)\n\nBuilding corpus... \nConverting to Lower Case... \nRemoving punctuation... \nRemoving stopwords... \nRemoving numbers... \nStemming... \nCreating Output... \n\n\nThe stm package also requires us to store the documents, meta data, and “vocab” in separate objects, essentially a list of words described in the documents.\n\n# Eliminates both extremely common terms and extremely rare terms, since such terms make word-topic assignment more difficult.\nout &lt;- prepDocuments(processed$documents, processed$vocab, processed$meta)\n\nRemoving 6469 of 11816 terms (6469 of 71023 tokens) due to frequency \nYour corpus now has 218 documents, 5347 terms and 64554 tokens.\n\ndocs &lt;- out$documents\nvocab &lt;- out$vocab\nmeta &lt;-out$meta\n\nThen have to make another decision about the number of topics we might expect to find in the corpus. Let’s start out with 10. We also need to specify how we want to use the meta data. This model uses the number of “comments”. It’s important to recognize that the variables selected in this stage can have significant ramifications. If we make the wrong choice, we could potentially miss identifying certain topics that are discussed on both liberal and conservative blogs or mistakenly categorize them as distinct subjects.\nIn addition, the stm package offers an argument that permits the specification of the desired type of initialization or randomization. For our purposes, we have chosen to use spectral initialization. Please see Bail, C. 2020 for more.\nThis below code may take some time if you are running it on a large body. You can read more about each function in the package documentation.\n\nFirst_STM &lt;- stm(documents = out$documents, vocab = out$vocab,\n              K = 10, \n              prevalence =~ comments,\n              max.em.its = 75, data = out$meta,\n              init.type = \"Spectral\", verbose = FALSE)\n\nWe start by inspecting our results by browsing the top words associated with each topic. The stm package has a useful function that visualizes these results called plot:\n\nplot(First_STM)\n\n\n\n\nThe visualization provides information on both the occurrence rate of the topic across the entire corpus and the top three words that are linked to the topic. As you will notice, in a second iteration of the model we may want to exclude words such as “like”.\nSome topics seem plausible, but many that do not seem very coherent or meaningful. You may want to improve your topic classification with more than one variable in the prevalence comments. Please see Bail, C. 2020 for more.\n\n\n7.3.4 Limitations of Topic Models\nFor various reasons, topic models have become a conventional tool for quantitative text analysis. Depending on the application, they can be more advantageous than simplistic word frequency or dictionary-based methods. Generally, topic models yield optimal outcomes when utilized on texts that are moderately lengthy and have a regular format.\nOn the other hand, topic models have a number of important limitations. To start, the term “topic” is somewhat vague, and it is now evident that topic models cannot generate extremely refined classifications of texts. Furthermore, if topic models are incorrectly perceived as an unbiased depiction of a text’s meaning, they can be easily misused. Once more, these instruments might be more accurately depicted as “tools for reading.” It is not advisable to excessively interpret the outcomes of topic models unless the researcher has solid theoretical prior knowledge regarding the number of topics in a particular corpus, or if the researcher has thoroughly verified the results of a topic model using both quantitative and qualitative methodologies described earlier."
  },
  {
    "objectID": "topic-modelling.html#questions",
    "href": "topic-modelling.html#questions",
    "title": "7  Topic Modelling",
    "section": "7.4 Questions",
    "text": "7.4 Questions\nFor the second assignment, we will focus on the United Kingdom as our geographical area of analysis. As for Sentiment Analysis chapter, we will use a dataset of tweets about migration posted by users in the United Kingdom during February 24th 2021 to July 1st 2022.\n\ntwitter_df &lt;- readRDS(\"./data/sentiment-analysis/uk_tweets_24022021_01072022.rds\")\n\n\nPrepare the Twitter data so that it can be analyzed in the topicmodels package\nRun three models and try to identify an appropriate value for k (the number of topics).\nCreate a chart of greatest differences between two relevant topics you have identified.\nUse the full_place_name or lat and long variables as meta data to create classification between different types of places in the UK. For example: urban/rural, classified by population/density, or simply between different regions. There are plenty of ways you can divide the tweets geographically, see some easy examples here. Then explore whether there are differences in topics according to different locations in the UK using the stm package.\nBONUS QUESTION: Discuss the limitations of topic models on short texts, such as Tweets. There have been a number of recent attempts to address this problem, and Graham Tierney has developed a very nice solution called stLDA-C.\n\nAnalyse and discuss: a) whether there are different topics related to migration that emerge, and what these are. b) how migration topics vary spatially.\nThe below code helps you geo-localise the Twitter data. You could then perform a spatial join to another sf objects that has population density or classifies areas in the UK.\n\nlibrary(sf)\n\n# subset the data frame to remove rows with missing values in x and y columns\ntwitter_df_clean &lt;- twitter_df[complete.cases(twitter_df[, c(\"lat\", \"long\")]), ]\n\ntwitter_df_clean &lt;- twitter_df_clean %&gt;% \n  sf::st_as_sf(coords = c(4,5))  %&gt;%  # create pts from coordinates\n  st_set_crs(4326)  # set the original CRS\n\nplot(twitter_df_clean$geometry)\n\n\n\n# Example spatial join. You can also specify other join types such as st_contains, st_within, and st_touches depending on your analysis requirements.\n# cities &lt;- st_join(point, cities, join = st_intersects)\n\n\n\n\n\nBar, Michael, Moshe Hazan, Oksana Leukhina, David Weiss, and Hosny Zoabi. 2018. “Why Did Rich Families Increase Their Fertility? Inequality and Marketization of Child Care.” Journal of Economic Growth 23: 427–63.\n\n\nBlei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3 (Jan): 993–1022.\n\n\nMarshall, Emily A. 2013. “Defining Population Problems: Using Topic Models for Cross-National Comparison of Disciplinary Development.” Poetics 41 (6): 701–24."
  },
  {
    "objectID": "longitudinal-1.html#dependencies",
    "href": "longitudinal-1.html#dependencies",
    "title": "8  Modelling Time",
    "section": "8.1 Dependencies",
    "text": "8.1 Dependencies\n\n# Data\nlibrary(sf)\nlibrary(readr)\nlibrary(tidyverse)\n\n# Dates and Times\nlibrary(lubridate)\n\n# Regression Spline Functions and Classes\nlibrary(splines)\n\n# graphs\nlibrary(ggplot2)\nlibrary(dygraphs)\nlibrary(plotly)\nlibrary(ggthemes)\nlibrary(ggpmisc)\nlibrary(gridExtra)\nlibrary(viridis)\nlibrary(ggformula)\nlibrary(ggimage)\nlibrary(grid)"
  },
  {
    "objectID": "longitudinal-1.html#data",
    "href": "longitudinal-1.html#data",
    "title": "8  Modelling Time",
    "section": "8.2 Data",
    "text": "8.2 Data\nWe will use a sample of Google Mobility data. Google has made available Community Mobility data to provide insights into what changed in response to policies aimed at combating COVID-19. The data also has accompanying reports which analyse movement trends over time by geography, across different categories of places such as retail and recreation, groceries and pharmacies, parks, transit stations, workplaces, and residential. Data is available from 2020 and 2022. Community Mobility data is no longer being updated as of 2022-10-15. All historical data will remain publicly available.\nLocation accuracy and the understanding of categorised places varies from region to region, so it is not recommend the data is used to compare changes between countries, or between regions with different characteristics (e.g. rural versus urban areas). Region that do not have statistically significant levels of data have been left out of the report.\nSome important facts about the data:\n\nChanges for each day are compared to a baseline value for that day of the week. The baseline is the median value.\nThe data that is included in the calculation depends on user settings, connectivity and whether it meets our privacy threshold.\nIf the privacy threshold isn’t met (when somewhere isn’t busy enough to ensure anonymity) a change for the day is not shown.\n\nFor more about this data please see here.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Time</span>"
    ]
  },
  {
    "objectID": "longitudinal-1.html#modelling-time",
    "href": "longitudinal-1.html#modelling-time",
    "title": "8  Modelling Time",
    "section": "8.3 Modelling Time",
    "text": "8.3 Modelling Time\nFirst we import the data we will be working with. We will be looking at Google Mobility data for Italy.\n\nmobility_it &lt;- read.csv(\"data/longitudinal-1/2022_IT_Region_Mobility_Report.csv\", header = TRUE)\n\nCheck out variables in the dataframe\n\n# Check out variables in the dataframe \ncolnames(mobility_it)\n\n [1] \"country_region_code\"                               \n [2] \"country_region\"                                    \n [3] \"sub_region_1\"                                      \n [4] \"sub_region_2\"                                      \n [5] \"metro_area\"                                        \n [6] \"iso_3166_2_code\"                                   \n [7] \"census_fips_code\"                                  \n [8] \"place_id\"                                          \n [9] \"date\"                                              \n[10] \"retail_and_recreation_percent_change_from_baseline\"\n[11] \"grocery_and_pharmacy_percent_change_from_baseline\" \n[12] \"parks_percent_change_from_baseline\"                \n[13] \"transit_stations_percent_change_from_baseline\"     \n[14] \"workplaces_percent_change_from_baseline\"           \n[15] \"residential_percent_change_from_baseline\"          \n\n\nLong data vs. Wide data\nLong data and wide data are two different formats used to store and organize data in a tabular form. Wide data is a format where each variable is stored in a separate column, and each observation or time point is stored in a separate row. This format is often useful when the number of variables is small compared to the number of observations, and is often used for descriptive statistics and exploratory data analysis.\n\n\n        dates           A          B\n1  2022-01-01 -0.56047565  1.2240818\n2  2022-01-02 -0.23017749  0.3598138\n3  2022-01-03  1.55870831  0.4007715\n4  2022-01-04  0.07050839  0.1106827\n5  2022-01-05  0.12928774 -0.5558411\n6  2022-01-06  1.71506499  1.7869131\n7  2022-01-07  0.46091621  0.4978505\n8  2022-01-08 -1.26506123 -1.9666172\n9  2022-01-09 -0.68685285  0.7013559\n10 2022-01-10 -0.44566197 -0.4727914\n\n\nLong data, on the other hand, is a format where each variable is represented by two or more columns: one column for the variable name and another for the variable values. This format is often useful when we have many variables or when we want to perform statistical analysis.\n\n\n        dates series       value\n1  2022-01-01      A -0.56047565\n2  2022-01-01      B  1.22408180\n3  2022-01-02      A -0.23017749\n4  2022-01-02      B  0.35981383\n5  2022-01-03      A  1.55870831\n6  2022-01-03      B  0.40077145\n7  2022-01-04      A  0.07050839\n8  2022-01-04      B  0.11068272\n9  2022-01-05      A  0.12928774\n10 2022-01-05      B -0.55584113\n11 2022-01-06      A  1.71506499\n12 2022-01-06      B  1.78691314\n13 2022-01-07      A  0.46091621\n14 2022-01-07      B  0.49785048\n15 2022-01-08      A -1.26506123\n16 2022-01-08      B -1.96661716\n17 2022-01-09      A -0.68685285\n18 2022-01-09      B  0.70135590\n19 2022-01-10      A -0.44566197\n20 2022-01-10      B -0.47279141\n\n\nBoth wide and long data formats have their own advantages and disadvantages, and the choice between them often depends on the specific data and the analysis that is planned. Transforming data from one format to another is a common task in data processing and analysis, and can be accomplished using various data manipulation tools in R, such as tidyr and reshape2 packages. Look back at the Italian google mobility data to check which format it is in.\nDate format\nTime series aim to study the evolution of one or several variables through time. Several packages that are part of the tidyverse family will help you analyse time series data in R. The lubridatepackage is your best friend to deal with the date format. ggplot2 will allow you to plot it efficiently. dygraphs will also help build attractive interactive charts.\nBuilding time series requires the time variable to be at the date format. The first step of your analysis must be to double check that R read your data correctly, i.e. at the date format. This is possible thanks to the str() function:\n\n# Checking date format\nstr(mobility_it)\n\n'data.frame':   36576 obs. of  15 variables:\n $ country_region_code                               : chr  \"IT\" \"IT\" \"IT\" \"IT\" ...\n $ country_region                                    : chr  \"Italy\" \"Italy\" \"Italy\" \"Italy\" ...\n $ sub_region_1                                      : chr  \"ALL\" \"ALL\" \"ALL\" \"ALL\" ...\n $ sub_region_2                                      : chr  \"\" \"\" \"\" \"\" ...\n $ metro_area                                        : logi  NA NA NA NA NA NA ...\n $ iso_3166_2_code                                   : chr  \"\" \"\" \"\" \"\" ...\n $ census_fips_code                                  : logi  NA NA NA NA NA NA ...\n $ place_id                                          : chr  \"ChIJA9KNRIL-1BIRb15jJFz1LOI\" \"ChIJA9KNRIL-1BIRb15jJFz1LOI\" \"ChIJA9KNRIL-1BIRb15jJFz1LOI\" \"ChIJA9KNRIL-1BIRb15jJFz1LOI\" ...\n $ date                                              : chr  \"01/01/2022\" \"02/01/2022\" \"03/01/2022\" \"04/01/2022\" ...\n $ retail_and_recreation_percent_change_from_baseline: int  -65 -27 -13 -13 -10 -30 -20 -27 -39 -22 ...\n $ grocery_and_pharmacy_percent_change_from_baseline : int  -80 7 28 30 42 -24 25 6 -7 19 ...\n $ parks_percent_change_from_baseline                : int  21 12 19 17 10 42 13 3 -36 -18 ...\n $ transit_stations_percent_change_from_baseline     : int  -49 -18 -36 -37 -37 -50 -38 -30 -34 -35 ...\n $ workplaces_percent_change_from_baseline           : int  -69 -13 -42 -41 -42 -78 -48 -27 -13 -21 ...\n $ residential_percent_change_from_baseline          : int  13 6 13 13 12 23 16 9 10 10 ...\n\n\nThis is already looking fine in the google mobility data, but in many other cases the lubridate package is such a life saver. It offers several function which name are composed by 3 letters: year (y), month (m) and day (d). Have a look at lubridate cheat sheet to see more about converting time variables to useful formats. There is also the anytime() function in the anytime package whose sole goal is to automatically parse strings as dates regardless of the format.\n\n#Convert the date column to a date format using the dmy() function \nmobility_it$date_fix &lt;- dmy(mobility_it$date)  \n\nTidy up some variables.\n\n# Rename variables\nmobility_it &lt;- mobility_it %&gt;%\n  rename(grocery_pharmacy = grocery_and_pharmacy_percent_change_from_baseline,\n         parks_percent_change_from_baseline = parks_percent_change_from_baseline,\n         transit = transit_stations_percent_change_from_baseline)\n\nInitial time-series plotting with ggplot2\nLet’s start easy by keeping only data for the whole of Italy.\n\n# Filter the dataframe to keep only the rows for all of Italy\nmobility_it_nogeo &lt;-filter(mobility_it, sub_region_1 == \"ALL\")\n\nggplot2 offers great features when it comes to visualize time series. The date format will be recognized automatically, resulting in a neat x axis labels.\n\n# Most basic bubble plot - uncomment to visualise\n#timeseries1 &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=grocery_pharmacy)) + geom_point()\n#timeseries1 \n\n# Adding lines and starting to clean up graph\ntimeseries2 &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=grocery_pharmacy)) +\n  geom_point(color=\"#061685\", shape=1) +\n  geom_line(color=\"#2c7bb6\") + \n  theme_tufte() +\n  xlab(\"\") +\n  ylab(\"Grocery and Pharmacy % change\") \n\ntimeseries2\n\n\n\n\n\n\n\n\nThe ggplot2 package recognizes the date format and automatically uses a specific type of X axis. If the time variable isn’t at the date format, this won’t work. Always check with str(data) how variables are understood by R. The scale_x_data() makes it a breeze to customize those labels.\n\n# Use the limit option of the scale_x_date() function to select a time frame in the data:\ntimeseries3 &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=grocery_pharmacy)) +\n  geom_point(color=\"#061685\", shape=1) +\n  geom_line( color=\"#2c7bb6\") +\n  xlab(\"\") +\n  ylab(\"Grocery and Pharmacy % change visits\") +\n  theme_tufte() +\n  scale_x_date(limit=c(as.Date(\"2022-01-01\"),as.Date(\"2022-04-01\"))) # Limiting between two dates \n\ntimeseries3\n\n\n\n\n\n\n\n\nplotly is also great to turn the resulting chart interactive in one more line of code. With the ggplotly() function you can hover circles to get a tooltip, or select an area of interest for zooming. You can zoom by selecting an area of interest. Hover over the line to get exact time and value. Export to a widget with htmlwidgets.\n\n# Usual chart\ntimeseries3bis &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=grocery_pharmacy)) +\n  geom_point(color=\"#061685\", shape=1) +\n  geom_line(color=\"#2c7bb6\") + \n  theme_tufte() +\n  xlab(\"\") +\n  ylab(\"Grocery and Pharmacy % change\") \n\n# Turn it interactive with ggplotly\ntimeseries_interactive &lt;- ggplotly(timeseries3bis)\ntimeseries_interactive\n\n\n\n\n# Of course this needs more cleaning to be a final output\n\n# To save the widget use the library(htmlwidgets)\n# saveWidget(p, file=paste0( getwd(), \"/HtmlWidget/ggplotlyAreachart.html\"))\n\nLinear trends\nNow let’s try to model trends in the data. Let’s first consider linear trends. Linear trends are the simplest way to model time as a quantitative variable. We can represent time as a series of evenly spaced points on a graph, and then fit a straight line to those points. The slope of the line tells us the rate at which the variable is changing over time. For example, if we are measuring population counts over time, a positive slope would indicate that a population increase, and a negative slope would indicate a population decrease.\n\n# Estimate linear regression model\nlinear_model &lt;- lm(grocery_pharmacy ~ date_fix, mobility_it_nogeo)\n\n# A simple plot line\nplot(mobility_it_nogeo$date_fix,                       \n     mobility_it_nogeo$grocery_pharmacy,\n     type = \"l\")\nlines(mobility_it_nogeo$date_fix,\n      predict(linear_model),\n      col = 2,\n      lwd = 2)\n\n\n\n\n\n\n\n# Extract coefficients of model and print\nmy_coef &lt;- coef(linear_model)            \nmy_coef                            \n\n  (Intercept)      date_fix \n-694.27809786    0.03706687 \n\n# Extract equation of model and print\nmy_equation &lt;- paste(\"y =\",        \n                     coef(linear_model)[[1]],\n                     \"+\",\n                     coef(linear_model)[[2]],\n                     \"* x\")\nmy_equation                        \n\n[1] \"y = -694.278097860568 + 0.037066871224827 * x\"\n\n\nLet’s plot this with ggplot to fit our line through our points. We can use the package ggpmisc to add the equation and R2 with stat_poly_line() and stat_poly_eq.\n\n# Using geom_smooth() for the linear fit\ntimeseries4a &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=grocery_pharmacy)) +\n  geom_point(color=\"#061685\", shape=1, alpha = 0.8) +\n  geom_smooth(formula = y ~ x, method=\"lm\") + # linear regression \n  xlab(\"\") +\n  ylab(\"Grocery & Pharmacy %\") +\n  stat_poly_line() +\n  stat_poly_eq(use_label(c(\"eq\", \"R2\"))) +\n theme_tufte() +\n  ggtitle(expression(paste(Delta, \"% Grocery & Pharmacy\")))\n  \ntimeseries4b &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=transit)) +\n  geom_point(color=\"#061685\", shape=1, alpha = 0.8) +\n  geom_smooth(formula = y ~ x, method=\"lm\") + #This is where your linear regression is\n  xlab(\"\") +\n  ylab(\"Transit stations %\") +\n  stat_poly_line() +\n  stat_poly_eq(use_label(c(\"eq\", \"R2\"))) +\n theme_tufte() +\n  ggtitle(expression(paste(Delta, \"% Transit stations\")))\n\ntimeseries4c &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=parks_percent_change_from_baseline)) +\n  geom_point(color=\"#061685\", shape=1, alpha = 0.8) +\n  geom_smooth(formula = y ~ x, method=\"lm\") + #This is where your linear regression is\n  xlab(\"\") +\n  ylab(\"Parks %\") +\n  stat_poly_line() +\n  stat_poly_eq(use_label(c(\"eq\", \"R2\"))) +\n theme_tufte() +\n  ggtitle(expression(paste(Delta, \"% Visit Parks\")))\n  \n  \ngrid.arrange(timeseries4a, timeseries4b, timeseries4c, ncol=3)\n\n\n\n\n\n\n\n\nAlternatively you can use the ols() function aswell followed by the stat_function(). See here for more details.\nLinear trends have some limitations. In many cases, the relationship between time and the variable we are measuring is not linear, and fitting a straight line may not capture the true nature of the relationship. In such cases, we may need to consider a quadratic trend.\nQuadratic trends\nQuadratic trends model time as a second-degree polynomial, which allows for a curved relationship between time and the variable we are measuring. Quadratic trends can capture more complex patterns in the data than linear trends, such as a gradual increase followed by a gradual decrease. For example, if we are measuring the number of COVID-19 cases over time, a quadratic trend may capture the initial exponential growth, followed by a flattening out of the curve.\nHowever, quadratic trends also have some limitations. They assume that the relationship between time and the variable we are measuring is symmetrical, which may not always be the case. In addition, they can be difficult to interpret, as the coefficient for the quadratic term does not have a straightforward interpretation.\n\ntimeseries5 &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=transit)) +\n  geom_point(color=\"#061685\", shape=1, alpha = 0.8) + \n  geom_smooth(formula = y ~ poly(x,2), method=\"lm\", se = T, level = 0.99) + # 2nd order polynomial & adjusting level of the confidence interval\n  xlab(\"\") +\n  ylab(\"Transit stations % change\") +\n theme_tufte() \n  \ntimeseries5\n\n\n\n\n\n\n\ntimeseries6 &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=transit)) +\n  geom_point(color=\"#061685\", shape=1, alpha = 0.8) + \n  geom_smooth(formula = y ~ poly(x,3), method=\"lm\", se = T, level = 0.99) + # 3rd order polynomial & adjusting level of the confidence interval\n  xlab(\"\") +\n  ylab(\"Transit stations % change\") +\n theme_tufte() \n  \ntimeseries6\n\n\n\n\n\n\n\n\nSplines\nFinally, we have splines. Splines are a more flexible way to model time as a quantitative variable. Splines allow us to fit a piecewise function to the data, where the function is a series of connected polynomial segments. Each segment captures a different part of the relationship between time and the variable we are measuring. Splines can capture more complex patterns in the data than linear or quadratic trends, and they can be customized to fit the specific shape of the relationship we are trying to model.\nHowever, splines also have some limitations. They can be computationally intensive, and the choice of the number and location of the knots (i.e., the points where the polynomial segments connect) can have a big impact on the results. Splines are useful when the underlying function is complex or unknown, and can be used for a variety of applications, including curve fitting, data smoothing, and prediction.\nIn R, you can plot x vs y using the plot() function. To plot a spline, you can use the spline() function to generate the points for the curve, and then plot the curve using the lines() function.\n\ntimeseries7 &lt;- ggplot(data = mobility_it_nogeo, aes(x=date_fix, y=grocery_pharmacy)) +\n  geom_point(color=\"#061685\", shape=1, alpha = 0.8) +\n  ggformula::stat_spline() + # spline\n  xlab(\"\") +\n  ylab(\"Grocery & Pharmacy %\") +\n theme_tufte() +\n  ggtitle(expression(paste(Delta, \"% Visit Parks\")))\n  \ntimeseries7\n\n\n\n\n\n\n\n\nLinear trends, quadratic trends, and splines are all ways to model time as a quantitative variable, each with their own strengths and weaknesses. The choice of method will depend on the specific research question and the shape of the relationship between time and the variable we are measuring. You can find more on descriptive timeseries analysis here.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Time</span>"
    ]
  },
  {
    "objectID": "longitudinal-1.html#modelling-time-and-space",
    "href": "longitudinal-1.html#modelling-time-and-space",
    "title": "8  Modelling Time",
    "section": "8.4 Modelling Time and Space",
    "text": "8.4 Modelling Time and Space\nWe can also examine the heterogeneity in the data by region. Some regions in Italy were had many more COVID-19 cases than others.\n\n# Mobility data by region over time\ntimeseries_all_1 &lt;- ggplot(data = mobility_it, aes(x=date_fix, y=transit, color = sub_region_1)) +\n  # geom_point(alpha = 0.8) + \n  geom_smooth(method=\"lm\",formula = y ~ poly(x,2), se=F) +\n  theme(\n  legend.position = \"bottom\",\n  panel.background = element_rect(fill = NA)) +\n  xlab(\"\") +\n  ylab(\"\") +\n  ggtitle(\"Change in use of transit stations\") \n\n# Initial Graph\ntimeseries_all_1  \n\n\n\n\n\n\n\nmobility_it_filtered &lt;- mobility_it %&gt;% \n  filter(sub_region_1 != \"ALL\")\n\n# Mobility data by region over time with some edits\ntimeseries_all_2 &lt;- ggplot(data = mobility_it_filtered, aes(x = date_fix, y = transit, color = sub_region_1)) +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), se = F, size = 1.2) +\n  scale_color_viridis(discrete = TRUE, option=\"mako\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(size = 14, hjust = 0.5, face = \"bold\"),\n    axis.text = element_text(size = 10),\n    axis.title = element_text(size = 12, face = \"bold\"),\n    legend.text = element_text(size = 10),\n    legend.title = element_text(size = 12, face = \"bold\")\n  ) +\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"Change in use of transit stations\",\n    color = \"Region\"\n  )\n\ntimeseries_all_2\n\n\n\n\n\n\n\n\nThese graphs would need further work. We could for example divide the data between North, Centre and Southern Italy. Let’s load the geojson of italian regions and plot the data.\n\n# Add the polygons of Italy to the environment\nitaly_iso3166 &lt;- st_read(\"data/longitudinal-1/italy_projected_simplified.geojson\")\n\nReading layer `italy_projected_simplified' from data source \n  `/Users/carmen/Documents/GitHub/r4ps/data/longitudinal-1/italy_projected_simplified.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 124 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1313364 ymin: 3933695 xmax: 2312062 ymax: 5220353\nProjected CRS: Monte Mario / Italy zone 1\n\n# create a simple plot\nplot(italy_iso3166$geometry)\n\n\n\n\n\n\n\n\nNow let’s join the data by regional code.\n\n# Join by regional code\nitaly_iso3166_tidy &lt;- italy_iso3166 %&gt;%\n  left_join(mobility_it, by=c(\"ISO3166.2\"=\"iso_3166_2_code\"))\n\n# check the first rows of the merged data table\n# head(italy_iso3166_tidy)\n\nTo start with, we can plot the data statically using ggplot.\n\n# One point in time\nitaly_sf_subset_1 &lt;- italy_iso3166_tidy %&gt;% filter(date_fix == \"2022-01-01\")\nitaly_sf_subset_2 &lt;- italy_iso3166_tidy %&gt;% filter(date_fix == \"2022-02-01\")\nitaly_sf_subset_3 &lt;- italy_iso3166_tidy %&gt;% filter(date_fix == \"2022-03-01\")\nitaly_sf_subset_4 &lt;- italy_iso3166_tidy %&gt;% filter(date_fix == \"2022-04-01\")\nitaly_sf_subset_5 &lt;- italy_iso3166_tidy %&gt;% filter(date_fix == \"2022-05-01\")\nitaly_sf_subset_6 &lt;- italy_iso3166_tidy %&gt;% filter(date_fix == \"2022-06-01\")\n\n#This may take some time to load\ng1 &lt;- ggplot() +\n  geom_sf(data = italy_sf_subset_1, aes(fill = transit, geometry = geometry)) +\n  scale_fill_gradient2(low = \"#d7191c\", mid = \"white\", high = \"darkblue\", \n                      midpoint = 0, guide = \"colourbar\") +\n  theme_void()\n\n \ng2 &lt;- ggplot() +\n  geom_sf(data = italy_sf_subset_2, aes(fill = transit, geometry = geometry)) +\n    scale_fill_gradient2(low = \"#d7191c\", mid = \"white\", high = \"darkblue\", \n                      midpoint = 0, guide = \"colourbar\") +\n  theme_void()\n\ng3 &lt;- ggplot() +\n  geom_sf(data = italy_sf_subset_3, aes(fill = transit, geometry = geometry)) +\n    scale_fill_gradient2(low = \"#d7191c\", mid = \"white\", high = \"darkblue\", \n                      midpoint = 0, guide = \"colourbar\") +\n  theme_void()\n\ng4 &lt;- ggplot() +\n  geom_sf(data = italy_sf_subset_4, aes(fill = transit, geometry = geometry)) +\n    scale_fill_gradient2(low = \"#d7191c\", mid = \"white\", high = \"darkblue\", \n                      midpoint = 0, guide = \"colourbar\") +\n  theme_void()\n\ng5 &lt;- ggplot() +\n  geom_sf(data = italy_sf_subset_5, aes(fill = transit, geometry = geometry)) +\n    scale_fill_gradient2(low = \"#d7191c\", mid = \"white\", high = \"darkblue\", \n                      midpoint = 0, guide = \"colourbar\") +\n  theme_void()\n\ng6 &lt;- ggplot() +\n  geom_sf(data = italy_sf_subset_6, aes(fill = transit, geometry = geometry)) +\n    scale_fill_gradient2(low = \"#d7191c\", mid = \"white\", high = \"darkblue\", \n                      midpoint = 0, guide = \"colourbar\") +\n  theme_void()\n\ngrid.arrange(g1, g2, g3, g4, g5, g6, ncol = 3, \n             top = textGrob(\"Change in use of transit by Italian Region\", gp=gpar(fontsize=18,font=2)))\n\n\n\n\n\n\n\n\nPlotting time and space interactively is more complex. There are quite a few examples of how to visualise data over space and time here.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Time</span>"
    ]
  },
  {
    "objectID": "longitudinal-1.html#questions",
    "href": "longitudinal-1.html#questions",
    "title": "8  Modelling Time",
    "section": "8.5 Questions",
    "text": "8.5 Questions\nFor the assignment, we will continue to focus on the United Kingdom as our geographical area of analysis. For this section, we will use Google Mobility data for the UK for 2021. It has the same format as the Google Mobility data for Italy used in this chapter. During this year the UK underwent a third national lockdown, limitations of gatherings and more. For details on the timeline you can have a look here.\nStart by loading both the csv and geojsons.\n\nmobility_gb &lt;- read.csv(\"data/longitudinal-1/2021_GB_Region_Mobility_Report.csv\", header = TRUE)\n\nuk_iso3166 &lt;- st_read(\"data/longitudinal-1/uk_projected_simplified.geojson\")\n\nReading layer `uk_projected_simplified' from data source \n  `/Users/carmen/Documents/GitHub/r4ps/data/longitudinal-1/uk_projected_simplified.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 245 features and 4 fields (with 16 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -50462.93 ymin: 5270.466 xmax: 655975.3 ymax: 1219809\nProjected CRS: OSGB36 / British National Grid\n\n\n\nChose three variables to focus on and plot them together using ggplot. Discuss what format the data and if you can see any shocks over time by referring to the UK COVID-19 timeline.\nChose one variable and fit a linear, quadratic or spline model. Justify why you think this is the most appropriate and how you would interpret the resulting equation.\nModel both time and space: use ggplot to create a graph of the data by region or other geographical area. You can chose what to focus on here and do not necessarily need to use all the geographical classifications available to you. Analyse the trends that appear in your plot.\nCreate a static or interactive map (bonus points) of your choice from the data.\n\nAnalyse and discuss what insights you obtain into people’s attitudes, concerns, and behaviors during the pandemic, as well as their response to interventions and policies.\n\n\n\n\nCinelli, Matteo, Walter Quattrociocchi, Alessandro Galeazzi, Carlo Michele Valensise, Emanuele Brugnoli, Ana Lucia Schmidt, Paola Zola, Fabiana Zollo, and Antonio Scala. 2020. “The COVID-19 Social Media Infodemic.” Scientific Reports 10 (1): 1–10.\n\n\nSchleicher, Andreas. 2020. “The Impact of COVID-19 on Education: Insights from\" Education at a Glance 2020\".” OECD Publishing.\n\n\nUgolini, Francesca, Luciano Massetti, Pedro Calaza-Martı́nez, Paloma Cariñanos, Cynnamon Dobbs, Silvija Krajter Ostoić, Ana Marija Marin, et al. 2020. “Effects of the COVID-19 Pandemic on the Use and Perceptions of Urban Green Space: An International Exploratory Study.” Urban Forestry & Urban Greening 56: 126888.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Time</span>"
    ]
  },
  {
    "objectID": "longitudinal-2.html#dependencies",
    "href": "longitudinal-2.html#dependencies",
    "title": "9  Assessing Interventions",
    "section": "9.1 Dependencies",
    "text": "9.1 Dependencies\n\n# Data\nlibrary(sf)\nlibrary(readr)\nlibrary(tidyverse)\n\n# Dates and Times\nlibrary(lubridate)\n\n# graphs\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(gridExtra)\nlibrary(viridis)\nlibrary(ggimage)\nlibrary(grid)\nlibrary(plotly)\n\n# Models\nlibrary(modelsummary)\nlibrary(broom)\nlibrary(gtools)"
  },
  {
    "objectID": "longitudinal-2.html#data",
    "href": "longitudinal-2.html#data",
    "title": "9  Assessing Interventions",
    "section": "9.2 Data",
    "text": "9.2 Data\nFirst let’s import the Greater London COVID-19 data:\n\n# import csv \ncovid_cases_london &lt;- read.csv(\"data/longitudinal-2/covid_cases_london.csv\", header = TRUE)\n# check out the variables\ncolnames(covid_cases_london)\n\n [1] \"areaType\"                       \"areaName\"                      \n [3] \"areaCode\"                       \"date\"                          \n [5] \"newCasesBySpecimenDate\"         \"cumCasesBySpecimenDate\"        \n [7] \"newFirstEpisodesBySpecimenDate\" \"cumFirstEpisodesBySpecimenDate\"\n [9] \"newReinfectionsBySpecimenDate\"  \"cumReinfectionsBySpecimenDate\" \n\n\nThen we do the same for the Lazio area data, which is the Region of the capital of Italy, Rome. We are choosing this region because it did not see sharp peaks in COVID-19 cases during the winter of 2020/2021.\n\n# import csv \ncovid_cases_lazio &lt;- read.csv(\"data/longitudinal-2/covid_cases_lazio.csv\", header = TRUE)\n# check out the variables\ncolnames(covid_cases_lazio)\n\n [1] \"data\"                    \"stato\"                  \n [3] \"codice_regione\"          \"denominazione_regione\"  \n [5] \"codice_provincia\"        \"denominazione_provincia\"\n [7] \"sigla_provincia\"         \"lat\"                    \n [9] \"long\"                    \"totale_casi\"            \n\n\nFirst we need to clean up the data somewhat and rename some variables in both dataframes to have 4 variables:\n\ndate: year-month-day\ngeo: geographical region\ncases: number of COVID-19 cases that day\narea : Lazio (Rome) or London\n\n\n# Rename the variables in the Lombardia data frame\ncovid_cases_lazio_ren &lt;- covid_cases_lazio %&gt;%\n  rename(date = data , geo = denominazione_provincia, totalcases = totale_casi) \n\n# Group the data by region and calculate total cases for each day (not cumulative cases)\ncovid_cases_lazio_daily &lt;- covid_cases_lazio_ren %&gt;%\n  group_by(geo) %&gt;%\n   mutate(cases = totalcases - lag(totalcases, default = 0)) %&gt;%\n  select(date, geo, cases) %&gt;%\n  mutate(area = \"Rome (Lazio)\") %&gt;%\n  filter(cases &gt;= 0)\n\n#df_milan &lt;- covid_cases_lombardia_new %&gt;%\n#  filter(geo == \"Milano\", new_cases &gt;= 0)\n\n# Rename the variables in the first data frame\ncovid_cases_london_ren &lt;- covid_cases_london %&gt;%\n  rename(date = date , geo = areaName, cases = newCasesBySpecimenDate) %&gt;%\n  select(date, geo, cases) %&gt;%\n  mutate(area = \"London\")\n\n# Correct date format\ncovid_cases_london_ren$date &lt;- as.Date(covid_cases_london_ren$date)\ncovid_cases_lazio_daily$date &lt;- as.Date(covid_cases_lazio_daily$date)\n\n# Append the renamed data frame to the second data frame\ncovid_combined &lt;- rbind(covid_cases_london_ren, covid_cases_lazio_daily)\n\n# Add a variable of log of cases\ncovid_combined &lt;- covid_combined %&gt;% \n          mutate(log_cases = log(cases))"
  },
  {
    "objectID": "longitudinal-2.html#data-exploration",
    "href": "longitudinal-2.html#data-exploration",
    "title": "9  Assessing Interventions",
    "section": "9.3 Data Exploration",
    "text": "9.3 Data Exploration\nSimilarly to the previous chapter. Let’s start by eyeballing the data.\n\n# Visualizing Cases in London\ncovid_cases_1 &lt;- ggplot(data = covid_cases_london_ren, aes(x = date, y = cases, color=geo)) +\n  geom_line() + \n  scale_color_viridis(discrete = TRUE, option=\"magma\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\")\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"Evolution in Covid-19 cases\",\n    color = \"Region\"\n  ) +\n  scale_x_date(date_breaks = \"6 months\")\n\nNULL\n\n  covid_cases_1\n\n\n\n# Visualizing Cases in Lazio (Rome)\ncovid_cases_2 &lt;- ggplot(data = covid_cases_lazio_daily, aes(x = date, y = cases, color=geo)) +\n  geom_line() + \n  scale_color_viridis(discrete = TRUE, option=\"magma\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\")\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"Evolution in Covid-19 cases\",\n    color = \"Region\"\n  ) +\n  scale_x_date(date_breaks = \"6 months\")\n\nNULL\n\ncovid_cases_2\n\n\n\n\nTo identify whether there is a timeperiod where a lockdown was implemented in one location but not the other, and how cases evolved, we can plot aggregates of both locations in one plot.\n\n# Aggregate the data by region for each day\ncovid_combined_agg &lt;- aggregate(cases ~ area + date, data = covid_combined, FUN = sum)\n\n# Visualizing aggregated\ncovid_cases_3 &lt;- ggplot(data = covid_combined_agg, aes(x = date, y = cases, color=area)) +\n  geom_line() + \n  scale_color_manual(values=c(\"darkblue\", \"darkred\")) + # set individual colors for the areas\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  ) +\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"Evolution in Covid-19 cases\",\n    color = \"Region\"\n  ) +\n  scale_x_date(date_breaks = \"6 months\")\n\ncovid_cases_3\n\n\n\n\nFrom an initial look at the data, the 2020/2021 winter period seems interesting as there is a high increase in London cases but not as much as a peak in Lazio cases. In fact, after a quick review of COVID-19 lockdowns, we found that:\n\nOn the 5th of November 2020, the UK Prime Minister announced a second national lockdown, coming into force in England\nOn 4 November 2020, Italian Prime Minister Conte announced a new lockdown as well, however this lockdown divided the country into three zones depending on the severity of the pandemic, corresponding to red, orange and yellow zones. The Lazio region, was a yellow zone for the duration of this second lockdown. In yellow zones, the only restrictions included compulsory closing for restaurant and bar activities at 6 PM, and online education for high schools only.\n\n\n# Usual chart\ncovid_cases_4 &lt;- ggplot(data = covid_combined_agg, aes(x = date, y = cases, color=area)) +\n  geom_line() + \n  scale_color_manual(values=c(\"darkblue\", \"darkred\")) + # set individual colors for the areas\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  ) +\n  labs(\n    x = \"\",\n    y = \"\",\n    title = \"Evolution in Covid-19 cases\",\n    color = \"Region\"\n  ) +\n  scale_x_date(limit=c(as.Date(\"2020-08-01\"), as.Date(\"2021-01-15\"))) +\n geom_vline(xintercept=as.numeric(as.Date(\"2020-11-05\")), linetype=\"dashed\") +\n  annotate(\"text\", x=as.Date(\"2020-11-06\"), y=25000, label=\"Lockdown\", \n           color=\"black\", fontface=\"bold\", angle=0, hjust=0, vjust=0)\n\ncovid_cases_4\n\n\n\n\nWe could make some assumptions and set this up as a quasi experiment. In social science, researchers are often using natural or quasi experimental setting as randomized experiments can rarely be conducted. This involves splitting the population at hand into a treatment and control group."
  },
  {
    "objectID": "longitudinal-2.html#difference-in-difference",
    "href": "longitudinal-2.html#difference-in-difference",
    "title": "9  Assessing Interventions",
    "section": "9.4 Difference in Difference",
    "text": "9.4 Difference in Difference\nPlotting Means\nFor a diff-in-diff analysis using COVID data, possible shocks that would make this type of quasi-experiment possible could be the following:\n\nNational lockdown: The first national lockdown in the UK was announced on March 23, 2020. This sudden shock to the economy and society could be used as a treatment group for the diff-in-diff analysis, with the pre-lockdown period as the control group.\nRegional lockdowns: The UK also implemented regional lockdowns throughout the pandemic, with different regions experiencing restrictions at different times. These regional lockdowns could be used as treatment groups, with regions that did not experience lockdowns as the control group.\nSchool closures: In response to the pandemic, schools in the UK were closed from March 20, 2020, until June 1, 2020, and then again from January 5, 2021, until March 8, 2021. The impact of school closures on education outcomes could be studied using a diff-in-diff approach, with the period before school closures as the control group.\nTravel restrictions: The UK implemented various travel restrictions throughout the pandemic, including quarantine requirements for travelers from certain countries. The impact of these travel restrictions on the tourism industry or the spread of the virus could be studied using a diff-in-diff approach.\nVaccine rollout: The UK began its COVID-19 vaccination program in December 2020. The impact of the vaccine rollout on various health and economic outcomes could be studied using a diff-in-diff approach, with the period before the rollout as the control group.\n\nThese are just a few examples of shocks that could be used for a diff-in-diff analysis using COVID data. The choice of shock will depend on the research question and the data available.\nThe DiD approach includes a before-after comparison for a treatment and control group. In our example:\n\nA cross-sectional comparison (= compare a sample that was treated (London) to an non-treated control group (Rome))\nA before-after comparison (= compare treatment group with itself, before and after the treatment (5th of November))\n\nThe main assumption is that without the change in the natural environment the outcome variable would have remained constant!\nFirst, we create a dummy variable to indicate the time when the treatment started. In our case this will be the 5th of November 2020. We will also limit the time-span of our data.\n\n# keep data from 2020-09-01 to 2021-01-01\ncovid_combined_filtered &lt;- covid_combined %&gt;%\n  filter(date &gt;= \"2020-09-01\" & date &lt;= \"2021-01-01\")\n\n# create a dummy variable to indicate the time when the treatment started (5 Nov 2020)\ncovid_combined_filtered &lt;- covid_combined_filtered %&gt;%\n  mutate(after_5nov = ifelse(date &gt;= \"2020-11-05\", 1, 0)) #changed to 05 Nov\n\n\n# Create a frequency table of area and treatment\nfreq_table &lt;- table(covid_combined_filtered$area, covid_combined_filtered$after_5nov)\n\n# Print the frequency table\nprint(freq_table)\n\n              \n                  0    1\n  London       3150  540\n  Rome (Lazio) 1436  240\n\n\nWe then want to plot averages to see differences between treatment/control groups and before/after. But we can also calculate the mean and 95% confidence interval. We can also use group_by() and summarize() to figure out group means before sending the data to ggplot.\n\nplot_data &lt;- covid_combined_filtered %&gt;% \n  # Make these categories instead of 0/1 numbers so they look nicer in the plot\n  mutate(after_5nov = factor(after_5nov, labels = c(\"Before 5 November 2020\", \"After 5 November 2020\"))) %&gt;% \n  group_by(area, after_5nov) %&gt;% \n  summarize(mean_cases = mean(cases),\n            se_cases = sd(cases) / sqrt(n()),\n            upper = mean_cases + (1.96 * se_cases),\n            lower = mean_cases + (-1.96 * se_cases)) \n\nggplot(plot_data, aes(x = area, y = mean_cases)) +\n  geom_pointrange(aes(ymin = lower, ymax = upper), \n                  color = \"darkred\", size = 1) +\n  facet_wrap(vars(after_5nov))\n\n\n\n\nHere, we can start to see a diff-in-diff plot, where there is little to no difference in means with our control city (Rome-Lazio) and a substancial jump in means in our treatment city (London). It looks there were many more cases of COVID-19 after the 5th of march in London, indicating the lockdown did not have an effect, at least in this time-frame. Why could that be?\nWe can also plot a more standard diff-in-diff format:\n\nggplot(plot_data, aes(x = after_5nov, y = mean_cases, color = area)) +\n  geom_pointrange(aes(ymin = lower, ymax = upper), size = 1) + \n  geom_line(aes(group = area)) +\n  scale_color_manual(values = c(\"darkblue\", \"darkred\"))\n\n\n\n\nThis second plot shows us it is probable that our diff-n-diff set up will not work. A clean classic diff-n-diff would look more like the following. Please note the following plot is theoretical.\n\n# import csv \ncovid_perfect_example &lt;- read_csv(\"data/longitudinal-2/example_covid.csv\")\n\n# label pre/post labels\ncovid_perfect_example &lt;- covid_perfect_example %&gt;%\n  mutate(after_5nov = factor(after_5nov, labels = c(\"Before 5 November 2020\", \"After 5 November 2020\")))\n\n# plot    \nggplot(covid_perfect_example, aes(x = after_5nov, y = mean_cases, color = area)) +\n  geom_pointrange(aes(ymin = lower, ymax = upper), size = 1) + \n  geom_line(aes(group = area)) +\n  scale_color_manual(values = c(\"darkblue\", \"darkred\"))\n\n\n\n\nDifference in Difference by hand\nWe can find the exact difference by filling out the 2x2 before/after treatment/control table:\n\n\n\n\nBefore\nAfter\nDifference\n\n\n\n\nTreatment\nA\nB\nB - A\n\n\nControl\nC\nD\nD - C\n\n\nDifference\nC - A\nD - B\n(D − C) − (B − A)\n\n\n\nA combination of group_by() and summarize() makes this really easy. We can pull each of these numbers out of the table with some filter()s and pull():\n\nbefore_treatment &lt;- covid_perfect_example %&gt;% \n  filter(after_5nov == \"Before 5 November 2020\", area == \"London\") %&gt;% \n  pull(mean_cases)\n\nbefore_control &lt;- covid_perfect_example %&gt;% \n  filter(after_5nov == \"Before 5 November 2020\", area == \"Lazio\") %&gt;% \n  pull(mean_cases)\n\nafter_treatment &lt;- covid_perfect_example %&gt;% \n  filter(after_5nov == \"After 5 November 2020\", area == \"London\") %&gt;% \n  pull(mean_cases)\n\nafter_control &lt;- covid_perfect_example %&gt;% \n  filter(after_5nov == \"After 5 November 2020\", area == \"Lazio\") %&gt;% \n  pull(mean_cases)\n\ndiff_treatment_before_after &lt;- after_treatment - before_treatment\ndiff_treatment_before_after\n\n[1] 156.35\n\ndiff_control_before_after &lt;- after_control - before_control\ndiff_control_before_after\n\n[1] 24.35387\n\ndiff_diff &lt;- diff_treatment_before_after - diff_control_before_after\ndiff_diff\n\n[1] 131.9961\n\n\nThe diff-in-diff estimate is 131.99, which means that the lockdown here caused an increase in cases in the time-window we are analysing. Not it’s intended effect!\nWe can visualise this really well with a bit of extra code:\n\nggplot(covid_perfect_example, aes(x = after_5nov, y = mean_cases, color = area)) +\n  geom_point() +\n  #geom_pointrange(aes(ymin = lower, ymax = upper), size = 1) + \n  #geom_line(aes(group = area)) +\n  geom_line(aes(group = as.factor(area))) +\n  scale_color_manual(values = c(\"darkblue\", \"darkred\")) +\n  # If you use these lines you'll get some extra annotation lines and\n  # labels. The annotate() function lets you put stuff on a ggplot that's not\n  # part of a dataset. Normally with geom_line, geom_point, etc., you have to\n  # plot data that is in columns. With annotate() you can specify your own x and\n  # y values.\n  annotate(geom = \"segment\", x = \"Before 5 November 2020\", xend = \"After 5 November 2020\",\n           y = before_treatment, yend = after_treatment - diff_diff,\n           linetype = \"dashed\", color = \"grey50\") +\n  annotate(geom = \"segment\", x = \"After 5 November 2020\", xend = \"After 5 November 2020\",\n           y = after_treatment, yend = after_treatment - diff_diff,\n           linetype = \"dotted\", color = \"blue\") +\n  annotate(geom = \"label\", x = \"After 5 November 2020\", y = after_treatment - (diff_diff / 2), \n           label = \"Program effect\", size = 3)\n\n\n\n\nIt is important for all diff-in-diff analyses to give careful attention to possible violations of the common trends assumption, especially considering the COVID-19 situation where many of these violations are likely to occur. Furthermore, due to the unique dynamics of COVID-19 such as lags between exposure and recorded infections, nonlinearities from person-to-person transmission, and the possibility of policies having differential effects over time, it further complicates the potential risks to the diff-in-diff research design.\n(Goodman-Bacon and Marcus 2020) comment on the following problems which can be consulted in their paper:\n\nPackaged Policies\nReverse Causality\nVoluntary Precautions\nDifference Data collection\nAnticipation Spillovers\nVariation in Policy Timing\n\n(Goodman-Bacon and Marcus 2020) also give great recommendations on how to address these problems, but this is far beyond the objective of this chapter.\nDifference-in-Difference with regression\nCalculating all the pieces by hand like that is tedious, so we can use regression to do it instead! Remember that we need to include indicator variables for treatment/control and for before/after, as well as the interaction of the two.\nThis is the equation:\n\\(\\Delta Y_{gt} = \\beta_0 + \\beta_1 London_{g} + \\beta_2 Post5Nov_{t} + \\beta_3 London_{g} \\times Post5Nov_{t} + \\beta_4 Rome_{g} + \\epsilon_{gt}\\)\nThe output will show the diff-in-diff coefficient estimate, standard error, t-value, and p-value, which can be used to determine whether there was a significant effect of the second lockdown 4 on Covid cases in November 2020.\n\nmodel_small &lt;- lm(cases ~ area + after_5nov + area * after_5nov,\n                  data = covid_combined_filtered)\n\n# Tidy the model output\ndiffndiff1 &lt;- tidy(model_small) \n\n# Add significance stars using stars.pval from gtools\ndiffndiff1$stars &lt;- stars.pval(diffndiff1$p.value)\n\n# View the results\ndiffndiff1\n\n# A tibble: 4 × 6\n  term                        estimate std.error statistic   p.value stars\n  &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;\n1 (Intercept)                     55.7      4.47      12.5 3.62e- 35 ***  \n2 areaRome (Lazio)               126.       7.99      15.8 8.81e- 55 ***  \n3 after_5nov                     301.      11.7       25.7 7.63e-138 ***  \n4 areaRome (Lazio):after_5nov   -269.      21.0      -12.8 5.37e- 37 ***  \n\n# Create a model summary table for the model\nsummary_table &lt;- modelsummary(list(\"Simple\" = model_small), estimate =c(\"{estimate}{stars}\"))\n\n# View the results\nsummary_table\n\n\n\n\n\nSimple\n\n\n\n\n(Intercept)\n55.694***\n\n\n\n(4.469)\n\n\nareaRome (Lazio)\n125.910***\n\n\n\n(7.986)\n\n\nafter_5nov\n300.656***\n\n\n\n(11.681)\n\n\nareaRome (Lazio) × after_5nov\n−269.302***\n\n\n\n(21.032)\n\n\nNum.Obs.\n5366\n\n\nR2\n0.130\n\n\nR2 Adj.\n0.130\n\n\nAIC\n74524.8\n\n\nBIC\n74557.7\n\n\nLog.Lik.\n−37257.375\n\n\nF\n267.482\n\n\nRMSE\n250.71"
  },
  {
    "objectID": "longitudinal-2.html#questions",
    "href": "longitudinal-2.html#questions",
    "title": "9  Assessing Interventions",
    "section": "9.5 Questions",
    "text": "9.5 Questions\nFor the assignment, you will continue to use Google Mobility data for the UK for 2021. For details on the timeline you can have a look here. You will need to do a bit of digging on when lockdowns or other COVID-19 related shock happened in 2021 to set up a diff-in-diff strategy. Have a look at Brodeur et al. (2021) to get some inspiration. They used Google Trends data to test whether COVID-19 and the associated lockdowns implemented in Europe and America led to changes in well-being.\nStart by loading both the csv\n\nmobility_gb &lt;- read.csv(\"data/longitudinal-1/2021_GB_Region_Mobility_Report.csv\", header = TRUE)\n\n\nVisualize the data with ggplot and identify what section of the data could be used to evaluate a COVID-19 intervention. Examples of these interventions could be a regional lockdown, school closures, travel restrictions or vaccine rollouts. Generate a clean ggplot which indicates which intervention you are going to examine.\nExplore differences in means through a frequency table and a graph of these averages. Chose whichever suits your purposes best.\nDefine and estimage a diff-in-diff regression. What do the results suggest? Was the intervention you chose effective? Discuss the reasons why it was or was not.\nDiscuss how the unique dynamics of COVID-19 and the possibility of policies having differential effects over time complicate the interpretation of your results.\n\nAnalyse and discuss what insights you obtain into people’s changes in behaviors during the pandemic in responde to an intervention.\n\n\n\n\nBrodeur, Abel, Andrew E Clark, Sarah Fleche, and Nattavudh Powdthavee. 2021. “COVID-19, Lockdowns and Well-Being: Evidence from Google Trends.” Journal of Public Economics 193: 104346.\n\n\nGoodman-Bacon, Andrew, and Jan Marcus. 2020. “Using Difference-in-Differences to Identify Causal Effects of COVID-19 Policies.”\n\n\nZhou, Muzhi, and Man-Yee Kan. 2021. “The Varying Impacts of COVID-19 and Its Related Measures in the UK: A Year in Review.” PLoS One 16 (9): e0257286."
  },
  {
    "objectID": "machine-learning.html#dependencies",
    "href": "machine-learning.html#dependencies",
    "title": "10  Machine Learning",
    "section": "10.1 Dependencies",
    "text": "10.1 Dependencies\n\n# Import the dplyr package for data manipulation\nsuppressMessages(library(dplyr))\n# Import the rpart package for decision tree modeling\nsuppressMessages(library(rpart))\n# Import the rpart.plot package for visualization of decision trees\nsuppressMessages(library(rpart.plot))\n# Import ggplot 2 to make plots\nsuppressMessages(library(ggplot2))\n# Import Metrics for performance metrics calculation\nsuppressMessages(library(Metrics))\n# Import caret for machine learning modeling and evaluation\nsuppressMessages(library(caret))\n# Import randomForest for the random forest algorithm\nsuppressMessages(library(randomForest))\n# Import ranger for the ranger implementation of random forest, which is optimised for performance\nsuppressMessages(library(ranger))"
  },
  {
    "objectID": "machine-learning.html#data",
    "href": "machine-learning.html#data",
    "title": "10  Machine Learning",
    "section": "10.2 Data",
    "text": "10.2 Data\nAs mentioned above, we will learn about decision trees and random forests through a practical example where the goal is to predict the net annual household income across different regions of the UK. Given that UK censuses take place separately in Northern Ireland, Scotland and England & Wales, we will focus only on England & Wales for simplicity.\nIn the code chunk below, we load a data set that has already been prepared for this notebook. It includes a variety of variables related to demographic characteristics of the population, aggregated at the Middle-Layer Super Output Area (MSOA) level. All the variables, except for the average annual household income, are derived from the 2021 census and the raw data can be downloaded from here. The annual household income data is from the Annual Survey of Hours and Earnings and can be downloaded from this link.\n\n# Load the data\ndf_MSOA &lt;- read.csv(\"./data/machine-learning/census2021-msoa-income.csv\")\n# Data cleaning, remove the X field\ndf_MSOA$X &lt;- NULL\n# Data cleaning, the fields \"date\", \"geography\" and \"geography.code\" are not needed\ndf_MSOA &lt;- subset(df_MSOA, select = -c(date, geography, geography.code))\n# More data cleaning, remove comma from income and turn it into a numeric value\ndf_MSOA$INCOME &lt;- as.numeric(gsub(\",\", \"\", df_MSOA$INCOME))\n\nFor a description of the variables in the columns of df_MSOA, we can load a dictionary for these variables:\n\n# Load variable dictionary\ndf_dictionary &lt;- read.csv(\"./data/machine-learning/Dictionary.csv\")\nhead(df_dictionary)\n\n                                      Dictionary       X\n1                                                       \n2                                           Name     Key\n3                 Lives in household (% persons)    inHH\n4    Lives in communal establishment (% persons)    inCE\n5 Never married or civil partnership (% persons)    SING\n6    Married or in civil partnership (% persons) MARRIED"
  },
  {
    "objectID": "machine-learning.html#splitting-the-data",
    "href": "machine-learning.html#splitting-the-data",
    "title": "10  Machine Learning",
    "section": "10.3 Splitting the data",
    "text": "10.3 Splitting the data\nFor most supervised learning problems, the goal is to find an algorithm or model that not only fits well known data, but also accurately predicts unknown values of the average annual household income based on a set of inputs. In other words, we want the algorithm to be generalisable. In order to measure the generalisability of the optimal algorithm, we can split the data into a training set containing input and output data and a test set. The training set is used to teach the algorithm how to map inputs to outputs, and the test set is used to estimate the prediction error of the final algorithm, which quantifies the generalisability of the model. The test set should never be used during the training stage.\nAs a rule of thumb, the data should be split so that 70% of the samples are in the training set and 30% in the test set, although this percentages might vary slightly according to the size of the original data set. Furthermore, to ensure generalisability, the data split should be done so that the distribution of outputs in both the training and test set is approximately the same.\nThe function create_train_test below (borrowed from here) allows us to select samples from the data to create the training set (when the train parameter is set to TRUE) and the test set (when the train parameter is set to FALSE).\n\ncreate_train_test &lt;- function(data, size = 0.7, train = TRUE) {\n n_row = nrow(data)\n total_row = size * n_row\n train_sample &lt;- 1: total_row\n if (train == TRUE) {\n return (data[train_sample, ])\n } else {\n return (data[-train_sample, ])\n }\n}\n\n\ndf_train &lt;- create_train_test(df_MSOA, 0.7, train = TRUE)\ndf_test &lt;- create_train_test(df_MSOA, 0.7, train = FALSE)\n\nIf the data is naively split into training and test sets as we did above, the distribution of outputs in the training and test set will not be the same, as the following plot shows.\n\nd1 &lt;- density(df_test$INCOME)\nplot(d1, col='blue', xlab=\"Income\", main=\"Distribution of income\")\n\nd2 &lt;- density(df_train$INCOME)\nlines(d2, col='red')\n\nlegend(52000, 0.00006, c(\"test set\",\"training set\"), lwd=c(2,2), col=c(\"blue\",\"red\"))\n\n\n\n\nThis is due to the fact that the dataset that we loaded at the beginning of this workbook is sorted so that some entries corresponding to MSOAs that are geographically close to each other are in consecutive rows. Therefore, to ensure that the distribution of outputs in training and test sets is the same, the data needs to be randomly shuffled. The code below achieves this goal, as it can be seen in the kernel density plot with the new data split.\n\n# Shuffle the entries of the original dataset\nshuffle_index &lt;- sample(1:nrow(df_MSOA))\ndf_MSOA &lt;- df_MSOA[shuffle_index, ]\n\n\n#Perform the data split with the new shuffled dataset\ndf_train &lt;- create_train_test(df_MSOA, 0.7, train = TRUE)\ndf_test &lt;- create_train_test(df_MSOA, 0.7, train = FALSE)\n\n\n#Plot the kernel density for both training and test data sets\nd1 &lt;- density(df_test$INCOME)\nplot(d1, col='blue', xlab=\"Income\", main=\"Distribution of income\")\n\nd2 &lt;- density(df_train$INCOME)\nlines(d2, col='red')\n\nlegend(52000, 0.00006, c(\"test set\",\"training set\"), lwd=c(2,2), col=c(\"blue\",\"red\"))\n\n\n\n\nBefore proceeding any further we should note here that one of the advantages of decision trees and random forests is that they are not sensitive to the magnitude of the input variables, so standardisation is not needed before fitting these models. However, there are other ML algorithms such as k-means, where standardisation is a crucial step to ensure the success of learning process and it should always take place before training the algorithm. Similarly, random forests are not sensitive to correlations between independent variables, so there is no need to check for correlations before training the models."
  },
  {
    "objectID": "machine-learning.html#decision-trees",
    "href": "machine-learning.html#decision-trees",
    "title": "10  Machine Learning",
    "section": "10.4 Decision trees",
    "text": "10.4 Decision trees\n\n10.4.1 Fitting the training data\nDecision trees are an ML algorithm capable of performing both classification and regression tasks, although in this workbook we will focus only on regression. One of their most notable advantages is that they are very interpretable, although their predictions are not always accurate. However, by aggregating decision trees through a method called bagging, the algorithm can become much more powerful.\nIn essence, a decision tree is like a flowchart that helps us make a decision based on a number of questions or conditions, which are represented by internal nodes. It starts with a root node and branches out into different paths, with each branch representing a decision. The final nodes represent the outcome and are known as leaf nodes. For example, imagine you are trying to decide what to wear to an event. You could create a decision tree with the root node being “Formal event?”, like in the diagram below. If the answer is “yes” you would proceed to the left and if the answer if “no”, you would proceed to the right. On the right, you would have another node for “Black tie?”, and again two “yes” and “no” branches emerging from it. On the left, you would have a node for “Everyday wear?” with its two branches. Each branch would eventually lead to a decision or action represented by the so-called leaf nodes, such as “wear suit”.\n\n\n\n\n\nThe decision tree to predict the annual household income of an MSOA based on its demographic features will be a lot more complex than the one depicted above. The branches will lead to leaf nodes representing the predicted values of annual household income. The internal nodes will represent conditions for the demographic variables (e.g. is the percentage of people aged 65 or over in this MSOA greater than X %?). Not all the demographic variables will be equally relevant to predict the annual household income at the MSOA level. To optimise the prediction process, conditions on the most relevant variables should be near the root of the tree so the most determining questions can be asked at the beginning of the decision-making process. The internal nodes that are further from the tree will help fine-tune the final predictions. But, how can we choose which are the most relevant variables? And, what are the right questions to ask in each internal node (e.g. if we are asking ‘is the percentage of people aged 65 or over in this MSOA greater than X %?’, what should be the value of X ?).\nLuckily, nowadays there are many off-the-shelf software packages available that can help us build decision trees with just a line of code. Here, we use the R package rpart, which is based on the Classification And Regression Tree (CART) algorithm proposed by Breiman (Breiman 1984). In particular, we can use the function rpart() to find the decision tree that best fits the training set. This function requires to use the formula method for expressing the model. In this case, the formula is INCOME ~., which means that we regard the variable INCOME as a function of all the other variables in the training data set. For more information on the formula method, you can check the R documentation. The function rpart() also requires to specify the method for fitting. Since we are performing a regression task (as opposed to classification), we need to set method to 'anova'.\n\nfit &lt;- rpart(INCOME ~., data = df_train, method = 'anova')\n\nWe can visualise the fitted decision tree with the rpart.plot() function from the library with the same name. Interestingly, the condition for the root node is associated with the variable representing the percentage of people born in Antarctica, Oceania and Other, so if an MSOA has less than 0.25% people born in those territories, the model predicts it to have lower annual household income. Can you think of why this might be the case?\n\nrpart.plot(fit)\n\n\n\n\nAs we can see, rpart() produces a decision tree with 11 leaf nodes and the conditions to reach each leaf are associated with only 8 demographic variables. However, in the original training set, there were many more demographic variables that have not been included in the model. The reason for this is that, behind the scenes, rpart() is trying to find a balance between the complexity of the tree (i.e. its depth) and the generalisability of the tree to predict new unseen data. If it is too deep, the tree runs the risk of overfitting the training data and failing to predict the annual household income for other MSOAs that are not included in the training set.\nTo illustrate the point of selecting a tree with 11 leaves, we can manually control the level of complexity allowed when fitting of a decision tree model. The lower we set the value of the parameter cp, the more complex the resulting tree will be, so cp can be regarded as penalty for the level of complexity or a cost complexity parameter. Below, we fit a decision tree with no penalty for generating a complex tree model, i.e. cp=0, and then we use the function plotcp() to plot the prediction error (PRESS statistic) that would be achieved with decision trees of different levels of complexity.\n\n# set seed for reproducibility\nset.seed(123)\n# train model\nfit2 &lt;- rpart(INCOME ~., data = df_train, method = 'anova', control = list(cp = 0))\n# plot error vs tree complexity \nplotcp(fit2)\n# draw vertical line at 11 trees\nabline(v=11, lty='dashed')\n\n\n\n\nAs we can see from the plot above, with more than 11 leaves, little reduction in the prediction error is achieved as the model becomes more and more complex. In other words, we start seeing diminishing returns in error reduction as the tree grows deeper. Hence rpart() is doing some behind-the-scenes tuning by pruning the tree so it only has 11 leaf nodes.\nThere are other model parameters that can be tuned in order to improve the model performance via the control argument of the rpart() function, just like we did above for cp. While we do not experiment with these additional parameters in the workbook, we provide brief descriptions below so you can explore them on your own time:\n\nminsplit controls the minimum number of data points required in each leaf node. The default is 20. Setting this lower will result in more leaves with very few data points belonging to the corresponding branch.\nmaxdepth controls the maximum number of internal nodes between the root node and the terminal nodes. By default, it is set to 30. Setting it higher allows to create deeper trees.\n\n\n\n10.4.2 Measuring the performance of regression models\nTo measure the performance of the regression tree that we fitted above, we can use the test set. We firstly use the predict() function from the rpart library to compute some predictions on the MSOA annual household income for the test set data.\n\npredict_unseen &lt;-predict(fit, df_test)\n\nThen, we compare the predictions with the actual values and measure the discrepancy with a regression error metric.\nNote that, to measure the performance of classification trees, the procedure would be slightly different and it would involve the computation of a confusion matrix and the accuracy metric.\nDifferent error metrics exist to measure the performance of regression models such as the Mean Squared Error (MSE), the Mean Absolute Error (MAE) or the Root Mean Squared Error (RMSE). The MSE is more sensitive to outliers than the MAE, however, the units of MSE are squared units. The RMSE solves the problem of the squared units associated with MSE by taking its squared root. The library Metrics provides the in-built function rmse() which makes the computation of RMSE straightforward:\n\nrmse(predict_unseen, df_test$INCOME)\n\n[1] 4573.012\n\n\nThis value does not have much meaning in isolation. A good or bad RMSE is always relative to the specific data set. For this reason, we need to establish a baseline RMSE that we can compare it with. Here we establish this baseline as the RMSE that would be obtained from a naive tree that merely predicts the mean annual household income value across all the data entries in the training set. If the fitted model achieves an RSME lower than the naive model, we say that the fitted model “has skill”. The following line of code confirms that our fitted model is better than the naive model.\n\nrmse(predict_unseen, mean(df_train$INCOME))\n\n[1] 5307.83\n\n\n\n\n10.4.3 Bagging\nEven though single decision trees have many advantages such as being very simple and interpretable, their predictions are not always accurate due to their high variance. This results in unstable predictions that may be dependent on the chosen training data.\nA method called bagging can help solve this issue by combining and averaging the predictions of multiple decision tree models. The method can actually be applied to any regression or classification model, however, it is most effective when applied to models that have high variance. Bagging works by following three steps:\n\nCreate m bootstrap samples from the training data (i.e. m random samples with replacement).\nFor each bootstrap sample, train an unpruned single tree (i.e. with cp=0).\nTo create a prediction for a new data point, input the data in the single trees fitted with each bootstrap sample. The prediction will be the average of all the individual predictions output by each tree.\n\nEach bootstrap sample typically contains about two-thirds of the training data, leaving one-third out. This left-out third is known as the out-of-bag (OOB) sample and it provides a natural opportunity to cross-validate the predictive performance of the model. By cross-validating the model, we can estimate how well our model will perform on new data without necessarily having to use the test set to test it. In other words, we can use the whole of the original data set for training and still quantify the performance of the model. However, for simplicity, we will train the model on the training set only here.\nBagging can be easily done with a library called caret. Here we fit a 10-fold cross-validated model, meaning that the bagging is applied so that there are 10 different OOB samples.\n\n# specify 10-fold cross validation\nctrl &lt;- trainControl(method = \"cv\",  number = 10) \n# set seed for reproducibility\nset.seed(123)\n# train the cross-validated bagged model\nbagged &lt;- caret::train(INCOME ~ ., data = df_train, method = \"treebag\", trControl = ctrl, importance = TRUE)\n\nprint(bagged)\n\nBagged CART \n\n4956 samples\n  48 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 4459, 4460, 4461, 4462, 4461, 4460, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  4232.607  0.6519682  3160.147\n\n\nWe see that the cross-validated value of RMSE with bagging is lower than that associated with the single decision tree that we trained with rcart. This indicates that the predictive performance is estimated to be better. We can compare the cross-validated value of the RMSE with the RMSE from the test set. These two quantities should be close:\n\npredict_bagged_unseen &lt;- predict(bagged, df_test)\nrmse(predict_bagged_unseen, df_test$INCOME)\n\n[1] 4052.085\n\n\nThe library caret has an additional function varImp() that helps us understand the variable importance across the bagged trees, i.e. the variables that are most relevant to determine the predictions of MSOA net annual household income. You are welcome to check the caret documentation to learn more about how the variable importance is determined. We can plot a rank of variable importance by running the code below.\n\nplot(varImp(bagged), 20)\n\n\n\n\nAs noted before, given the variables included in our original data set, the percentage of people born in Antarctica, Oceania and Other is, almost invariably, the best predictor of annual household income, although due to the randomness introduced in bagging, this could sometimes change."
  },
  {
    "objectID": "machine-learning.html#random-forests",
    "href": "machine-learning.html#random-forests",
    "title": "10  Machine Learning",
    "section": "10.5 Random forests",
    "text": "10.5 Random forests\nWhile bagging considerably improves the performance of decision trees, the resulting models are still subject to some issues. Mainly, the multiple trees that are fitted through the bagging process are not completely independent of each other since all the original variables are considered at every split in every tree. As a consequence, trees in different bootstrap samples have similar structure (with almost always the same variables near the root) and the variance in the predictions cannot be reduced optimally. This issue is known as tree correlation.\nRandom forests optimally reduce the variance of the predicted values by minimising the tree correlation. This is achieved in two steps:\n\nLike in bagging, different trees are fitted from bootstrap samples.\nHowever, when an internal node is to be created in a given tree, the search for the optimal variable in that node is limited to only a random subset of the explanatory variables. By default, the number of variables in these subsets is one-third of the total number of variables, although this proportion is considered a tuning parameter for the model.\n\n\n10.5.1 Basic implementation\nSeveral R implementations for random forest fitting exist, however, the most well known is provided by the randomForest library. By default it performs 500 trees (i.e. 500 bootstrap samples) and randomly selects one-third of the explanatory variables for each split, although these parameters can be manually tuned. The random forest model can be trained by executing just a line of code:\n\n# set seed for reproducibility\nset.seed(123)\n# train model\nfit_rf &lt;- randomForest(formula= INCOME ~., data = df_train)\n# print summary of fit\nfit_rf\n\n\nCall:\n randomForest(formula = INCOME ~ ., data = df_train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 16\n\n          Mean of squared residuals: 10244978\n                    % Var explained: 79.61\n\n\nAs we can see from above, the mean of squared residuals, which is the same as the MSE, is 9903736 for 500 trees and therefore, RMSE = 3147. This metric is computed by averaging residuals from the OOB samples. To illustrate how the MSE varies as more bootstrap samples are added to the model, we can plot the fitted model:\n\nplot(fit_rf, main = \"Errors vs no. of trees\")\n\n\n\n\nWe see that the MSE becomes stable with approximately 100 trees, but it continues to decrease slowly. To find the number of trees that lead to the minimum error, we can run the following line:\n\nwhich.min(fit_rf$mse)\n\n[1] 426\n\n\nBy computing the RMSE, we can compare the performance of this model with performance of the models in the previous sections:\n\nsqrt(fit_rf$mse[which.min(fit_rf$mse)])\n\n[1] 3197.19\n\n\nThis is a much lower value than what we obtained with just a single tree and even after applying bagging! Remember, this RMSE is based on the OOB samples, but we could also obtain it from the test set. randomForest() allows us to easily compare the RMSE obtained from OOB data and from the test set.\n\n# format test set for comparison of errors with randomForest\nx_test &lt;- df_test[setdiff(names(df_test), \"INCOME\")]\ny_test &lt;- df_test$INCOME\n\n# set seed for reproducibility\nset.seed(123)\n# include test data in training\nrf_oob_compare &lt;- randomForest(formula = INCOME ~ ., data  = df_train, xtest = x_test, ytest = y_test)\n\n# extract OOB & test errors\noob_rmse &lt;- sqrt(rf_oob_compare$mse)\ntest_rmse &lt;- sqrt(rf_oob_compare$test$mse)\n\n# plot error vs no. of trees\nx=1:rf_oob_compare$ntree\nplot(x, oob_rmse, type=\"l\", col='blue', lwd=2, xlab = \"No. of trees\", ylab = \"RMSE\", main = \"Comparison of RMSE\")\nlines(x, test_rmse, type=\"l\", col='red', lwd=2)\nlegend(350, 4500, c(\"OOB\",\"test\"), lwd=c(2,2), col=c(\"blue\",\"red\"))\n\n\n\n\n\n\n10.5.2 Tuning\nYou may have noticed that the number of trees is not the only parameter we can tune in a random forest model. Below we list the model parameters, a.k.a. hyperparameters that can be tuned to improve the performance of the random tree models:\n\nnum.trees is the number of trees. It should be large enough to make sure the MSE (or the RMSE) stabilises, but not too large that it creates unnecessary work.\nmtry is the number of variables that are randomly sampled at each split. The default is one-third of the number of variables in the original data set. If mtry was equal to the total number of variables, the random forest model would be equivalent to bagging. Similarly, if mtry was equal to 1, it would mean that only one variable is chosen, but then the results can become too biased. To find the optimal value of mtry, it is common to attempt 5 values evenly spread between 2 and the total number of variables in the original data set.\nsample.fraction controls the number of data points in each bootstrap sample, i.e. the number of samples chosen to create each tree. By default, it is 63.25% (about two-thirds) of the training set since on average, this guarantees unique data points in a sample. If the sample size is smaller, it could reduce the training time but it could also introduce some bias in the model. If the sample size is larger, it could lead to overfitting. When tuning the model, this parameter is frequently kept between 60 and 80% of the total size of the training set.\nmin.node.size is the minimal node size to split at. Default is 5 for regression.\nmax.depth is the maximum depth of the trees.\n\nIn order to find the combination of hyperparameters that leads to the best performing model, we need to try them all and select the one with the lowest MSE or RMSE. This is usually a computationally heavy task, so as the models and the training data become larger, the process of tuning can become very slow. The library ranger provides a C++ implementation of the random forest algorithm and allows to perform hyperparameter search faster than randomForest.\nAs mentioned, to find the best performing model, we need to find the right combination of hyperparameters. So the first step in the tuning process is to generate a “grid” of possible combinations of hyperparameters. If we only wanted to tune ntree (as we did in the previous subsection when we found that the number of trees leading to the lowest value of MSE is 495), the grid would be simply a list of possible ntree values. To illustrate more complex tuning, here we generate a grid that considers mtry, sample.fraction and min.node.size. The grid is created as follows:\n\n# Considering that there are 48 explanatory variables in the original dataset, we will try values of mtry between 10 and 30. The sample size will go from 60% and 80% of the total size of the training set. We will try minimal node size splits between 5 and 20.\nhyper_grid &lt;- expand.grid(mtry = seq(10, 20, by=2), \n                          sample.fraction = c(0.60, 0.65, 0.70, 0.75, 0.80),\n                          min.node.size = seq(3, 9, by=2))\n\n# total number of hyperparameter combinations\nnrow(hyper_grid)\n\n[1] 120\n\n\nNext, we can loop through the grid and generate, for each hyperparameter combination, random forest models based on 500 trees. For each random forest model, we will add the OOB RMSE error to the grid so we can find what hyperparameter combination minimises this error. Note that we set the value of seed for code reproducibility purposes.\n\nfor(i in 1:nrow(hyper_grid)) {\n  \n  # train model\n  fit_rf_tuning &lt;- ranger(formula = INCOME ~ ., \n    data = df_train, \n    num.trees = 500,\n    mtry = hyper_grid$mtry[i],\n    sample.fraction = hyper_grid$sample.fraction[i],\n    min.node.size = hyper_grid$min.node.size[i],\n    seed = 123)\n  \n  # add OOB error to grid\n  hyper_grid$OOB_RMSE[i] &lt;- sqrt(fit_rf_tuning$prediction.error)\n}\n\nFrom the fitted models, the one that produces the minimum OOB RMSE and hence, the best-performing one, is given by the combination of parameters printed below:\n\nhyper_grid[which.min(hyper_grid$OOB_RMSE),]\n\n   mtry sample.fraction min.node.size OOB_RMSE\n58   16             0.8             5  3203.65\n\n\nmtry= 14, sample.fraction =0.8 and min.node.size=3. The OOB RMSE is 3141.089, slightly lower than the error we obtained with the default model for random forest with no tuning, yay!"
  },
  {
    "objectID": "machine-learning.html#questions",
    "href": "machine-learning.html#questions",
    "title": "10  Machine Learning",
    "section": "10.6 Questions",
    "text": "10.6 Questions\nFor this set of questions, you will use a data set very similar to the one used in the examples above. However, instead of focusing on predicting the net annual household income, you will focus in the median house price paid in each MSOA in 2021. The raw data for median house price can be downloaded here, but we have created a clean dataset for you. You can load the relevant data set by running the code below:\n\n# Load the data\ndf_housing &lt;- read.csv(\"./data/machine-learning/census2021-msoa-houseprice.csv\")\n# Data cleaning, remove the X field\ndf_housing$X &lt;- NULL\n# Data cleaning, the fields \"date\", \"geography\" and \"geography.code\" are not needed\ndf_housing &lt;- subset(df_housing, select = -c(date, geography, geography.code))\n# More data cleaning, remove comma from income and turn it into a numeric value\ndf_housing$HOUSEPRICE &lt;- as.numeric(gsub(\",\", \"\", df_housing$HOUSEPRICE))\n\nFor the following questions, you will use the whole dataset for training and evaluate the performance of the ML models via the OOB error.\n\nTrain a model of decision trees with bagging using caret. Use 10-fold cross-validation and the default number of trees in the training process. Set the seed to 123 so that your results are reproducible. Report the OOB RMSE and the first three most important variables for the decision process in the fitted model using the function varImp(). Comment on why you think these three demographic variables are relatively important to determine the median house price for the MSOAs in England & Wales. What is special about these demographic characteristics? Do not include any plots.\nTrain a random forest model to predict mean house price at the MSOA level using randomForest with the default settings. Without including any plots, report the number of trees that produces the minimum OOB MSE. What would be the associated minimum RMSE? Like before, set the seed to 123 to ensure your results are reproducible. Do you observe any improvements in model performance with respect to the model you fitted in question 1?\nUse ranger to tune the random forest model. Perform the hyperparameter search through a grid that considers only the number of trees and the number of variables to be sampled at each split. For the number of trees, try values from 490 to 500, separated by 1 unit. For the number of variables to be sampled at each split, try values from 10 to 20, also separated by 1 unit. Set the seed to 123. Without including any plots, report the combination of parameter values that leads to the model with the lowest OOB RMSE. Report the value of the OOB RMSE. Do you observe any further improvements? In the context of predicting mean house price, do you think this RMSE is acceptable? Justify your answer.\nUsing your own words, explain what are the advantages of using random forests to predict median house prices instead of multilinear regression. You should include references to the ML literature to support your arguments.\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. New York: Springer.\n\n\nBoehmke, Brad. 2019. “Hands-on Machine Learning with r. Chapter 9: Decision Trees.” https://bradleyboehmke.github.io/HOML/DT.html.\n\n\nBreiman, L. 1984. Classification and Regression Trees (1st Ed.). Routledge."
  },
  {
    "objectID": "data-sets.html#greater-machester-land-use-data",
    "href": "data-sets.html#greater-machester-land-use-data",
    "title": "11  Data sets",
    "section": "11.1 Greater Machester land use data",
    "text": "11.1 Greater Machester land use data\n\nAvailability\nThe dataset is stored on a gpkg file that can be found, within the structure of this project, under:\n\nst_LSOA &lt;- st_read(\"./data/geodemographics/manchester_land_cover_2011.gpkg\")\n\nReading layer `manchester_land_cover_2011' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/geodemographics/manchester_land_cover_2011.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1673 features and 44 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 351662.3 ymin: 381166 xmax: 406087.2 ymax: 421037.7\nProjected CRS: OSGB36 / British National Grid\n\n\n\n\nVariables\nThe variables included in this dataset follow the land use classification of the CORINE Land Cover dataset.\n\n\nSource & Pre-processing\nThe data was sourced from What do ‘left behind’ areas look like over time? and cleaned on Python."
  },
  {
    "objectID": "data-sets.html#british-administrative-boundaries-lsoas-and-las",
    "href": "data-sets.html#british-administrative-boundaries-lsoas-and-las",
    "title": "11  Data sets",
    "section": "11.2 British administrative boundaries (LSOAs and LAs)",
    "text": "11.2 British administrative boundaries (LSOAs and LAs)\n\nAvailability\nThe dataset for the boundaries of the lower-layer super-output areas (LSOAs) within London is stored as a shapefile that can be found under:\n\nst_LSOA &lt;- st_read(\"data/geodemographics-old/LSOA_2011_London_gen_MHW/LSOA_2011_London_gen_MHW.shp\")\n\nReading layer `LSOA_2011_London_gen_MHW' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/geodemographics-old/LSOA_2011_London_gen_MHW/LSOA_2011_London_gen_MHW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4835 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n\nThe dataset for the boundaries of the local authority distrits (LADs) for the UK is stored as a shapefile that can be found under:\n\nLA_UK &lt;- st_read(\"./data/networks/Local_Authority_Districts_(December_2022)_Boundaries_UK_BFC/LAD_DEC_2022_UK_BFC.shp\")\n\nReading layer `LAD_DEC_2022_UK_BFC' from data source \n  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/networks/Local_Authority_Districts_(December_2022)_Boundaries_UK_BFC/LAD_DEC_2022_UK_BFC.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 374 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -116.1928 ymin: 5336.966 xmax: 655653.8 ymax: 1220302\nProjected CRS: OSGB36 / British National Grid\n\n\n\n\nVariables\nFor each of the 4,835 LSOAs, the following characteristics are available:\n\nnames(st_LSOA)\n\n [1] \"LSOA11CD\"  \"LSOA11NM\"  \"MSOA11CD\"  \"MSOA11NM\"  \"LAD11CD\"   \"LAD11NM\"  \n [7] \"RGN11CD\"   \"RGN11NM\"   \"USUALRES\"  \"HHOLDRES\"  \"COMESTRES\" \"POPDEN\"   \n[13] \"HHOLDS\"    \"AVHHOLDSZ\" \"geometry\" \n\n\nwhere:\n\nLSOA11CD: Lower-Layer Super-Output Area code\nLSOA11NM: Lower-Layer Super-Output Area code\nMSOA11CD: Medium-Layer Super-Output Area code\nMSOA11NM: Medium-Layer Super-Output Area code\nLAD11CD: Local Authority District code\nLAD11NM: Local Authority District name\nRGN11CD: Region code\nRGN11NM: Region name\nUSUALRES: Usual residents\nHHOLDRES: Household residents\nCOMESTRES: Communal Establishment residents\nPOPDEN: Population density\nHHOLDS: Number of households\nAVHHOLDSZ: Average household size\ngeometry: Polygon of LSOA\n\nFor each of the 374 LADs, the following characteristics are available:\n\nnames(LA_UK)\n\n [1] \"OBJECTID\"   \"LAD22CD\"    \"LAD22NM\"    \"BNG_E\"      \"BNG_N\"     \n [6] \"LONG\"       \"LAT\"        \"GlobalID\"   \"SHAPE_Leng\" \"SHAPE_Area\"\n[11] \"geometry\"  \n\n\nwhere:\n\nOBJECTID: object identifier\nLAD22CD: Local Authority District code\nLAD22NM: Local Authority District name\nBNG_E: Location Easting\nBNG_N: Location Northing\nLONG: Location Longitude\nLAT: Location Latitude\nGlobalID: Global Identifier\nSHAPE_Leng: Boundary length\nSHAPE_Area: Area within boundary\ngeometry: Polygon of LAD\n\n\n\nProjection\nThe shapes of each LSOA are stored as polygons an expressed in the OSGB36 projection:\n\nst_crs(st_LSOA)\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6277]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.999601272,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nSimilarly, the shapes of each LAD are stored as polygons an expressed in the OSGB36 projection:\n\nst_crs(LA_UK)\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\n\n\nSource & Pre-processing\nThe boundaries for the LSOAs within London can be found directly from the London Datastore website.\nThe boundaries for the LADs for the UK can be found on the ONS Open Geography Portal website. To filter for the London LADs, i.e. the London boroughs, we run the following line of code:\n\nLND_boroughs &lt;- LA_UK %&gt;% filter(grepl('E09', LAD22CD))"
  },
  {
    "objectID": "data-sets.html#worldpop-population-count-data-for-ukraine",
    "href": "data-sets.html#worldpop-population-count-data-for-ukraine",
    "title": "11  Data sets",
    "section": "11.3 Worldpop population count data for Ukraine",
    "text": "11.3 Worldpop population count data for Ukraine"
  },
  {
    "objectID": "data-sets.html#census-population-count-data-for-uk",
    "href": "data-sets.html#census-population-count-data-for-uk",
    "title": "11  Data sets",
    "section": "11.4 Census population count data for UK",
    "text": "11.4 Census population count data for UK"
  },
  {
    "objectID": "data-sets.html#ukraines-administrative-boundaries",
    "href": "data-sets.html#ukraines-administrative-boundaries",
    "title": "11  Data sets",
    "section": "11.5 Ukraine’s administrative boundaries",
    "text": "11.5 Ukraine’s administrative boundaries"
  },
  {
    "objectID": "data-sets.html#internal-migration-flows-between-us-metropolitan-areas-and-between-london-boroughs",
    "href": "data-sets.html#internal-migration-flows-between-us-metropolitan-areas-and-between-london-boroughs",
    "title": "11  Data sets",
    "section": "11.6 Internal migration flows between US metropolitan areas and between London boroughs",
    "text": "11.6 Internal migration flows between US metropolitan areas and between London boroughs\n\nAvailability\nThe dataset for the migration flows between US metropolitan areas can be found as a csv file under:\n\ndf_metro &lt;- read.csv(\"./data/networks/metro_to_metro_2015_2019_US_migration.csv\")\n\nThe dataset for the migration flows between London boroughs can be found as a csv file under:\n\ndf_borough &lt;- read.csv(\"./data/networks/LA_to_LA_2019_London_clean.csv\")\n\n\n\nVariables\nFor each of the 52,930 movements recorded on the dataset for the migration flows between US metropolitan areas, the following fields are available:\n\nnames(df_metro)\n\n [1] \"MSA_Current_Code\"                                \n [2] \"MSA_Current_Name\"                                \n [3] \"MSA_Current_State\"                               \n [4] \"MSA_Current_Population_1_Year_and_Over_Estimate\" \n [5] \"MSA_Current_Population_1_Year_and_Over_MOE\"      \n [6] \"MSA_Previous_Code\"                               \n [7] \"MSA_Previous_Name\"                               \n [8] \"MSA_Previous_State\"                              \n [9] \"MSA_Previous_Population_1_Year_and_Over_Estimate\"\n[10] \"MSA_Previous_Population_1_Year_and_Over_MOE\"     \n[11] \"Movers_Metro_to_Metro_Flow_Estimate\"             \n[12] \"Movers_Metro_to_Metro_Flow_MOE\"                  \n\n\nAll the fields that start with MSA_Current_ or MSA_Previous_ refer to the characteristics of the origin and destination metropolican areas. The relevant fields for the analysis in this book are:\n\nMovers_Metro_to_Metro_Flow_Estimate: Estimate of number of people moving between origin and destination\nMovers_Metro_to_Metro_Flow_MOE: Margin of error for the above estimate\n\nMore details on the methodology to obtain the estimates and the margin of error for each population movement can be found on the US Census Bureau website.\nFor each of the 1,053 movements recorded on the dataset for the migration flows between London boroughs, the following fields are available:\n\nnames(df_borough)\n\n[1] \"OutLA\" \"InLA\"  \"Moves\"\n\n\nwhere:\n\nOutLA is the code corresponding to the origin borough\nInLA is the code corresponding to the destination borough\nMoves is the number of internal migration moves within each flow. Note that the numbers are not integers. This is because of the various scaling processes used to produce the dataset, which are described in more detail in the latest methodology document, which can be found here.\n\n\n\nSource & pre-processing\nThe dataset for the migration flows between US metropolitan areas can be downloaded from the US Census Bureau website. The data was cleaned on Microsoft Excel.\nThe dataset for the migration flows between London boroughs can be downloaded from the ONS website. The data was cleaned on Microsoft Excel."
  },
  {
    "objectID": "data-sets.html#twitter-data-on-public-opinion-originated-in-the-us-and-in-the-uk",
    "href": "data-sets.html#twitter-data-on-public-opinion-originated-in-the-us-and-in-the-uk",
    "title": "11  Data sets",
    "section": "11.7 Twitter data on public opinion originated in the US and in the UK",
    "text": "11.7 Twitter data on public opinion originated in the US and in the UK"
  },
  {
    "objectID": "data-sets.html#reddit-data",
    "href": "data-sets.html#reddit-data",
    "title": "11  Data sets",
    "section": "11.8 Reddit data",
    "text": "11.8 Reddit data"
  },
  {
    "objectID": "data-sets.html#google-mobility-data-for-italy-and-the-uk",
    "href": "data-sets.html#google-mobility-data-for-italy-and-the-uk",
    "title": "11  Data sets",
    "section": "11.9 Google mobility data for Italy and the UK",
    "text": "11.9 Google mobility data for Italy and the UK"
  },
  {
    "objectID": "data-sets.html#covid-19-cases-data-for-london-and-rome",
    "href": "data-sets.html#covid-19-cases-data-for-london-and-rome",
    "title": "11  Data sets",
    "section": "11.10 COVID-19 cases data for London and Rome",
    "text": "11.10 COVID-19 cases data for London and Rome"
  },
  {
    "objectID": "data-sets.html#census-msoa-data-for-england-and-wales",
    "href": "data-sets.html#census-msoa-data-for-england-and-wales",
    "title": "11  Data sets",
    "section": "11.11 Census MSOA data for England and Wales",
    "text": "11.11 Census MSOA data for England and Wales\n\nAvailability\nThe dataset for the demographic census data of each MSOA in England and Wales can be loaded as a csv file from:\n\ndf_MSOA &lt;- read.csv(\"./data/machine-learning/census2021-msoa-income.csv\")\n\nA very similar dataset for the demographic census data of each MSOA in England and Wales which also contains data on the median house price can be loaded as a csv file from:\n\ndf_housing &lt;- read.csv(\"./data/machine-learning/census2021-msoa-houseprice.csv\")\n\n\n\nVariables\nFor each of the 7,080 MSOAs recorded in England and Wales, the following fields are available:\n\nnames(df_MSOA)\n\n [1] \"X\"                \"date\"             \"geography\"        \"geography.code\"  \n [5] \"inHH\"             \"inCE\"             \"SING\"             \"MARRIED\"         \n [9] \"SEP\"              \"DIV\"              \"WIDOW\"            \"UK\"              \n[13] \"EU\"               \"AFR\"              \"AS\"               \"AM\"              \n[17] \"OC\"               \"BO\"               \"DENSITY\"          \"Y14orUNDER\"      \n[21] \"Y15to19\"          \"Y20to24\"          \"Y25to29\"          \"Y30to34\"         \n[25] \"Y35to49\"          \"Y40to44\"          \"Y45to49\"          \"Y50to54\"         \n[29] \"Y55to59\"          \"Y60to64\"          \"Y65orOVER\"        \"F\"               \n[33] \"M\"                \"HH1\"              \"HH2\"              \"HH3\"             \n[37] \"HH4\"              \"HH5\"              \"HH6\"              \"ADD1YagoSAME\"    \n[41] \"ADD1YagoSTUDENT\"  \"ADD1YagoUK\"       \"ADD1YagoNONUK\"    \"NHH\"             \n[45] \"OWN\"              \"MORTGAGE\"         \"SHAREDOWN\"        \"RENTfromCOUNCIL\" \n[49] \"RENTotherSOCIAL\"  \"RENTprivate\"      \"RENTprivateOTHER\" \"RENTfree\"        \n[53] \"INCOME\"          \n\n\nFor a description of the variables in the columns of df_MSOA, we can load a dictionary for these variables:\n\ndf_dictionary &lt;- read.csv(\"./data/machine-learning/Dictionary.csv\")\nhead(df_dictionary)\n\n                                      Dictionary       X\n1                                                       \n2                                           Name     Key\n3                 Lives in household (% persons)    inHH\n4    Lives in communal establishment (% persons)    inCE\n5 Never married or civil partnership (% persons)    SING\n6    Married or in civil partnership (% persons) MARRIED\n\n\n\n\nSource & pre-processing\nData on the the census characteristics for different MSOAs can be downloaded from the Nomis website. Data on the average net household income can be obtained from the ONS website.\nData on the median houseprice for different MSOAs can be downloaded from the ONS website.\nAll the data has been pre-processed on Microsoft Excel."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abbott, Andrew, and Angela Tsay. 2000. “Sequence Analysis and\nOptimal Matching Methods in Sociology: Review and Prospect.”\nSociological Methods & Research 29 (1): 3–33.\n\n\nArribas-Bel, Dani, Mark Green, Francisco Rowe, and Alex Singleton. 2021.\n“Open Data Products-A Framework for Creating Valuable Analysis\nReady Data.” Journal of Geographical Systems 23 (4):\n497–514. https://doi.org/10.1007/s10109-021-00363-5.\n\n\nBackman, Mikaela, Esteban Lopez, and Francisco Rowe. 2020. “The\nOccupational Trajectories and Outcomes of Forced Migrants in Sweden.\nEntrepreneurship, Employment or Persistent Inactivity?” Small\nBusiness Economics 56 (3): 963–83. https://doi.org/10.1007/s11187-019-00312-z.\n\n\nBail, Christopher A., Lisa P. Argyle, Taylor W. Brown, John P. Bumpus,\nHaohan Chen, M. B. Fallin Hunzaker, Jaemin Lee, Marcus Mann, Friedolin\nMerhout, and Alexander Volfovsky. 2018. “Exposure to Opposing\nViews on Social Media Can Increase Political Polarization.”\nProceedings of the National Academy of Sciences 115 (37):\n9216–21. https://doi.org/10.1073/pnas.1804840115.\n\n\nBar, Michael, Moshe Hazan, Oksana Leukhina, David Weiss, and Hosny\nZoabi. 2018. “Why Did Rich Families Increase Their Fertility?\nInequality and Marketization of Child Care.” Journal of\nEconomic Growth 23: 427–63.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Information Science and Statistics. New York: Springer.\n\n\nBlei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent\nDirichlet Allocation.” Journal of Machine Learning\nResearch 3 (Jan): 993–1022.\n\n\nBoehmke, Brad. 2019. “Hands-on Machine Learning with r. Chapter 9:\nDecision Trees.” https://bradleyboehmke.github.io/HOML/DT.html.\n\n\nBreiman, L. 1984. Classification and Regression\nTrees (1st Ed.). Routledge.\n\n\nBrodeur, Abel, Andrew E Clark, Sarah Fleche, and Nattavudh Powdthavee.\n2021. “COVID-19, Lockdowns and Well-Being: Evidence from Google\nTrends.” Journal of Public Economics 193: 104346.\n\n\nCabrera-Arnau, Carmen, Chen Zhong, Michael Batty, Ricardo Silva, and\nSoong Moon Kang. 2022. “Inferring Urban Polycentricity from the\nVariability in Human Mobility Patterns.” arXiv. https://doi.org/10.48550/ARXIV.2212.03973.\n\n\nCadwalladr, Carole, and Emma Graham-Harrison. 2018. “Revealed: 50\nMillion Facebook Profiles Harvested for Cambridge Analytica in Major\nData Breach.” The Guardian 17 (1): 22.\n\n\nCesare, Nina, Hedwig Lee, Tyler McCormick, Emma Spiro, and Emilio\nZagheni. 2018. “Promises and Pitfalls of Using Digital Traces for\nDemographic Research.” Demography 55 (5): 1979–99. https://doi.org/10.1007/s13524-018-0715-2.\n\n\nCheong, Pauline Hope, Rosalind Edwards, Harry Goulbourne, and John\nSolomos. 2007. “Immigration, Social Cohesion and Social Capital: A\nCritical Review.” Critical Social Policy 27 (1): 24–49.\nhttps://doi.org/10.1177/0261018307072206.\n\n\nCinelli, Matteo, Walter Quattrociocchi, Alessandro Galeazzi, Carlo\nMichele Valensise, Emanuele Brugnoli, Ana Lucia Schmidt, Paola Zola,\nFabiana Zollo, and Antonio Scala. 2020. “The COVID-19 Social Media\nInfodemic.” Scientific Reports 10 (1): 1–10.\n\n\nCowper, Andy. 2020. “Covid-19: Are We Getting the Communications\nRight?” BMJ, March, m919. https://doi.org/10.1136/bmj.m919.\n\n\nDolega, Les, Francisco Rowe, and Emma Branagan. 2021. “Going\nDigital? The Impact of Social Media Marketing on Retail Website Traffic,\nOrders and Sales.” Journal of Retailing and Consumer\nServices 60 (May): 102501. https://doi.org/10.1016/j.jretconser.2021.102501.\n\n\nElbagir, Shihab, and Jing Yang. 2020. “Sentiment Analysis on\nTwitter with Python’s Natural Language Toolkit and VADER\nSentiment Analyzer.” IAENG Transactions on Engineering\nSciences, January. https://doi.org/10.1142/9789811215094_0005.\n\n\nEuropean Commision. 2019. “10 Trends Shaping\nMigration.” https://op.europa.eu/en/publication-detail/-/publication/aa25fb8f-10cc-11ea-8c1f-01aa75ed71a1.\n\n\nFielding, A. J. 1992. “Migration and Social Mobility: South East\nEngland as an Escalator Region.” Regional Studies 26\n(1): 1–15. https://doi.org/10.1080/00343409212331346741.\n\n\nFranklin, Rachel. 2022. “Quantitative Methods II: Big\nTheory.” Progress in Human Geography 47 (1): 178–86. https://doi.org/10.1177/03091325221137334.\n\n\nGabadinho, Alexis, Gilbert Ritschard, Nicolas S. Müller, and Matthias\nStuder. 2011. “Analyzing and Visualizing State Sequences\ninRwithTraMineR.”\nJournal of Statistical Software 40 (4). https://doi.org/10.18637/jss.v040.i04.\n\n\nGabadinho, Alexis, Gilbert Ritschard, Matthias Studer, and Nicolas S\nMüller. 2009. “Mining Sequence Data in r with the TraMineR\nPackage: A User’s Guide.” Geneva: Department of Econometrics\nand Laboratory of Demography, University of Geneva.\n\n\nGhani, Norjihan Abdul, Suraya Hamid, Ibrahim Abaker Targio Hashem, and\nEjaz Ahmed. 2019. “Social Media Big Data Analytics: A\nSurvey.” Computers in Human Behavior 101 (December):\n417–28. https://doi.org/10.1016/j.chb.2018.08.039.\n\n\nGonzález-Leonardo, Miguel, Niall Newsham, and Francisco Rowe. 2023.\n“Understanding Population Decline Trajectories in Spain Using\nSequence Analysis.” Geographical Analysis, January. https://doi.org/10.1111/gean.12357.\n\n\nGoodman-Bacon, Andrew, and Jan Marcus. 2020. “Using\nDifference-in-Differences to Identify Causal Effects of COVID-19\nPolicies.”\n\n\nGreen, Mark, Frances Darlington Pollock, and Francisco Rowe. 2021.\n“New Forms of Data and New Forms of Opportunities to Monitor and\nTackle a Pandemic.” In, 423–29. Springer International\nPublishing. https://doi.org/10.1007/978-3-030-70179-6_56.\n\n\nHilbert, Martin, and Priscila López. 2011. “The\nWorld’s Technological Capacity to Store, Communicate, and\nCompute Information.” Science 332 (6025): 60–65. https://doi.org/10.1126/science.1200970.\n\n\nHome Affairs Committee. 2020. “Oral evidence:\nHome Office preparedness for Covid-19 (Coronavirus), HC\n232.” London: House of Commons. https://committees.parliament.uk/oralevidence/359/default/.\n\n\nHutto, C., and Eric Gilbert. 2014. “VADER: A Parsimonious\nRule-Based Model for Sentiment Analysis of Social Media Text.”\nProceedings of the International AAAI Conference on Web and Social\nMedia 8 (1): 216–25. https://doi.org/10.1609/icwsm.v8i1.14550.\n\n\nJoint Research Centre. 2022. Data innovation in demography,\nmigration and human mobility. LU: European Commission. Publications\nOffice. https://doi.org/10.2760/027157.\n\n\nKashyap, Ridhi, R. Gordon Rinderknecht, Aliakbar Akbaritabar, Diego\nAlburez-Gutierrez, Sofia Gil-Clavel, André Grow, Jisu Kim, et al. 2022.\n“Digital and Computational Demography.” http://dx.doi.org/10.31235/osf.io/7bvpt.\n\n\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in\nData: An Introduction to Cluster Analysis. John Wiley & Sons.\n\n\nKitchin, Rob. 2014. “Big Data, New Epistemologies and Paradigm\nShifts.” Big Data & Society 1 (1): 205395171452848.\nhttps://doi.org/10.1177/2053951714528481.\n\n\nLazer, David, Alex Pentland, Lada Adamic, Sinan Aral, Albert-László\nBarabási, Devon Brewer, Nicholas Christakis, et al. 2009.\n“Computational Social Science.” Science 323\n(5915): 721–23. https://doi.org/10.1126/science.1167742.\n\n\nLazer, David, Alex Pentland, Duncan J. Watts, Sinan Aral, Susan Athey,\nNoshir Contractor, Deen Freelon, et al. 2020. “Computational\nSocial Science: Obstacles and Opportunities.” Science\n369 (6507): 1060–62. https://doi.org/10.1126/science.aaz8170.\n\n\nLiang, Hai, and King-wa Fu. 2015. “Testing Propositions Derived\nfrom Twitter Studies: Generalization and Replication in Computational\nSocial Science.” Edited by Zi-Ke Zhang. PLOS ONE 10 (8):\ne0134270. https://doi.org/10.1371/journal.pone.0134270.\n\n\nMarshall, Emily A. 2013. “Defining Population Problems: Using\nTopic Models for Cross-National Comparison of Disciplinary\nDevelopment.” Poetics 41 (6): 701–24.\n\n\nNewman, Mark. 2018. Networks / Mark Newman. Second edition.\nOxford: Oxford University Press.\n\n\nNewsham, Niall, and Francisco Rowe. 2022a. “Understanding the\nTrajectories of Population Decline Across Rural and Urban Europe: A\nSequence Analysis.” https://doi.org/10.48550/ARXIV.2203.09798.\n\n\n———. 2022b. “Understanding Trajectories of Population Decline\nAcross Rural and Urban Europe: A Sequence Analysis.”\nPopulation, Space and Place, December. https://doi.org/10.1002/psp.2630.\n\n\nOgnyanova, K. 2016. “Network Analysis with r and Igraph: NetSci x\nTutorial.” www.kateto.net/networks-r-igraph.\n\n\nPatias, Nikos, Francisco Rowe, and Dani Arribas-Bel. 2021.\n“Trajectories of Neighbourhood Inequality in Britain: Unpacking\nInter-Regional Socioeconomic Imbalances,\n1971-2011.” The Geographical Journal 188\n(2): 150–65. https://doi.org/10.1111/geoj.12420.\n\n\nPatias, Nikos, Francisco Rowe, Stefano Cavazzi, and Dani Arribas-Bel.\n2021. “Sustainable Urban Development Indicators in Great Britain\nfrom 2001 to 2016.” Landscape and Urban Planning 214\n(October): 104148. https://doi.org/10.1016/j.landurbplan.2021.104148.\n\n\nPetti, Samantha, and Abraham Flaxman. 2020. “Differential Privacy\nin the 2020 US Census: What Will It Do? Quantifying the Accuracy/Privacy\nTradeoff.” Gates Open Research 3 (April): 1722. https://doi.org/10.12688/gatesopenres.13089.2.\n\n\nPrieto Curiel, Rafael, Carmen Cabrera-Arnau, and Steven Richard Bishop.\n2022. “Scaling Beyond Cities.” Frontiers in\nPhysics 10. https://doi.org/10.3389/fphy.2022.858307.\n\n\nRosa, H., N. Pereira, R. Ribeiro, P. C. Ferreira, J. P. Carvalho, S.\nOliveira, L. Coheur, P. Paulino, A. M. Veiga Simão, and I. Trancoso.\n2019. “Automatic Cyberbullying Detection: A Systematic\nReview.” Computers in Human Behavior 93 (April): 333–45.\nhttps://doi.org/10.1016/j.chb.2018.12.021.\n\n\nRowe, Francisco. 2021a. “Using Twitter Data to Monitor Immigration\nSentiment.” http://dx.doi.org/10.31219/osf.io/sf7u4.\n\n\n———. 2021b. “Big Data and Human Geography.” http://dx.doi.org/10.31235/osf.io/phz3e.\n\n\n———. 2022a. “Introduction to Geographic Data Science.”\nOpen Science Framework, August. https://doi.org/10.17605/OSF.IO/VHY2P.\n\n\n———. 2022b. “Using Digital Footprint Data to Monitor Human\nMobility and Support Rapid Humanitarian Responses.” Regional\nStudies, Regional Science 9 (1): 665–68. https://doi.org/10.1080/21681376.2022.2135458.\n\n\nRowe, Francisco, and Dani Arribas-Bel. 2022. “Spatial Modelling\nfor Data Scientists.” Open Science Framework, August. https://doi.org/10.17605/OSF.IO/8F6XR.\n\n\nRowe, Francisco, Jonathan Corcoran, and Martin Bell. 2016. “The\nReturns to Migration and Human Capital Accumulation Pathways:\nNon-Metropolitan Youth in the School-to-Work Transition.” The\nAnnals of Regional Science 59 (3): 819–45. https://doi.org/10.1007/s00168-016-0771-8.\n\n\nRowe, Francisco, Michael Mahony, Eduardo Graells-Garrido, Marzia Rango,\nand Niklas Sievers. 2021. “Using Twitter to Track Immigration\nSentiment During Early Stages of the COVID-19 Pandemic.” Data\n& Policy 3. https://doi.org/10.1017/dap.2021.38.\n\n\nRowe, Francisco, Michael Mahony, Niklas Sievers, Marzia Rango, and\nEduardo Graells-Garrido. 2021. “Sentiment\ntowards Migration during COVID-19. What Twitter Data Can Tell\nUs.” IOM Publications.\n\n\nRowe, Francisco, Ruth Neville, and Miguel González-Leonardo. 2022.\n“Sensing Population Displacement from Ukraine Using Facebook Data:\nPotential Impacts and Settlement Areas.” http://dx.doi.org/10.31219/osf.io/7n6wm.\n\n\nSalmela-Aro, Katariina, Noona Kiuru, Jari-Erik Nurmi, and Mervi Eerola.\n2011. “Mapping Pathways to Adulthood Among Finnish University\nStudents: Sequences, Patterns, Variations in Family- and Work-Related\nRoles.” Advances in Life Course Research 16 (1): 25–41.\nhttps://doi.org/10.1016/j.alcr.2011.01.003.\n\n\nSchleicher, Andreas. 2020. “The Impact of COVID-19 on Education:\nInsights from\" Education at a Glance 2020\".” OECD\nPublishing.\n\n\nSielge, Jullia, and David Robinson. 2022. Welcome to Text Mining\nwith r. O’Reilly. https://www.tidytextmining.com.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2019. “Geographic Data\nScience.” Geographical Analysis 53 (1): 61–75. https://doi.org/10.1111/gean.12194.\n\n\n“Stop the Coronavirus Stigma Now.” 2020. Nature\n580 (7802): 165–65. https://doi.org/10.1038/d41586-020-01009-0.\n\n\nTatem, Andrew J. 2017. “WorldPop, Open Data for Spatial\nDemography.” Scientific Data 4 (1). https://doi.org/10.1038/sdata.2017.4.\n\n\nTurok, Ivan, and Vlad Mykhnenko. 2007. “The Trajectories of\nEuropean Cities, 19602005.” Cities 24 (3):\n165–82. https://doi.org/10.1016/j.cities.2007.01.007.\n\n\nUgolini, Francesca, Luciano Massetti, Pedro Calaza-Martı́nez, Paloma\nCariñanos, Cynnamon Dobbs, Silvija Krajter Ostoić, Ana Marija Marin, et\nal. 2020. “Effects of the COVID-19 Pandemic on the Use and\nPerceptions of Urban Green Space: An International Exploratory\nStudy.” Urban Forestry & Urban Greening 56: 126888.\n\n\nZagheni, Emilio, and Ingmar Weber. 2015. “Demographic Research\nwith Non-Representative Internet Data.” Edited by Nikolaos\nAskitas and Professor Professor Klaus F. Zimmermann. International\nJournal of Manpower 36 (1): 13–25. https://doi.org/10.1108/ijm-12-2014-0261.\n\n\nZhou, Muzhi, and Man-Yee Kan. 2021. “The Varying Impacts of\nCOVID-19 and Its Related Measures in the UK: A Year in Review.”\nPLoS One 16 (9): e0257286."
  },
  {
    "objectID": "network.html",
    "href": "network.html",
    "title": "5  Network Analysis",
    "section": "",
    "text": "5.1 Dependencies\nTo run the code in the rest of this workbook, we will need to load the following R packages:\n#Support for simple features, a standardised way to encode spatial vector data\nlibrary(sf)\n#Data manipulation\nlibrary(dplyr)\n# An R package for network manipulation and analysis\nlibrary(igraph)\n# Provides a number of useful functions for working with character strings in R\nlibrary(stringr)\n# for data visualization\nlibrary(ggplot2)   \n# for graph visualization\nlibrary(ggraph)    \n# for tidy data handling with graphs\nlibrary(tidygraph) \n# for geospatial visualization\nlibrary(ggspatial)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Network Analysis</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Overview",
    "section": "",
    "text": "1.1 Aims\nThis module aims to:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "topic-modelling.html",
    "href": "topic-modelling.html",
    "title": "7  Topic Modelling",
    "section": "",
    "text": "7.1 Dependencies\nAs shown in the Figure below by we can use tidy text principles to approach topic modeling with the same set of tidy tools used for other data analysis in R. In this chapter, we’ll learn to work with LDA objects from the topicmodels package, tidying such models so that they can be analysed with the help of ggplot2 and dplyr.\nWe use the libraries below.\n#Topic models package that allows tidying such models with ggplot2 and dplyr\nlibrary(topicmodels)\n#A framework for text mining applications within R.\nlibrary(tm)\n#The Life-Changing Magic of Tidying Text\nlibrary(tidytext)\n# Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library\nlibrary(SnowballC)\n# Data manipulation\nlibrary(tidyverse)\n#Create Elegant Data Visualisations Using the Grammar of Graphics\nlibrary(ggplot2)\nlibrary(ggthemes)\n# Reddit Data Extraction Toolkit\nlibrary(RedditExtractoR)\n# Flexibly Reshape Data\nlibrary(reshape2)\n# Estimation of Structural Topic Models\nlibrary (stm)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Topic Modelling</span>"
    ]
  },
  {
    "objectID": "longitudinal-1.html",
    "href": "longitudinal-1.html",
    "title": "8  Modelling Time",
    "section": "",
    "text": "8.1 Dependencies\n# Data\nlibrary(sf)\nlibrary(readr)\nlibrary(tidyverse)\n\n# Dates and Times\nlibrary(lubridate)\n\n# Regression Spline Functions and Classes\nlibrary(splines)\n\n# graphs\nlibrary(ggplot2)\nlibrary(dygraphs)\nlibrary(plotly)\nlibrary(ggthemes)\nlibrary(ggpmisc)\nlibrary(gridExtra)\nlibrary(viridis)\nlibrary(ggformula)\nlibrary(ggimage)\nlibrary(grid)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling Time</span>"
    ]
  }
]