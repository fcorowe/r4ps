% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{soul}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{siunitx}

  \newcolumntype{d}{S[
    input-open-uncertainty=,
    input-close-uncertainty=,
    parse-numbers = false,
    table-align-text-pre=false,
    table-align-text-post=false
  ]}
  
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Population Science},
  pdfauthor={Francisco Rowe, Carmen Cabrera-Arnau, Elisabetta Pietrostefani},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Population Science}
\author{Francisco Rowe, Carmen Cabrera-Arnau, Elisabetta Pietrostefani}
\date{2024-01-30}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[borderline west={3pt}{0pt}{shadecolor}, sharp corners, frame hidden, boxrule=0pt, enhanced, breakable, interior hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{welcome}{%
\chapter*{Welcome}\label{welcome}}
\addcontentsline{toc}{chapter}{Welcome}

\markboth{Welcome}{Welcome}

This is the website for ``Population Science''. This is a course
designed and delivered by Dr Carmen Cabrera-Arnau, Prof Francisco Rowe
and Dr Elisabetta Pietrostefani from the Geographic Data Science Lab at
the University of Liverpool, United Kingdom. You will learn applied
tools and cutting-edge analytical approaches to use digital footprint
data to explore and understand human population trends and patterns,
including supervised and unsupervised machine learning approaches,
network analysis and causal inference methods.

The website is \emph{free to use} and is licensed under the
\href{https://creativecommons.org/licenses/by-nc-nd/4.0/}{Attribution-NonCommercial-NoDerivatives
4.0 International}. A compilation of this web course is hosted as a
GitHub repository that you can access:

\begin{itemize}
\tightlist
\item
  As a
  \href{https://github.com/fcorowe/r4ps/archive/refs/heads/main.zip}{download}
  of a \texttt{.zip} file that contains all the materials.
\item
  As an \href{https://www.population-science.net}{html website}.
\item
  As a
  \href{https://www.population-science.net/Population-Science.pdf}{pdf
  document}
\item
  As a \href{https://github.com/fcorowe/r4ps}{GitHub repository}.
\end{itemize}

\hypertarget{contact}{%
\section*{Contact}\label{contact}}
\addcontentsline{toc}{section}{Contact}

\markright{Contact}

\begin{itemize}
\item
  \textbf{Module lead:} \textbf{\emph{Dr Carmen Cabrera-Arnau}} -
  c.cabrera-arnau {[}at{]} liverpool.ac.uk - Lecturer in Geographic Data
  Science
\item
  \emph{Prof Francisco Rowe} - f.rowe-gonzalez {[}at{]} liverpool.ac.uk
  - Professor in Population Data Science
\item
  \emph{Dr Elisabetta Pietrostefani} - e.pietrostefani {[}at{]}
  liverpool.ac.uk - Lecturer in Geographic Data Science

  Find us in Roxby Building, University of Liverpool, UK
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{overview}{%
\chapter{Overview}\label{overview}}

The module provides students with an introduction to the use of data
science and digital footprint data to analyse human population dynamics.
Established approaches to study population dynamics rely on traditional
data sources, such as censuses and surveys. Digital footprint data have
emerged as a novel source of information providing an opportunity to
understand key societal population issues at an unprecedented temporal
and spatial granularity at scale (Rowe 2021b). Yet, these data represent
major methodological challenges to traditional demographic approaches
(Rowe 2021b). Machine learning, artificial intelligence and data science
approaches are needed to overcome these methodological challenges.

\hypertarget{aims}{%
\section{Aims}\label{aims}}

This module aims to:

\begin{itemize}
\item
  provide an introduction to fundamental theories of population science;
\item
  introduce students to novel data and approaches to understanding
  population dynamics and societal change; and,
\item
  equip students with skills and experience to conduct population
  science using computational, data science approaches.
\end{itemize}

\hypertarget{learning-outcomes}{%
\section{Learning Outcomes}\label{learning-outcomes}}

By the end of the module, students should be able to:

\begin{itemize}
\item
  gain an appreciation of relevant demographic theory to help interpret
  patterns of population change;
\item
  develop an understanding of the types of demographic and social
  science methods that are essential for interpreting and analysing
  digital footprint data in the context of population dynamics;
\item
  develop the ability to apply different methods to understand
  population dynamics and societal change;
\item
  gain an appreciation of how population science approaches can produce
  relevant evidence to inform policy debates;
\item
  develop critical awareness of modern demographic analysis and ethical
  considerations in the use of digital footprint data.
\end{itemize}

\hypertarget{feedback}{%
\section{Feedback}\label{feedback}}

\emph{Formal assessment of two computational essays}. Written
assignment-specific feedback will be provided within three working weeks
of the submission deadline. Comments will offer an understanding of the
mark awarded and identify areas which can be considered for improvement
in future assignments.

\emph{Verbal face-to-face feedback}. Immediate face-to-face feedback
will be provided during computer, discussion and clinic sessions in
interaction with staff. This will take place in all live sessions during
the semester.

\emph{Online forum}. Asynchronous written feedback will be provided via
an online forum. Students are encouraged to contribute by asking and
answering questions relating to the module content. Staff will monitor
the forum Monday to Friday 9am-5pm, but it will be open to students to
make contributions at all times. Response time will vary depending on
the complexity of the question and staff availability.

\hypertarget{computational-environment}{%
\section{Computational Environment}\label{computational-environment}}

To reproduce the code in the book, you need the following software
packages:

\begin{itemize}
\tightlist
\item
  R-4.3.2
\item
  RStudio 2023.12.0-369
\item
  Quarto 1.4.543
\item
  the list of libraries in the next section
\end{itemize}

To check your version of:

\begin{itemize}
\tightlist
\item
  R and libraries run \texttt{sessionInfo()}
\item
  RStudio click \texttt{help} on the menu bar and then \texttt{About}
\item
  Quarto check the \texttt{version} file in the quarto folder on your
  computer.
\end{itemize}

To install and update:

\begin{itemize}
\tightlist
\item
  R, download the appropriate version from
  \href{https://cran.r-project.org}{The Comprehensive R Archive Network
  (CRAN)}
\item
  RStudio, download the appropriate version from
  \href{https://posit.co/download/rstudio-desktop/}{Posit}
\item
  Quarto, download the appropriate version from
  \href{https://quarto.org/docs/get-started/}{the Quarto website}
\end{itemize}

\hypertarget{list-of-libraries}{%
\subsection{List of libraries}\label{list-of-libraries}}

The list of libraries used in this book is provided below:

\begin{itemize}
\tightlist
\item
  ``tidyverse'',
\item
  ``viridis''
\item
  ``viridisLite''
\item
  ``ggthemes''
\item
  ``patchwork''
\item
  ``showtext''
\item
  ``RColorBrewer''
\item
  ``lubridate''
\item
  ``tmap''
\item
  ``sjPlot''
\item
  ``sf''
\item
  ``sp''
\item
  ``kableExtra''
\item
  ``ggcorrplot''
\item
  ``plotrix''
\item
  ``cluster''
\item
  ``factoextra''
\item
  ``igraph''
\item
  ``stringr''
\item
  ``rpart''
\item
  ``rpart.plot''
\item
  ``ggplot2''
\item
  ``Metrics''
\item
  ``caret''
\item
  ``randomForest''
\item
  ``ranger''
\item
  ``wpgpDownloadR''
\item
  ``devtools''
\item
  ``ggseqplot''
\item
  ``tidytext''
\item
  ``tm''
\item
  ``textdata''
\item
  ``topicmodels''
\item
  ``RedditExtractoR''
\item
  ``stm''
\item
  ``dygraphs''
\item
  ``plotly''
\item
  ``ggpmisc''
\item
  ``ggformula''
\item
  ``ggimage''
\item
  ``modelsummary''
\item
  ``gtools''
\item
  ``webshot''
\item
  ``gridExtra''
\item
  ``broom''
\item
  ``rtweet''
\end{itemize}

You need to ensure you have installed the list of libraries used in this
book, running the following code:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# package names}
\NormalTok{packages }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{( }\StringTok{"tidyverse"}\NormalTok{, }\StringTok{"viridis"}\NormalTok{, }\StringTok{"viridisLite"}\NormalTok{, }\StringTok{"ggthemes"}\NormalTok{, }\StringTok{"patchwork"}\NormalTok{, }\StringTok{"showtext"}\NormalTok{, }\StringTok{"RColorBrewer"}\NormalTok{, }\StringTok{"lubridate"}\NormalTok{, }\StringTok{"tmap"}\NormalTok{, }\StringTok{"sjPlot"}\NormalTok{, }\StringTok{"sf"}\NormalTok{, }\StringTok{"sp"}\NormalTok{, }\StringTok{"kableExtra"}\NormalTok{, }\StringTok{"ggcorrplot"}\NormalTok{, }\StringTok{"plotrix"}\NormalTok{, }\StringTok{"cluster"}\NormalTok{, }\StringTok{"factoextra"}\NormalTok{, }\StringTok{"igraph"}\NormalTok{, }\StringTok{"stringr"}\NormalTok{, }\StringTok{"rpart"}\NormalTok{, }\StringTok{"rpart.plot"}\NormalTok{, }\StringTok{"ggplot2"}\NormalTok{, }\StringTok{"Metrics"}\NormalTok{, }\StringTok{"caret"}\NormalTok{, }\StringTok{"randomForest"}\NormalTok{, }\StringTok{"ranger"}\NormalTok{, }\StringTok{"devtools"}\NormalTok{, }\StringTok{"vader"}\NormalTok{, }\StringTok{"wpgpDownloadR"}\NormalTok{, }\StringTok{"ggseqplot"}\NormalTok{, }\StringTok{"tidytext"}\NormalTok{, }\StringTok{"tm"}\NormalTok{, }\StringTok{"textdata"}\NormalTok{, }\StringTok{"topicmodels"}\NormalTok{, }\StringTok{"RedditExtractoR"}\NormalTok{, }\StringTok{"stm"}\NormalTok{, }\StringTok{"dygraphs"}\NormalTok{, }\StringTok{"plotly"}\NormalTok{, }\StringTok{"ggpmisc"}\NormalTok{, }\StringTok{"ggformula"}\NormalTok{, }\StringTok{"ggimage"}\NormalTok{, }\StringTok{"modelsummary"}\NormalTok{, }\StringTok{"gtools"}\NormalTok{, }\StringTok{"gridExtra"}\NormalTok{, }\StringTok{"broom"}\NormalTok{, }\StringTok{"rtweet"}\NormalTok{)}

\CommentTok{\# install packages not yet installed}
\NormalTok{installed\_packages }\OtherTok{\textless{}{-}}\NormalTok{ packages }\SpecialCharTok{\%in\%} \FunctionTok{rownames}\NormalTok{(}\FunctionTok{installed.packages}\NormalTok{())}
\ControlFlowTok{if}\NormalTok{ (}\FunctionTok{any}\NormalTok{(installed\_packages }\SpecialCharTok{==} \ConstantTok{FALSE}\NormalTok{)) \{}
  \FunctionTok{install.packages}\NormalTok{(packages[}\SpecialCharTok{!}\NormalTok{installed\_packages])}
\NormalTok{\}}

\CommentTok{\# packages loading}
\FunctionTok{invisible}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(packages, library, }\AttributeTok{character.only =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{assessment}{%
\section{Assessment}\label{assessment}}

The final module mark is composed of the \emph{two computational
essays}. Together they are designed to cover the materials introduced in
the entirety of content covered during the semester. A computational
essay is an essay whose narrative is supported by code and computational
results that are included in the essay itself. Each teaching week, you
will be required to address a set of questions relating to the module
content covered in that week, and to use the material that you will
produce for this purpose to build your computational essay.

\ul{Assignment 1 (50\%)} refers to the set of questions at the end of
Chapter~\ref{sec-chp2}, Chapter~\ref{sec-chp3}, Chapter~\ref{sec-chp4}
and Chapter~\ref{sec-chp5}. You are required to use your responses to
build your computational essay. Each chapter provides more specific
guidance of the tasks and discussion that you are required to consider
in your assignment.

\ul{Assignment 2 (50\%)} refers to the set of questions at the end of
Chapter~\ref{sec-chp6}, Chapter~\ref{sec-chp7}, Chapter~\ref{sec-chp8},
Chapter~\ref{sec-chp9} and Chapter~\ref{sec-chp10}. You are required to
use your responses to build your computational essay. Each chapter
provides more specific guidance of the tasks and discussion that you are
required to consider in your assignment.

\hypertarget{format-requirements}{%
\subsection{Format Requirements}\label{format-requirements}}

Both assignments will have the same requirements:

\begin{itemize}
\tightlist
\item
  Maximum word count: 2,000 words, excluding figures and references.
\item
  Up to three maps, plot or figures (a figure may include more than one
  map and/or plot and will only count as one but needs to be integrated
  in the figure)
\item
  Up to two tables.
\end{itemize}

Assignments need to be prepared in ``\emph{Quarto Document}'' format
(i.e.~qmd extension) and then converted into a self-contained HTML file
that will then be submitted via Turnitin. The document should only
display content that will be assessed. Intermediate steps do not need to
be displayed. Messages resulting from loading packages, attaching data
frames, or similar messages do not need to be included as output code.
Useful resources to customise your R notebook can be found on
\href{https://quarto.org/docs/guide/}{Quarto's website}.

Two Quarto Document templates will be available via
\href{https://canvas.liverpool.ac.uk/courses/70394/modules}{the module
Canvas site}.

Submission is electronic only via Turnitin on Canvas.

\hypertarget{marking-criteria}{%
\subsubsection{Marking criteria}\label{marking-criteria}}

The Standard Environmental Sciences School marking criteria apply, with
a stronger emphasis on evidencing the use of regression models, critical
analysis of results and presentation standards. In addition to these
general criteria, the code and outputs (i.e.~tables, maps and plots)
contained within the notebook submitted for assessment will be assessed
according to the extent of documentation and evidence of expertise in
changing and extending the code options illustrated in each chapter.
Specifically, the following criteria will be applied:

\begin{itemize}
\tightlist
\item
  \ul{0-15}: no documentation and use of default options.
\item
  \ul{16-39}: little documentation and use of default options.
\item
  \ul{40-49}: some documentation, and use of default options.
\item
  \ul{50-59}: extensive documentation, and edit of some of the options
  provided in the notebook (e.g.~change north arrow location).
\item
  \ul{60-69}: extensive well organised and easy to read documentation,
  and evidence of understanding of options provided in the code
  (e.g.~tweaking existing options).
\item
  \ul{70-79}: all above, plus clear evidence of code design skills
  (e.g.~customising graphics, combining plots (or tables) into a single
  output, adding clear axis labels and variable names on graphic
  outputs, etc.).
\item
  \ul{80-100}: all as above, plus code containing novel contributions
  that extend/improve the functionality the code was provided with
  (e.g.~comparative model assessments, novel methods to perform the
  task, etc.).
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{sec-chp2}{%
\chapter{Introducing Population Science}\label{sec-chp2}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Population science sits at the intersection between population studies
and data science. As the general field of population studies, population
science seeks to quantitatively understand human populations, including
the three key demographic processes of population change, namely
fertility, mortality and migration. It seeks to understand the size,
structure, temporal changes and spatial distribution of populations, and
the drivers and impacts that underpin their variations and regularities.
It considers the ways in which structural social, economic, political
and environmental factors shape population trends. What is unique about
population science is that it seeks to leverage on the ongoing digital
revolution characterised by technological advances in computer
processing, digitalised information storage capacity and digital
connectivity (Hilbert and López 2011).

The digital revolution ushered in the 1990s has unleashed a data
revolution. Technological advances in computational power, storage and
digital network platforms have enabled the emergence of ``Big Data'' or
``digital footprint data''. These technological developments have
enabled the production, processing, analysis and storage of large
volumes of digital data. Analysing 1986-2007 data, Hilbert and López
(2011) estimated that the world has already passed the point at which
more data were being collected than could be physically stored. They
estimated that the global general-purpose computing capacity grew at an
annual rate of 58 percent between 1986 and 2007, exceeding that of
global storage capacity (23 percent). We can now digitally captured and
generated data that previously could not easily be recorded and stored.

The unprecedented amount of information that we can now capture through
digital technology offers unique opportunities to advance our
understanding of \emph{micro} human behaviour (e.g.~individual-level
decision making, preferences and choices) and \emph{macro} population
processes (e.g.~structural population processes and trends). Digital
footprint data offer a continuous flow of information to capture human
population dynamics at unprecedentedly fine spatial and temporal
resolution in real or near real-time comprising entire social systems.
We can capture and study micro individual behaviours such as online time
use, purchasing behaviour, visitation patterns and public opinion from
data sources, such as mobile phones, social media and retail website
platforms. These behaviours can also be aggregated to shed light into
macro structural processes and trends, such as urban mobility, consumer
demand, transport usage, population ageing and decline. Fundamentally
digital footprint data thus have the potential to become a key pillar
informing and supporting decision making. They can inform business to
increase sales revenue, football clubs to improve team performance, and
governments to tackle major societal issues, such as the COVID-19
pandemic and global warming, influencing policy, practice and governance
structures.

Yet, the use of digital footprint data also poses major conceptual,
methodological and ethical challenges (Rowe 2021b). It is these
challenges that motivated this module. Digital footprint data are a
by-product of an administrative process or service, and it is not
purposely collected for research. Turning raw digital footprint data
into actionable, usable information thus requires a unique combination
of technical computational expertise and subject-specific knowledge.
Traditionally university programmes have tended to focus on providing
technical training, such as statistics or on specific knowledge
subjects. But they are rarely found as a single coherent package. This
module aims to fills this gap by offering training in the use of digital
footprint data, and sophisticated methodological approaches (including
machine learning, artificial intelligence, network science and
statistical methods) to tackle important population issues, such as
population segmenting, decline and mobility. Access to digital footprint
data are highly variable; hence, we do not focus on this here. However,
we encourage users of this book to read a report put together by the
Joint Research Centre (2022) identifying and discussing key data sources
focusing population processes.

The name of this module \emph{Population Science} reflects the inclusive
and interdisciplinary perspective we hope to capture. The data
revolution has led to the emergence of a range of sub-disciplines,
seeking to leverage on the use of digital footprint data to study human
behaviour and population processes. These emerging sub-disciplines have
tended to focus on discipline-specific issues such as digital demography
(Kashyap et al. 2022), or particular methodological approaches, such as
the use of networks principles in computational social sciences (Lazer
et al. 2009). Population science seeks to integrate these perspectives
and provide a fertile framework for critique, collaboration and
co-creation across these emerging areas of scholarship in the study of
human population. And, of course, take a spatial perspective adopting
geographic data science approaches (Singleton and Arribas-Bel 2019).

Specifically, this chapter aims to discuss key opportunities and
challenges of digital footprint data to analyse human population
dynamics. We place a particular focus on the challenges relating to
privacy, bias and privacy issues. The chapter starts by defining digital
footprint data before discussing the key opportunities offered by these
data and the challenges they pose.

\hypertarget{defining-digital-footprint-data}{%
\section{Defining digital footprint
data}\label{defining-digital-footprint-data}}

We define digital footprint data as:

\begin{quote}
\emph{the data recorded by digital technology resulting from the
interactions of people among themselves or with their social and
physical environment, and they can take the form of images, video, text
and numbers.}
\end{quote}

Data footprint data are distinctive features in their volume, velocity,
variety, exhaustiveness, resolution, relational nature and flexibility
(Kitchin 2014). They can take different forms. Traditional data used to
be mostly numeric. Digital footprint data has facilitated the
collection, storage and analysis of text (e.g.~Twitter posts), image
(e.g.~Instagram photos) and video (e.g.~CCTV footage) data.

Multiple digital systems contribute to the storage and generation of
digital footprint data. Kitchin (2014) identified three broad systems
directed, automated and volunteered systems. Directed systems comprise
digital administrative systems operated by a human recording data on
places or people e.g.~immigration control, biometric scanning and health
records. Automated systems involve digital systems which automatically
and autonomously record and process data with little human intervention
e.g.~mobile phone applications, electronic smartcard ticketing, energy
smart meter and traffic sensors. Volunteered systems involve digital
spaces in which humans contribute data through interactions on social
media platforms (e.g.~Twitter and Facebook) or crowdsourcing
(e.g.~OpenStreetMap and Wikipedia).

\begin{figure}

{\centering \includegraphics{figs/chp2/fig2-systems.png}

}

\caption{Digital footprint systems}

\end{figure}

\hypertarget{opportunities-of-digital-footprint-data}{%
\section{Opportunities of digital footprint
data}\label{opportunities-of-digital-footprint-data}}

Digital footprint data offer unique opportunities for the analysis of
human population patterns. As Rowe (2021b) argues, digital footprint
data offer three key promises in relation to traditional data sources,
such as surveys and censuses. They generally provide greater
spatio-temporal granularity, wider coverage and timeliness.

Digital footprint data offer high geographic and temporal
\emph{granularity}. Most digital footprint data are time-stamped and
geographically referenced with high precision. Digital technology, such
as mobile phone and geographical positioning systems enables the
generation of a continuous steams of time-stamped location data. Such
information thus provides an opportunity to trace and enhance our
understanding human populations over highly granular spatial scales and
time intervals, going beyond the static representation afforded by most
traditional data sources. Spatial human interactions, and how people use
and are influenced by their environment, can be analysed in a temporally
dynamic way.

Digital footprint data provide extensive \emph{coverage.} Contrasting to
traditional random sampling, digital footprint data promise information
on universal or near-universal population or geographical systems.
Social media platforms, such as Twitter generate data to capture the
entire universe of Twitter users. Satellite technology produces imagery
snapshots to composite a representation of the Earth. Electronic
smartcard ticketing systems produce information to capture the
population of users in the system. Because the information is typically
consistently collected and storage, the coverage of digital footprint
data offer the potential to study human behaviour of entire systems at a
global scale based on harmonised definitions, which is rarely possible
using traditional data sources.

Digital footprint data are generated in \emph{real-time}. Unlike
traditional systems of data collection and release, digital footprint
data can be streamed continuously in real- or near real-time. Commercial
transactions are generally recorded on bank ledgers as bank card
payments occur at retail shops. Individual mobile phone's location are
captured as applications ping cellular antennas. Such information offer
an opportunity to monitor and response to rapidly evolving situations,
such as the COVID-19 pandemic (Green, Pollock, and Rowe 2021), natural
disasters (Rowe 2022b) and conflicts (Rowe, Neville, and
González-Leonardo 2022).

We also loudly and clearly argue that while digital footprint data
should be seen as a key asset to support government and business
decision making processes, they should not be considered at the expenses
of traditional data sources. Digital footprint data and traditional data
sources should be used to complement each another. As indicated earlier,
digital footprint data are the by-product of administrative processes or
services. They were not designed with the aim of doing research. They
require considerable work of data re-engineering to re-purpose them and
turn them into an analysis-ready data product that can be used for
further analysis (Arribas-Bel et al. 2021). Yet, as we will discuss
below significant challenges remain. As the saying goes ``\emph{all data
are dirty, but some data are useful}''. This quote used in the data
science community to convey the idea that data are often imperfect, but
they can still be used to gain valuable insights. Our message is that
digital footprint data and traditional data sources should be
triangulated to leverage on their strengths and mitigate their
weaknesses.

\hypertarget{challenges-of-digital-footprint-data}{%
\section{Challenges of digital footprint
data}\label{challenges-of-digital-footprint-data}}

Digital footprint data also impose key conceptual, methodological and
ethical challenges. In this section, we provide a brief explanation of
challenges in these areas, focusing particularly on issues around
biases, privacy, ethics and new methods. We focus on these issues
because they are of practical importance and probably of most interest
to the readers of this book. Excellent discussions have been written
and, if you are interested in learning more about the challenges
relating to digital footprint data, we recommend Kitchin (2014), Cesare
et al. (2018), Lazer et al. (2020) and Rowe (2021b).

\hypertarget{conceptual-challenges}{%
\subsection{Conceptual challenges}\label{conceptual-challenges}}

Conceptually, the emergence of digital footprint data has led to the
rethinking and questioning of existing theoretical social science
approaches (Franklin 2022). On the one hand, digital footprint data
provide an opportunity to explore existing theories or hypotheses
through different lens and test the consistency of existing beliefs. For
example, economics theories discuss the existence of temporal and
spatial equilibrium. Resulting hypotheses are generally tested through
mathematical theoretical models or empirical analyses relying on
temporally static data. The existence of equilibrium has thus remained
hard to assess. Digital footprint data provide an opportunity to
empirically test temporal and spatial equilibrium ideas based on
suitable temporally dynamic data. They can enable the testing of cause
and impact hypotheses, rather than only focusing on static associations.

On the other hand, digital footprint data sparked new questions. Digital
footprint data provide data on previously unmeasured activities. Data
now capture activities that were previously difficult to quantify, such
as personal communications, social networks, search and information
gathering, and location data. These data offer an opportunity to develop
new questions expanding existing theories by looking inside the ``black
box'' of households, organisations and markets. They may also open the
door to developing entirely new questions such as the role of digital
technology in shaping human behaviour, and the role of artificial
intelligence on productivity and financial markets.

\hypertarget{methodological-challenges}{%
\subsection{Methodological challenges}\label{methodological-challenges}}

Methodologically, the need for a wide and new set of digital skills and
expertise to handle, store and analyse large volumes of data is a key
challenge. As indicated earlier, digital footprint data are not created
for research purposes. They need to be reengineered for research. Large
streams of digital footprint data cannot be stored on local memory. They
can rarely be read as a single unit on a local computer and may involve
performing the same task numerous times in regular basis, requiring
therefore large storage, computational capacity and computer science
expertise. The manipulation and storage of digital footprint data often
require technical expertise in data management systems, such as SQL,
Google Cloud Storage and Amazon S3, as well as in efficient computing
involving expertise in distributed computing systems and parallelisation
frameworks. The analysis and modelling of digital footprint data may
entail competencies in the application of machine learning and
artificial intelligence. While these competencies generally form part of
a computer science programme, they are rarely taught in an integrated
framework focusing on addressing societal or business challenges
relating to human populations - where the key focus is their
application.

An additional methodological challenge is the presence of biases in
digital footprint data. Digital footprint data are representative of a
specific segment of the population but little is known which segments
and how their representation varies across data sets and digital
technology. Digital footprint data may comprise multiple sources of
biases. They may reflect differences in the use of a digital device
(e.g.~mobile phone) and/or a piece of digital technology (e.g.~a mobile
phone application) Schlosser et al. (2021) . They may also reflect
differences in \emph{frequency} in the use of digital technology
(e.g.~number of times an individual uses a mobile phone application) -
and this frequency may in turn reflect differences in algorithmic
decisions embedded in digital platforms, such as suggesting content
based on prior interactions to increase engagement with a given mobile
phone application. Some work has been done on assessing biases as well
as developing approaches to mitigate their influence Ribeiro,
Benevenuto, and Zagheni (2020).

\hypertarget{ethical-challenges}{%
\subsection{Ethical challenges}\label{ethical-challenges}}

Privacy represents a major ethical challenge. Digital footprint data are
highly sensitive, and hence, anonymisation and disclosure control are
required. Individual records must be anonymised so they are not
identifiable. The high degree of granularity and personal information of
these records may and have been used in ethically questionable ways; for
example, Cambridge Analytica used information of Facebook users to
segment the population and target politically motivated content
(Cadwalladr and Graham-Harrison 2018). Anonymising information, however,
imposes a key challenge as there is a trade-off between accuracy and
privacy (Petti and Flaxman 2020). Anonymisation may reduce the usability
of data. The greater the degree of privacy, the lower is the degree of
accuracy of the resulting data and \emph{vice versa}. Identifying the
optimal point balancing the privacy-accuracy trade-off is the key
challenge. If doing incorrectly, we could end up drawing inferences that
do not reflect the actual population processes displayed in the data, or
have artificially been encoded in the data through noise or reshuffling.
The application of data differential privacy to the US census provides a
recent good example of this challenge. An emblematic case is
\href{https://www.bloomberg.com/news/articles/2021-08-12/data-scientists-ask-can-we-trust-the-2020-census}{New
York's Liberty Island} which has no resident population, but official US
census reported 48 residents which was the result of adding statistical
noise to the data, in order to enhance privacy.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Digital footprint data present unique oppotunites to enhance our
understanding of population processes and support individual, business
and government decisions to improve targeted processes and outcomes.
Businesses have used digital footprint data to segment their consumer
populations and improve their targeting of marketing content, products
and ultimately increase sales and revenue (Dolega, Rowe, and Branagan
2021). Governments and health care institutions, particularly during the
COVID-19 have leverage digital footprint data to monitor the spread of
the pandemic and develop appropriate mitigation responses (Green,
Pollock, and Rowe 2021). However, the use of digital footprint data
poses major conceptual, methodological and ethical challenges - which
need to be overcome to unleash their full potential. The aim of this
book is to address of the key methodological challenges. In particular,
this book seeks to provide applied training on the practical application
of commonly used machine learning and artificial intelligence approaches
to leverage on digital footprint data in the understanding of human
behaviour and population processes.

\bookmarksetup{startatroot}

\hypertarget{sec-chp3}{%
\chapter{Geodemographics}\label{sec-chp3}}

In this chapter we introduce the topic of geodemographics and
geodemographic classifications. The chapter is based on the following
references:

\begin{itemize}
\item
  \href{https://data.cdrc.ac.uk/dataset/creating-geodemographic-classification-using-k-means-clustering-r}{Creating
  a Geodemographic Classification Using K-means Clustering in R} (Guy
  Lansley and James Cheshire, 2018)
\item
  \href{https://jo-wilkin.github.io/GEOG0030/coursebook/geodemographic-classification.html}{Lecture
  10} of the 2020-21 Work Book for the module GEOG0030 on
  Geocomputation, delivered at UCL Department of Geography.
\end{itemize}

Geodemographics is the statistical study of populations based on the
characteristics of their location. It includes the application of
geodemographic classifications (GDCs) or profiling whereby different
locations are classified into groups based on the similarity in their
characteristics.

Assuming that the geodemographic characteristics of a group are an
indicator of how the people in that group behave, GDC can be a very
useful tool to predict the behavioral patterns of different regions. For
this reason, geodemographics and GDC have applications in many areas,
from marketing and retail to public health or service planning
industries.

\hypertarget{sec-sec31}{%
\section{Dependencies}\label{sec-sec31}}

This chapter uses the libraries below. Ensure they are installed on your
machine, then execute the following code chunk to load them:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Support for simple features, a standardised way to encode spatial vector data}
\FunctionTok{library}\NormalTok{(sf)}
\CommentTok{\#Data manipulation}
\FunctionTok{library}\NormalTok{(dplyr)}
\CommentTok{\#A system for creating graphics}
\FunctionTok{library}\NormalTok{(ggplot2)}
\CommentTok{\#Easy viisualisation of a correlation matrix using ggplot2}
\FunctionTok{library}\NormalTok{(ggcorrplot)}
\CommentTok{\#Color maps designed to improve graph readability}
\FunctionTok{library}\NormalTok{(viridis)}
\CommentTok{\#Alternative way of plotting, useful for radial plots}
\FunctionTok{library}\NormalTok{(plotrix)}
\CommentTok{\#Methods for cluster analysis}
\FunctionTok{library}\NormalTok{(cluster)}
\CommentTok{\#Thematic maps can be generated with great flexibility}
\FunctionTok{library}\NormalTok{(tmap)}
\CommentTok{\#Provides some easy{-}to{-}use functions to extract and visualize the output of multivariate data analyses}
\FunctionTok{library}\NormalTok{(factoextra)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
 \CommentTok{\#Obtain the working directory, where we will save the data directory}
\FunctionTok{getwd}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps"
\end{verbatim}

\hypertarget{sec-sec32}{%
\section{Data}\label{sec-sec32}}

\hypertarget{land-use-data-for-greater-manchester}{%
\subsection{Land use data for Greater
Manchester}\label{land-use-data-for-greater-manchester}}

Land use patterns offer insights into the demographic characteristics
and needs of a population within a specific area. How residents use
their land reflects their preferences, demands, and activities. The
extent of residential land indicates the size, density, and distribution
of the population, as well as housing preferences and household sizes.
The extent of commercial and industrial land use relates to economic
activities, employment opportunities, and the overall economic vitality
of a region. Agricultural land use might indicate the food demands and
production capabilities of the population. Additionally, land designated
for public services, education, healthcare, and recreation showcases the
amenities and social infrastructure necessary to support the
population's needs and quality of life. Classifying regions by their
similarity in their land use can therefore provide valuable insights
into the lifestyle, economic status, and social requirements of the
residents in a given area.

In this Chapter we will be looking at land use data provided by
\href{https://github.com/GDSL-UL/APPG-LBA/blob/main/README.md}{What do
`left behind' areas look like over time?}, a project created by the
Geographic Data Science Lab at the University of Liverpool to host open
reproducible code and resulting data for the Briefing: ``What do `left
behind' areas look like over time?''. The land use data is based on
digital data from the CORINE Land Cover (CLC) dataset. This well-known
dataset is compiled and maintained by the European Environment Agency
(EEA) and derived from satellite imagery and aerial photographs, using
remote sensing technology and image analysis techniques.

In particular, we use the file
\texttt{manchester\_land\_cover\_2011.gpkg} based on the file
\texttt{lsoa\_land\_cover.csv} available
\href{https://github.com/GDSL-UL/APPG-LBA/blob/main/data/lsoa_land_cover.csv}{here},
which contains land use data for each Lower Layer Super Output Area
(LSOA) in Greater Manchester.

LSOAs are geographic hierarchies designed to improve the reporting of
small area statistics in England and Wales. LSOAs are built from groups
of contiguous Output Areas (OAs) and have been automatically generated
to be as consistent as possible in population size, with a minimum
population of 1,000. For this reason, their spatial extent varies
depending on how densely populated a region is. The average population
of an LSOA in London in 2010 was 1,722.

\hypertarget{import-the-data}{%
\subsection{Import the data}\label{import-the-data}}

In the code chunk below we load the dataset described above,
\texttt{manchester\_land\_cover\_2011.gpkg} as a simple feature object
and call it \texttt{sf\_LSOA}. We will be generating some maps to show
the geographical distribution of our data and results. To do this, we
need the data that defines the geographical boundaries of the LSOAs.
This data is included in the \texttt{sf\_LSOA} variable, in the column
called \texttt{geom}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The raw data can be obtained from link below, but it has been cleaned by Carmen Cabrera{-}Arnau for this chapter}
\CommentTok{\# https://github.com/GDSL{-}UL/APPG{-}LBA/blob/main/data/lsoa\_land\_cover.csv}

\NormalTok{sf\_LSOA }\OtherTok{\textless{}{-}} \FunctionTok{st\_read}\NormalTok{(}\StringTok{"./data/geodemographics/manchester\_land\_cover\_2011.gpkg"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Reading layer `manchester_land_cover_2011' from data source 
  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/geodemographics/manchester_land_cover_2011.gpkg' 
  using driver `GPKG'
Simple feature collection with 1673 features and 44 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 351662.3 ymin: 381166 xmax: 406087.2 ymax: 421037.7
Projected CRS: OSGB36 / British National Grid
\end{verbatim}

\hypertarget{sec-sec34}{%
\section{Preparing the data for GDC}\label{sec-sec34}}

\hypertarget{choice-of-geographic-units}{%
\subsection{Choice of geographic
units}\label{choice-of-geographic-units}}

Normally, GDCs involve the analysis of aggregated data into geographic
units. Very small geographic units of data aggregation can provide more
detailed results, but if the counts are too low, this could lead to
re-identification issues.

As mentioned above, the data for this chapter is aggregated into LSOAs.
The size of the LSOAs is small enough to produce detailed results and is
also a convenient choice, since it is broadly used in the UK Census and
other official data-reporting exercises.

We can visualise the LSOAs within Greater Manchester simply by plotting
the geometry column of \texttt{st\_LSOA}, which can be selected with the
function \texttt{st\_geometry()}. More sophisticated ways of visualising
geographical data are available in R as we will see in forthcoming
sections.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{st\_geometry}\NormalTok{(sf\_LSOA), }\AttributeTok{border=}\FunctionTok{adjustcolor}\NormalTok{(}\StringTok{"gray20"}\NormalTok{, }\AttributeTok{alpha.f=}\FloatTok{0.4}\NormalTok{), }\AttributeTok{lwd=}\FloatTok{0.6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{geodemographics_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\end{figure}

\hypertarget{variables-of-interest}{%
\subsection{Variables of interest}\label{variables-of-interest}}

Any classification task must be based on certain criteria that
determines how elements are grouped into classes. For GDC, these
criteria are characteristics of the spatial units under study. In this
case, we have prepared the file
\texttt{manchester\_land\_cover\_2011.gpkg} to contain some interesting
land use data corresponding to each LSOA. The data frame
\texttt{sf\_LSOA} contains this data and we can visualise its first few
lines by using the function \texttt{head()} and first four columns by
subsetting the simple feature object:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(sf\_LSOA[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Simple feature collection with 6 features and 4 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 368766.7 ymin: 411048.3 xmax: 372572.6 ymax: 414726.9
Projected CRS: OSGB36 / British National Grid
   LSOA11CD    LSOA11NM Sea.and.ocean..523...2012.
1 E01004766 Bolton 005A                          0
2 E01004767 Bolton 005B                          0
3 E01004768 Bolton 001A                          0
4 E01004769 Bolton 003A                          0
5 E01004770 Bolton 003B                          0
6 E01004771 Bolton 003C                          0
  Continuous.urban.fabric..111...2012.                           geom
1                                    0 MULTIPOLYGON (((371567 4119...
2                                    0 MULTIPOLYGON (((371807.5 41...
3                                    0 MULTIPOLYGON (((370197.2 41...
4                                    0 MULTIPOLYGON (((371924.3 41...
5                                    0 MULTIPOLYGON (((372572.6 41...
6                                    0 MULTIPOLYGON (((371554.4 41...
\end{verbatim}

As we can see, each row contains information about an LSOA and each
column (starting from the third column) represents a demographic
characteristic of the LSOA and the people living there. With the
function \texttt{names()}, we can get the names of the columns in
\texttt{sf\_LSOA}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(sf\_LSOA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "LSOA11CD"                                                                                           
 [2] "LSOA11NM"                                                                                           
 [3] "Sea.and.ocean..523...2012."                                                                         
 [4] "Continuous.urban.fabric..111...2012."                                                               
 [5] "Discontinuous.urban.fabric..112...2012."                                                            
 [6] "Industrial.or.commercial.units..121...2012."                                                        
 [7] "Sport.and.leisure.facilities..142...2012."                                                          
 [8] "Non.irrigated.arable.land..211...2012."                                                             
 [9] "Mineral.extraction.sites..131...2012."                                                              
[10] "Pastures..231...2012."                                                                              
[11] "Natural.grasslands..321...2012."                                                                    
[12] "Moors.and.heathland..322...2012."                                                                   
[13] "Peat.bogs..412...2012."                                                                             
[14] "Transitional.woodland.shrub..324...2012."                                                           
[15] "Water.bodies..512...2012."                                                                          
[16] "Sparsely.vegetated.areas..333...2012."                                                              
[17] "Coniferous.forest..312...2012."                                                                     
[18] "Mixed.forest..313...2012."                                                                          
[19] "Broad.leaved.forest..311...2012."                                                                   
[20] "Construction.sites..133...2012."                                                                    
[21] "Green.urban.areas..141...2012."                                                                     
[22] "Port.areas..123...2012."                                                                            
[23] "Land.principally.occupied.by.agriculture..with.significant.areas.of.natural.vegetation..243...2012."
[24] "Water.courses..511...2012."                                                                         
[25] "Road.and.rail.networks.and.associated.land..122...2012."                                            
[26] "Dump.sites..132...2012."                                                                            
[27] "Airports..124...2012."                                                                              
[28] "Intertidal.flats..423...2012."                                                                      
[29] "Estuaries..522...2012."                                                                             
[30] "Beaches..dunes..sands..331...2012."                                                                 
[31] "Inland.marshes..411...2012."                                                                        
[32] "Salt.marshes..421...2012."                                                                          
[33] "Complex.cultivation.patterns..242...2012."                                                          
[34] "Coastal.lagoons..521...2012."                                                                       
[35] "Bare.rocks..332...2012."                                                                            
[36] "Fruit.trees.and.berry.plantations..222...2012."                                                     
[37] "Burnt.areas..334...2012."                                                                           
[38] "Agro.forestry.areas..244...2012."                                                                   
[39] "LSOA11NMW"                                                                                          
[40] "BNG_E"                                                                                              
[41] "BNG_N"                                                                                              
[42] "LONG"                                                                                               
[43] "LAT"                                                                                                
[44] "GlobalID"                                                                                           
[45] "geom"                                                                                               
\end{verbatim}

We can explore the summary statistics for each of the variables with the
\texttt{summary()} function applied on the variable of interest. For
example, to obtain the summary statistics for the proportion of land
dedicated to discontinuous urban fabric in 20212
\texttt{Discontinuous.urban.fabric..112...2012}, we can run the code
below:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(sf\_LSOA}\SpecialCharTok{$}\NormalTok{Discontinuous.urban.fabric..}\DecValTok{112}\NormalTok{...}\FloatTok{2012.}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.0000  0.4445  0.7474  0.6823  0.9958  1.0000 
\end{verbatim}

This tells us that the mean or average proportion of land dedicated to
discontinuous urban fabric in LSOAs within Greater Manchester is 0.6823.
It also tells us that 1 is the proportion of land dedicated to
discontinuous urban fabric in the LSOA with the maximum proportion of
land dedicated to this use.

To visualise the whole distribution of the variable
\texttt{Discontinuous.urban.fabric..112...2012.}, we can plot a
histogram:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(sf\_LSOA}\SpecialCharTok{$}\NormalTok{Discontinuous.urban.fabric..}\DecValTok{112}\NormalTok{...}\FloatTok{2012.}\NormalTok{, }\AttributeTok{breaks=}\DecValTok{15}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"\% Discontinuous.urban.fabric..112...2012."}\NormalTok{, }\AttributeTok{ylab=}\StringTok{\textquotesingle{}Number of LSOAs\textquotesingle{}}\NormalTok{, }\AttributeTok{main=}\ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{geodemographics_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\end{figure}

The histogram reveals that many LSOAs have a high proportion of
discontinuous urban fabric, but there are a few with more than 50\% of
their land dedicated to this use.

Now the question is whether the LSOAs with similar proportions of
discontinuous urban fabric are also spatially close. To find out, we
need to map the data. We can do this by using the \texttt{tmap} library
functions. We observe that, LSOAs in the South of Greater Manchester
tend to have high proportions of discontinuous urban fabric, while LSOAs
in the peripheries have a low proportion of this type of land use.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{legend\_title }\OtherTok{=} \FunctionTok{expression}\NormalTok{(}\StringTok{"\% Discontinuous urban fabric (2012)"}\NormalTok{)}
\NormalTok{map\_discountinuous }\OtherTok{=} \FunctionTok{tm\_shape}\NormalTok{(sf\_LSOA) }\SpecialCharTok{+}
  \FunctionTok{tm\_fill}\NormalTok{(}\AttributeTok{col =} \StringTok{"Discontinuous.urban.fabric..112...2012."}\NormalTok{, }\AttributeTok{title =}\NormalTok{ legend\_title, }\AttributeTok{text.size =} \DecValTok{28}\NormalTok{, }\AttributeTok{palette =} \FunctionTok{viridis}\NormalTok{(}\DecValTok{256}\NormalTok{), }\AttributeTok{style =} \StringTok{"cont"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# add fill}
  \FunctionTok{tm\_layout}\NormalTok{(}\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.69}\NormalTok{, }\FloatTok{0.02}\NormalTok{), }\AttributeTok{legend.title.size=}\FloatTok{0.9}\NormalTok{, }\AttributeTok{inner.margins=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.26}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{tm\_borders}\NormalTok{(}\AttributeTok{col =} \StringTok{"white"}\NormalTok{, }\AttributeTok{lwd =}\NormalTok{ .}\DecValTok{01}\NormalTok{)  }\SpecialCharTok{+} \CommentTok{\# add borders}
  \FunctionTok{tm\_compass}\NormalTok{(}\AttributeTok{type =} \StringTok{"arrow"}\NormalTok{, }\AttributeTok{position =} \FunctionTok{c}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\StringTok{"top"}\NormalTok{) , }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# add compass}
  \FunctionTok{tm\_scale\_bar}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{10}\NormalTok{), }\AttributeTok{text.size =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{position =}  \FunctionTok{c}\NormalTok{(}\StringTok{"left"}\NormalTok{, }\StringTok{"bottom"}\NormalTok{)) }\CommentTok{\# add scale bar}

\NormalTok{map\_discountinuous}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{geodemographics_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\end{figure}

\hypertarget{sec-sec3}{%
\section{Standardisation}\label{sec-sec3}}

\hypertarget{across-geographic-units}{%
\subsection{Across geographic units}\label{across-geographic-units}}

LSOAs have been designed to have similar population sizes, so their area
fluctuates. If the area of an LSOA is bigger or smaller, this can affect
the figures corresponding to land use (e.g.~presumably, the larger the
total area, the hectares will be dedicated to a certain use).

To counter the effect of varying sizes across geographic units, we
always need to standardise the data so it is given as a proportion or a
percentage. This has already been done in the dataset
\texttt{manchester\_land\_cover\_2011.gpkg}, however, if you were to
create your own dataset, you need to take this into account. To compute
the right percentages, it is important to consider the right
denominator. For example, if we are computing the percentage of people
over the age of 65 in a given geographic unit, we can divide the number
of people over 65 by the total population in that geographic unit, then
multiply by 100. However, if we are computing the percentage of
single-person households, we need to divide the number of single-person
households by the total number of households (and not by the total
population), then multiply by 100.

\hypertarget{variable-standardisation}{%
\subsection{Variable standardisation}\label{variable-standardisation}}

Data outliers are often present when analysing data from the real-world.
These values are generally extreme and difficult to treat statistically.
In GDC, they could end up dominating the classification process. To
avoid this, we need to standardise the input variables as well, so that
they all contribute equally to the classification process.

There are different methods for variable standardisation, but here we
will achieve this by computing the Z-scores for each variable, i.e.~for
variable \(X\), Z-score = \(\dfrac{X-mean(X)}{std(X)}\) where \(std()\)
refers to standard deviation. In R, obtaining the Z-score of a variable
is very simple with the function \texttt{scale()}. Since we want to
obtain the Z-scores of all the variables under consideration, we can
loop over the columns corresponding to the variables that we want to
standardise. Before doing this, we create a dataframe based on
\texttt{sf\_LSOA} but dropping the geometry column:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creates a new data frame based on sf\_LSOA}
\NormalTok{df\_std }\OtherTok{\textless{}{-}}\NormalTok{ sf\_LSOA }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{st\_drop\_geometry}\NormalTok{()}
\CommentTok{\# extracts column names from df\_std}
\NormalTok{colnames\_df\_std }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(df\_std)}

\CommentTok{\# loops columns from position 3 : the last column}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{3}\SpecialCharTok{:} \DecValTok{38}\NormalTok{)\{}
\NormalTok{df\_std[[}\FunctionTok{c}\NormalTok{(colnames\_df\_std[i])]] }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(}\FunctionTok{as.numeric}\NormalTok{(df\_std[, colnames\_df\_std[i]]))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

After the standardisation, some columns in \texttt{df\_std} contain NaN.
For simplicity, we will remove these columns from the resulting
dataframe as below, and we will redefine the variable containing the
column names so that it only stores the column names corresponding to
landuse variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_std }\OtherTok{\textless{}{-}}\NormalTok{ df\_std }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select\_if}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \SpecialCharTok{!}\FunctionTok{any}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(.)))}
\NormalTok{colnames\_df\_std }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(df\_std)[}\DecValTok{3}\SpecialCharTok{:}\NormalTok{(}\FunctionTok{ncol}\NormalTok{(df\_std)}\SpecialCharTok{{-}}\DecValTok{6}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-sec35}{%
\section{Checking for variable association}\label{sec-sec35}}

Before diving into the clustering process, it is necessary to check for
variable associations. Two variables that are strongly associated could
be conveying essentially the same information. Consequently, excessive
weight could be attributed to the phenomenon they refer to in the
clustering process. There are different techniques to check for variable
association, but here we focus on the Pearson's correlation matrix.

Each row and column in a Pearson's correlation matrix represents a
variable. Each entry in the matrix represents the level of correlation
between the variables represented by the corresponding row and column.
In R, a Pearson's correlation matrix can be created very easily with the
\texttt{corr()} function, where the method parameter is set to
``pearson''. As a general rule, two variables with correlation
coefficient greater than 0.8 or smaller than -0.8 are considered to be
highly correlated. If this is the case, we might want to discard one of
the two variables since the information they convey is redundant.
However, in some cases, it might be reasonable to keep both variables if
we can argue that they both have a similar but unique meaning.

The correlation coefficients by themselves are not enough to conclude
whether two variables are correlated. Each correlation coefficient must
be computed in combination with its p-value. For this reason, we also
apply the \texttt{cor\_pmat()} function below, which outputs a matrix of
p-values corresponding to each correlation coefficient. Here, we set the
confidence level at 0.9, therefore, p-values smaller than 0.1 are
considered to be statistically significant. In the correlation matrix
plot, we add crosses to indicate which correlation coefficients are not
significant (i.e.~those above 0.1). Those crosses indicate that there is
not enough statistical evidence to reject the claim that the variables
in the corresponding row and column are uncorrelated.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Matrix of Pearson correlation coefficients}
\NormalTok{corr\_mat }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(df\_std[,}\FunctionTok{c}\NormalTok{(colnames\_df\_std)], }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{)}
\CommentTok{\# Matrix of p{-}values}
\NormalTok{corr\_pmat }\OtherTok{\textless{}{-}} \FunctionTok{cor\_pmat}\NormalTok{(df\_std[,}\FunctionTok{c}\NormalTok{(colnames\_df\_std)], }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\CommentTok{\# Barring the no significant coefficient}
\FunctionTok{ggcorrplot}\NormalTok{(corr\_mat, }\AttributeTok{tl.cex=}\DecValTok{5}\NormalTok{, }\AttributeTok{hc.order =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{outline.color =} \StringTok{"white"}\NormalTok{, }\AttributeTok{p.mat =}\NormalTok{ corr\_pmat, }\AttributeTok{colors =} \FunctionTok{c}\NormalTok{(}\FunctionTok{viridis}\NormalTok{(}\DecValTok{3}\NormalTok{)), }\AttributeTok{lab=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{lab\_size=}\FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{geodemographics_files/figure-pdf/unnamed-chunk-12-1.pdf}

}

\end{figure}

Among the statistically significant values in the correlation matrix, we
can see that none of the variables display a strong correlation
(i.e.~close to -1 or 1), except of course, each variable with itself (as
shown in the diagonal cells of the correlogram). In fact, many of the
measured correlations are not significant. Therefore, we do not need to
remove any variables from the analysis. However, if we had found a
strong correlation between a pair of variables which is statistically
significant, we could remove one of the two variables, since they would
be both capturing very similar information.

\hypertarget{sec-sec36}{%
\section{The clustering process}\label{sec-sec36}}

\hypertarget{k-means}{%
\subsection{K-means}\label{k-means}}

K-means clustering is a way of grouping similar items together. To
illustrate the method, imagine you have a bag filled with vegetables,
and you want to separate them into smaller bags based on their color,
size and flavour. K-means would do this for you by first randomly
selecting a number k of vegetables (you provide k, e.g.~k=4), and then
grouping all the other vegetables based on which of the k vegetables
selected initially they are closest to in color, size and flavour. This
process is repeated a few times until the vegetables are grouped as best
as possible. The end result is k smaller bags, each containing veg of
similar color, size and flavour. This is similar to how k-means groups
similar items in a data set into clusters.

More technically, k-means clustering is actually an algorithm of
unsupervised learning (we will learn more about this in Chapter 10) that
partitions a set of points into k clusters, where k is a user-specified
number. The algorithm iteratively assigns each point to the closest
cluster, based on the mean of the points in the cluster, until no point
can be moved to a different cluster to decrease the sum of squared
distances between points and their assigned cluster mean. The result is
a partitioning of the points into k clusters, where the points within a
cluster are as similar as possible to each other, and as dissimilar as
possible from points in other clusters.

In R, k-means can be easily applied by using the function
\texttt{k-means()}, where some of the required arguments are: the
dataset, the number of clusters (which is called centers), the number of
random sets to choose (\texttt{nstart}) or the maximum number of
iterations allowed. For example, for a 4-cluster classification, we
would run the following line of code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Km }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(df\_std[,}\FunctionTok{c}\NormalTok{(colnames\_df\_std)], }\AttributeTok{centers=}\DecValTok{4}\NormalTok{, }\AttributeTok{nstart=}\DecValTok{20}\NormalTok{, }\AttributeTok{iter.max =} \DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{number-of-clusters}{%
\subsection{Number of clusters}\label{number-of-clusters}}

As mentioned above, the number of clusters k is a parameter of the
algorithm that has to be specified by the user. Ultimately, there is no
right or wrong answer to the question `what is the optimum number of
clusters?'. Deciding the value of k in the k-means algorithm can be a
somewhat subjective process where in most cases, common sense is the
most useful approach. For example, you can ask yourself if the obtained
groups are meaningful and easy to interpret or if, on the other hand,
there are too few or too many clusters, making the results unclear.

However, there are some techniques and guidelines to help us decide what
the right number of clusters is. Here we explore the silhouette score
method.

The \textbf{silhouette score} of a data point (in this case an LSOA and
its demographic data) is a measure of how similar this data point is to
the data points in its own cluster compared to the data points in other
clusters. The silhouette score ranges from -1 to 1, with a higher value
indicating that the data point is well matched to its own cluster and
poorly matched to neighbouring clusters. A score close to 1 means that
the data point is distinctly separate from other clusters, whereas a
score close to -1 means the data point may have been assigned to the
wrong cluster. Given a number of clusters k obtained with k-means, we
can compute the average silhouette score over all the data points. Then,
we can plot the average silhouette score against k. The optimal value of
\emph{k} will be the one with the highest k score.

You can run the code below to compute the average silhouette score
corresponding to different values of k ranging from 2 to 20. The optimum
number of clusters is given by the value of \emph{k} at which the
average silhouette is maximised.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Specify a random number generator seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }

\NormalTok{silhouette\_score }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(k)\{}
\NormalTok{  km }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(df\_std[,}\FunctionTok{c}\NormalTok{(colnames\_df\_std)], }\AttributeTok{centers =}\NormalTok{ k, }\AttributeTok{nstart=}\DecValTok{5}\NormalTok{, }\AttributeTok{iter.max =} \DecValTok{1000}\NormalTok{)}
\NormalTok{  ss }\OtherTok{\textless{}{-}} \FunctionTok{silhouette}\NormalTok{(km}\SpecialCharTok{$}\NormalTok{cluster, }\FunctionTok{dist}\NormalTok{(df\_std[,}\FunctionTok{c}\NormalTok{(colnames\_df\_std)]))}
  \FunctionTok{mean}\NormalTok{(ss[, }\DecValTok{3}\NormalTok{])}
\NormalTok{\}}
\NormalTok{k }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{:}\DecValTok{20}
\NormalTok{avg\_sil }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(k, silhouette\_score)}
\FunctionTok{plot}\NormalTok{(k, }\AttributeTok{type=}\StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{, avg\_sil, }\AttributeTok{xlab=}\StringTok{\textquotesingle{}Number of clusters\textquotesingle{}}\NormalTok{, }\AttributeTok{ylab=}\StringTok{\textquotesingle{}Average Silhouette Scores\textquotesingle{}}\NormalTok{, }\AttributeTok{frame=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{geodemographics_files/figure-pdf/unnamed-chunk-14-1.pdf}

}

\end{figure}

From the figure, we can see that the optimum k is 12. Note, this number
might be different when you run the programme since the clustering
algorithm involves some random steps, unless you set the seed to 123 as
we did above. A number of clusters of k=12 is too high for our results
to be insightful and interpretable. Therefore, to keep our
interpretations simpler, we will pick a lower number of clusters to help
us undertand our data better, such as 5.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Specify a random number generator seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }
\NormalTok{Km }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(df\_std[,}\FunctionTok{c}\NormalTok{(colnames\_df\_std)], }\AttributeTok{centers=}\DecValTok{5}\NormalTok{, }\AttributeTok{nstart=}\DecValTok{20}\NormalTok{, }\AttributeTok{iter.max =} \DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{other-clustering-methods}{%
\subsection{Other clustering methods}\label{other-clustering-methods}}

There are several other clustering methods apart from k-means. Each
method has its own advantages and disadvantages, and the choice of
method will ultimately depend on the specific data and clustering
problem. We will not explore these methods in detail, but below we
include some of their names and a brief description. If you want to
learn about them, you can refer to the book ``Pattern Recognition and
Machine Learning'' by Christopher Bishop (Bishop 2006).

\begin{itemize}
\item
  Fuzzy C-means: a variation of k-means where a data point can belong to
  multiple clusters with different membership levels.
\item
  Hierarchical clustering: this method forms a tree-based representation
  of the data, where each leaf node represents a single data point and
  the branches represent clusters. A popular version of this method is
  agglomerative hierarchical clustering, where individual data points
  start as their own clusters, and are merged together in a bottom-up
  fashion based on similarity.
\item
  DBSCAN: a density-based clustering method that groups together nearby
  points and marks as outliers those points that are far away from any
  cluster.
\item
  Gaussian Mixture Model (GMM): GMMs are probabilistic models that
  assume each cluster is generated from a Gaussian distribution. They
  can handle clusters of different shapes, sizes, and orientations.\\
\end{itemize}

\hypertarget{sec-sec37}{%
\section{GDC results}\label{sec-sec37}}

\hypertarget{mapping-the-clusters}{%
\subsection{Mapping the clusters}\label{mapping-the-clusters}}

Our LSOAs are now grouped into 5 clusters according to the similarity in
their land use characteristics. We can include to our dataset the
cluster where each geographical unit belongs to:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_std}\SpecialCharTok{$}\NormalTok{cluster }\OtherTok{\textless{}{-}}\NormalTok{ Km}\SpecialCharTok{$}\NormalTok{cluster}
\end{Highlighting}
\end{Shaded}

To map the results from clustering, we add the spatial inforamtion which
we recover from \texttt{sf\_LSOA}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Get the geometry column from the sf object}
\NormalTok{geometry\_column }\OtherTok{\textless{}{-}} \FunctionTok{st\_geometry}\NormalTok{(sf\_LSOA)}

\CommentTok{\# Set the geometry column in the dataframe which becomes a simple feature object}
\NormalTok{sf\_std }\OtherTok{\textless{}{-}} \FunctionTok{st\_set\_geometry}\NormalTok{(df\_std, geometry\_column)}
\end{Highlighting}
\end{Shaded}

Finally, we can plot the results of the clustering process on a map
using functions from the \texttt{tmap} library:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{map\_cluster }\OtherTok{=} \FunctionTok{tm\_shape}\NormalTok{(sf\_std) }\SpecialCharTok{+}
  \FunctionTok{tm\_fill}\NormalTok{(}\AttributeTok{col =} \StringTok{"cluster"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Cluster"}\NormalTok{, }\AttributeTok{palette =} \FunctionTok{viridis}\NormalTok{(}\DecValTok{256}\NormalTok{), }\AttributeTok{style =} \StringTok{"cont"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# add fill}
  \FunctionTok{tm\_borders}\NormalTok{(}\AttributeTok{col =} \StringTok{"white"}\NormalTok{, }\AttributeTok{lwd =}\NormalTok{ .}\DecValTok{01}\NormalTok{)  }\SpecialCharTok{+} \CommentTok{\# add borders}
  \FunctionTok{tm\_layout}\NormalTok{(}\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.02}\NormalTok{), }\AttributeTok{legend.title.size=}\FloatTok{0.9}\NormalTok{, }\AttributeTok{inner.margins=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.05}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{tm\_compass}\NormalTok{(}\AttributeTok{type =} \StringTok{"arrow"}\NormalTok{, }\AttributeTok{position =} \FunctionTok{c}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\StringTok{"top"}\NormalTok{) , }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# add compass}
  \FunctionTok{tm\_scale\_bar}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{10}\NormalTok{), }\AttributeTok{text.size =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{position =}  \FunctionTok{c}\NormalTok{(}\StringTok{"left"}\NormalTok{, }\StringTok{"bottom"}\NormalTok{)) }\CommentTok{\# add scale bar}

\NormalTok{map\_cluster}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{geodemographics_files/figure-pdf/unnamed-chunk-18-1.pdf}

}

\end{figure}

\textbf{Note:} sometimes, the number of items in a cluster may be very
small. In that case, you may want to merge two clusterx to make the
number of items in each group more homogeneous or perhaps change k in
the k-means algorithm.

\hypertarget{cluster-interpretation}{%
\subsection{Cluster interpretation}\label{cluster-interpretation}}

The map above not only displays the clusters where each LSOA belongs,
but it also shows that there is a tendency for LSOAs belonging to the
same cluster to be geographically close. This indicates that LSOAs with
similar land use characteristics are close to each other. However, we
still need to understand what each cluster represents.

The so-called cluster centers (kmCenters) are the data points that,
within each cluster, provide a clear indication of the average
characteristics of the cluster where they belong based on the variables
used in the classification. The data used in the clustering process was
Z-score standardised, so the values of each variable corresponding to
the cluster centers are still presented as Z-scores. Zero indicates the
mean for each variable across all the data points in the sample, and
values above or below zero indicate the number of standard deviations
from the average. This makes it easy to understand the unique
characteristics of each cluster relative to the entire sample. To
visualise the characteristics and meaning of the clusters centers and
their corresponding clusters, we use radial plots. Below we produce a
radial plot for cluster 1. Can you see which variables are higher or
lower than average in this cluster? If you want to visualise the radial
plot for other clusters, you will need to change the number inside the
brackets of \texttt{KmCenters\textbackslash{}{[}1,\textbackslash{}{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creates a radial plot for cluster 1}
\CommentTok{\# the boxed.radial (False) prevents white boxes forming under labels}
\CommentTok{\# radlab rotates the labels}
\NormalTok{KmCenters }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(Km}\SpecialCharTok{$}\NormalTok{centers)}
\NormalTok{KmCenters }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(KmCenters)}
\FunctionTok{radial.plot}\NormalTok{(KmCenters[}\DecValTok{1}\NormalTok{,], }\AttributeTok{labels =} \FunctionTok{colnames}\NormalTok{(KmCenters),}
\AttributeTok{boxed.radial =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{show.grid =} \ConstantTok{TRUE}\NormalTok{,}
\AttributeTok{line.col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{radlab =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rp.type=}\StringTok{"p"}\NormalTok{, }\AttributeTok{label.prop=}\DecValTok{1}\NormalTok{, }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{geodemographics_files/figure-pdf/unnamed-chunk-19-1.pdf}

}

\end{figure}

\hypertarget{testing}{%
\subsection{Testing}\label{testing}}

We will evalueate the fit of the k-means model with 5 clusters by
creating an \emph{x-y} plot of the of the first two principal components
of each data point. Each point is coloured according to the cluster
where it belongs. Remember that the aim of principal component analysis
is to create the minimum number of new variables based on a combination
of the original variables that can explain the variability in the data.
The first principal component is the new variable that captures the most
variability.

In the plot below, we can see the first and second principal components
in the x and y axes respectively, with the axis label indicating the
amount of variability that these components are able to explain. To
create the plot, we use the \texttt{fviz\_cluster()} function from the
factoextra library.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_cluster}\NormalTok{(Km, }\AttributeTok{data =}\NormalTok{ df\_std[,}\FunctionTok{c}\NormalTok{(colnames\_df\_std)], }\AttributeTok{geom =} \StringTok{"point"}\NormalTok{, }\AttributeTok{ellipse =}\NormalTok{ F, }\AttributeTok{pointsize =} \FloatTok{0.5}\NormalTok{,}
\AttributeTok{ggtheme =} \FunctionTok{theme\_classic}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{geodemographics_files/figure-pdf/unnamed-chunk-20-1.pdf}

}

\end{figure}

There are obvious clusters in the plot, but some points are in the
overlapping regions of two or more clusters, making it unclear to what
cluster they should really belong. This is a result of the fact that the
plot is only representing two of the principal components, and there are
other variables that are not captured in this 2-dimensional
representation.

\hypertarget{questions}{%
\section{Questions}\label{questions}}

For this set of questions, we will be using the same dataset that we
used for Chapter 3, but for Greater London:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sf\_LSOA }\OtherTok{\textless{}{-}} \FunctionTok{st\_read}\NormalTok{(}\StringTok{"./data/geodemographics/london\_land\_cover\_2011.gpkg"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Reading layer `london_land_cover_2011' from data source 
  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/geodemographics/london_land_cover_2011.gpkg' 
  using driver `GPKG'
Simple feature collection with 4835 features and 44 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6
Projected CRS: OSGB36 / British National Grid
\end{verbatim}

Prepare your data for a geodemographic classification (GDC). To do this,
start by standardising the land use variables. Then, check for variable
association using a correlation matrix. Discard any variables if
necessary. Now you should be ready to group the data into clusters using
the k-means algorithm. Report the optimal number of clusters based on
the average silhouette score method. Select the number of clusters for a
GDC with k-means according to this method or otherwise. Join the
resulting dataset with the LSOA boundary data. Every time you apply the
\texttt{kmeans()} function, you should set nstart=20 and iter.max=1000.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Essay questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Describe how you prepared your data for the GDC. There is no need to
  include figures, but you should briefly explain how you reached
  certain decisions. For example, did you discard any variables due to
  their strong association with other variables in the dataset? How did
  you pick the number of clusters for your GDC?
\item
  Map the resulting clusters and generate a radial plot for one of the
  clusters. You should create just one figure with as many subplots as
  needed.
\item
  Describe what you observe and comment on your results. Do you observe
  any interesting patterns? Do the results of this GDC agree with what
  you would expect? Justify your answer.
\end{enumerate}

\bookmarksetup{startatroot}

\hypertarget{sec-chp4}{%
\chapter{Sequence Analysis}\label{sec-chp4}}

This chapter illustrates the use of sequence analysis and
\href{https://www.worldpop.org}{WorldPop} raster data to identify
trajectories of population decline in Ukraine. Developed in Biology for
the analysis of DNA sequencing, sequence analysis offers a novel
approach to generate a more holistic understanding of population decline
trajectories capturing differences in the ordering, frequency, timing
and occurrence of population decline. The \emph{longitudinal} and
\emph{categorical} nature is a key feature of the data that can be
analysed using sequence analysis. In this chapter, We first show how to
manipulate gridded, raster population to create a spatial data frame,
and explore national and sub-national population trends and spatial
structure of population change. We describe the process of implementing
sequence analysis to identify trajectories of population decline in a
four-stage process. We first define different levels of population
change. Second, we apply measure the difference or similarity between
individual area-level trajectories. Third, we use an unsupervised
machine learning clustering algorithm to identify representative
trajectories. Fourth, we use different visualisation tools to understand
key features encoded in trajectories.

The chapter is based on the following references:

Gabadinho et al. (2011) describe the functionalities of the
\texttt{TraMineR} package to visualise and analysis categorical sequence
data;

Newsham and Rowe (2022b), González-Leonardo, Newsham, and Rowe (2023)
provide empirical applications to identify and study trajectories of
population decline across Europe and in Spain;

Tatem (2017) provide an overview of the \emph{WorldPop} data project;

Patias, Rowe, and Arribas-Bel (2021), Patias et al. (2021) provide good
examples of the use of sequence analysis to define trajectories of
inequality and urban development.

\hypertarget{dependencies}{%
\section{Dependencies}\label{dependencies}}

We use the libraries below. Note that to use the
\texttt{theme\_tufte2()} used for \texttt{ggplot()} objects in this
chapter, you need to call the file \texttt{data-viz-themes.R} in the
repository.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# data manipulation}
\FunctionTok{library}\NormalTok{(tidyverse)}
\CommentTok{\# spatial data manipulation}
\FunctionTok{library}\NormalTok{(stars)}
\FunctionTok{library}\NormalTok{(sf)}
\CommentTok{\# download world pop data}
\FunctionTok{library}\NormalTok{(wpgpDownloadR) }\CommentTok{\# you may need to install this package running \textasciigrave{}install.packages("devtools")\textasciigrave{} \textasciigrave{}devtools::install\_github("wpgp/wpgpDownloadR")\textquotesingle{}}
\CommentTok{\# data visualisation}
\FunctionTok{library}\NormalTok{(viridis)}
\FunctionTok{library}\NormalTok{(RColorBrewer)}
\FunctionTok{library}\NormalTok{(patchwork)}
\FunctionTok{library}\NormalTok{(ggseqplot) }\CommentTok{\# may need to install by running \textasciigrave{}devtools::install\_github("maraab23/ggseqplot")\textasciigrave{}}
\CommentTok{\# sequence analysis}
\FunctionTok{library}\NormalTok{(TraMineR)}
\CommentTok{\# cluster analysis}
\FunctionTok{library}\NormalTok{(cluster)}
\end{Highlighting}
\end{Shaded}

Key packages to this chapter are \texttt{TraMineR},\texttt{stars} and
\texttt{ggseqplot}. \texttt{TraMineR} is the go-to package in social
sciences for exploring, analysing and rendering sequences based on
categorical data. \texttt{stars} is designed to handle spatio-temporal
data in the form of dense arrays, with space and time as dimensions.
\texttt{stars} provides classes and methods for reading, manipulating,
plotting and writing data cubes. It is a very powerful package. It
interacts nicely with \texttt{sf} and is suggested to be superior to
\texttt{raster} and \texttt{terra}, which are also known for their
capacity to work with multilayer rasters. \texttt{stars} is suggested to
\href{https://r-spatial.github.io/stars/}{deal with more complex data
types} and
\href{https://www.r-bloggers.com/2021/05/a-comparison-of-terra-and-stars-packages/}{be
faster} than \texttt{raster} and \texttt{terra}. \texttt{ggseqplot}
provides functionality to visualise categorical sequence data based on
\texttt{ggplot} capabilities. This differs from \texttt{TraMineR} which
is based on the base function \texttt{plot}. We prefer
\texttt{ggseqplot} for the wide usage of \texttt{ggplot} as a data
visualisation tool in R.

\hypertarget{data}{%
\section{Data}\label{data}}

The key aim of this chapter is to define representative trajectories of
population decline using sequence analysis and WorldPop data. We use
WorldPop data for the period extending from 2000 to 2020. WorldPop
offers open access gridded population estimates at a high spatial
resolution for all countries in the world. WoldPop produces these
gridded datasets using a top-down (i.e.~dissagregating administrative
area counts into smaller grid cells) or bottom-up (i.e.~interpolating
data from counts from sample locations into grid cells) approach. You
can learn about about these approaches and the data available from
\href{https://www.worldpop.org}{WorldPop}.

WorldPop population data are available in various formats:

\begin{itemize}
\item
  Two spatial resolutions: 100m and 1km;
\item
  Constrained and unconstrained counts to built settlement areas;
\item
  Adjusted or unadjusted to United Nations' (UN) national population
  counts;
\item
  Two formats i.e.~\texttt{tiff} and \texttt{csv} formats.
\end{itemize}

We use annual 1km gridded, UN adjusted, unconstrained population count
data for Ukraine during 2000-2021 in tiff format. We use tiff formats to
illustrate the manipulation of raster data. Such skills will come handy
if you ever decide to work with satellite imagery or image data in
general.

Before calling the data, let's see how we can use \texttt{wpgpDownloadR}
package. Let's browse the data catalogue.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wpgpListCountries}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in readLines(con, n = 1): incomplete final line found on
'/var/folders/9z/ql42lpgn22x_c5353k3ycqfr0000gn/T//Rtmplf1ZSG/wpgpDatasets.md5'
\end{verbatim}

\begin{verbatim}
  ISO ISO3            Country
1 643  RUS             Russia
2 360  IDN          Indonesia
3 840  USA      United States
4 850  VIR Virgin_Islands_U_S
5 304  GRL          Greenland
6 156  CHN              China
\end{verbatim}

By using the ISO3 country code, let's look for the available datasets
for Ukraine.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wpgpListCountryDatasets}\NormalTok{(}\AttributeTok{ISO3 =} \StringTok{"UKR"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in readLines(con, n = 1): incomplete final line found on
'/var/folders/9z/ql42lpgn22x_c5353k3ycqfr0000gn/T//Rtmplf1ZSG/wpgpDatasets.md5'
\end{verbatim}

\begin{verbatim}
     ISO ISO3 Country Covariate
232  804  UKR Ukraine  ppp_2000
481  804  UKR Ukraine  ppp_2001
730  804  UKR Ukraine  ppp_2002
979  804  UKR Ukraine  ppp_2003
1228 804  UKR Ukraine  ppp_2004
1477 804  UKR Ukraine  ppp_2005
                                                                                                                                                                 Description
232  Estimated total number of people per grid-cell 2000 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)
481  Estimated total number of people per grid-cell 2001 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)
730  Estimated total number of people per grid-cell 2002 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)
979  Estimated total number of people per grid-cell 2003 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)
1228 Estimated total number of people per grid-cell 2004 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)
1477 Estimated total number of people per grid-cell 2005 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)
\end{verbatim}

The \texttt{wpgpDownloadR} package includes 100m resolution data. To
keep things efficient, we use 1km gridded population counts from the
\href{https://hub.worldpop.org/geodata/listing?id=75}{WorldPop data
page}. Obtain population data for Ukraine 2000-2020. We start by reading
the set of tiff files using the \texttt{read\_stars} function from the
\texttt{star} package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a list of file names}
\NormalTok{file\_list }\OtherTok{\textless{}{-}}\NormalTok{ fs}\SpecialCharTok{::}\FunctionTok{dir\_ls}\NormalTok{(}\StringTok{"./data/sequence{-}analysis/raster"}\NormalTok{)}
\NormalTok{file\_list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
./data/sequence-analysis/raster/ukr_ppp_2000_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2001_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2002_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2003_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2004_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2005_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2006_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2007_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2008_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2009_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2010_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2011_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2012_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2013_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2014_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2015_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2016_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2017_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2018_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2019_1km_Aggregated_UNadj.tif
./data/sequence-analysis/raster/ukr_ppp_2020_1km_Aggregated_UNadj.tif
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read a list of raster data}
\NormalTok{pop\_raster }\OtherTok{\textless{}{-}} \FunctionTok{read\_stars}\NormalTok{(file\_list, }\AttributeTok{quiet =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We map the data for 2000 to get a quick understanding of the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(pop\_raster[}\DecValTok{1}\NormalTok{], }\AttributeTok{col =} \FunctionTok{inferno}\NormalTok{(}\DecValTok{100}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
downsample set to 3
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{sequence-analysis_files/figure-pdf/unnamed-chunk-6-1.pdf}

}

\end{figure}

Next we read shapefile of administrative boundaries in the form of
polygons. We obtain these data from the \href{https://gadm.org}{GADM
website}. GADM provides maps and spatial data for individuals countries
at the national and sub-national administrative divisions. In this
chapter, we will work with these data as they come directly from the
website which provides a more realistic and similar context to which you
will probably come across in the ``real-world''.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read spatial data frame}
\NormalTok{ukr\_shp }\OtherTok{\textless{}{-}} \FunctionTok{st\_read}\NormalTok{(}\StringTok{"./data/sequence{-}analysis/ukr\_shp/gadm41\_UKR\_2.shp"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{st\_simplify}\NormalTok{(., }\CommentTok{\# simplify boundaries for efficiency}
              \AttributeTok{preserveTopology =}\NormalTok{ T,}
              \AttributeTok{dTolerance =} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# 1km}
\NormalTok{  sf}\SpecialCharTok{::}\FunctionTok{st\_make\_valid}\NormalTok{(.) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{fortify}\NormalTok{(.) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# turns maps into a data frame so they can more easily be plotted with ggplot2}
  \FunctionTok{st\_transform}\NormalTok{(., }\StringTok{"EPSG:4326"}\NormalTok{) }\CommentTok{\# set projection system}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Reading layer `gadm41_UKR_2' from data source 
  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/sequence-analysis/ukr_shp/gadm41_UKR_2.shp' 
  using driver `ESRI Shapefile'
Simple feature collection with 629 features and 13 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 22.14045 ymin: 44.38597 xmax: 40.21807 ymax: 52.37503
Geodetic CRS:  WGS 84
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ukr\_shp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Simple feature collection with 629 features and 13 fields (with 1 geometry empty)
Geometry type: GEOMETRY
Dimension:     XY
Bounding box:  xmin: 22.14519 ymin: 44.38681 xmax: 40.21807 ymax: 52.375
Geodetic CRS:  WGS 84
First 10 features:
       GID_2 GID_0 COUNTRY   GID_1   NAME_1 NL_NAME_1            NAME_2
1          ?   UKR Ukraine       ?        ?         ?                 ?
2  UKR.1.1_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська        Cherkas'ka
3  UKR.1.2_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська       Cherkas'kyi
4  UKR.1.3_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська   Chornobaivs'kyi
5  UKR.1.4_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська     Chyhyryns'kyi
6  UKR.1.5_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська       Drabivs'kyi
7  UKR.1.6_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська Horodyshchens'kyi
8  UKR.1.7_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська       Kamians'kyi
9  UKR.1.8_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська         Kanivs'ka
10 UKR.1.9_1   UKR Ukraine UKR.1_1 Cherkasy Черкаська        Kanivs'kyi
         VARNAME_2 NL_NAME_2      TYPE_2                     ENGTYPE_2 CC_2
1                ?        NA           ?                            NA   NA
2               NA        NA Mis'ka Rada City of Regional Significance   NA
3               NA        NA       Raion                      District   NA
4  Chornobayivskyi        NA       Raion                      District   NA
5               NA        NA       Raion                      District   NA
6               NA        NA       Raion                      District   NA
7  Gorodyschenskyi        NA       Raion                      District   NA
8               NA        NA       Raion                      District   NA
9               NA        NA       Misto                          City   NA
10              NA        NA       Raion                      District   NA
     HASC_2                       geometry
1         ? POLYGON ((30.59574 50.40547...
2  UA.CK.CM POLYGON ((32.1715 49.43881,...
3  UA.CK.CR POLYGON ((32.03393 49.49881...
4  UA.CK.CB POLYGON ((32.17991 49.44486...
5  UA.CK.CY POLYGON ((32.26144 49.20893...
6  UA.CK.DR POLYGON ((32.41852 49.83724...
7  UA.CK.HO POLYGON ((31.56959 49.42509...
8  UA.CK.KN POLYGON ((32.19797 49.20946...
9  UA.CK.KM MULTIPOLYGON (((31.4459 49....
10 UA.CK.KR POLYGON ((31.5851 49.62482,...
\end{verbatim}

Let's have a quick look at the resolution of the administrative areas we
will be working. The areas below represent areas at the administrative
area level 2 in the spatial data frame \texttt{ukr\_shp}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(ukr\_shp}\SpecialCharTok{$}\NormalTok{geometry)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sequence-analysis_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\end{figure}

We ensure that the \texttt{pop\_raster} object is in the same projection
system as \texttt{ukr\_shp}. So we can make both objects to work
together.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pop\_raster }\OtherTok{\textless{}{-}} \FunctionTok{st\_transform}\NormalTok{(pop\_raster, }\FunctionTok{st\_crs}\NormalTok{(ukr\_shp))                      }
\end{Highlighting}
\end{Shaded}

\hypertarget{data-wrangling}{%
\subsection{Data wrangling}\label{data-wrangling}}

For our application, we want to work with administrative areas for three
reasons. First, public policy and planning decisions are often made
based on administrative areas. These are the areas local governments
have jurisdiction, represent and can exert power. Second, migration is a
key component of population change and hence directly determines
population decline. At a small area, residential mobility may also
impact patterns of population potentially adding more complexity and
variability to the process. Third, WorldPop data are modelled population
estimates with potentially high levels of uncertainty or errors in
certain locations. Our aim is to mitigate the potential impacts of these
errors.

We therefore recommend working with aggregated data. We aggregate the
1km gridded population data to administrative areas in Ukraine. We use
\texttt{system.time} to time the duration of the proccess of aggregation
which could take some time depending on your local computational
environment.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{system.time}\NormalTok{(\{}

\NormalTok{popbyarea\_df }\OtherTok{=} \FunctionTok{aggregate}\NormalTok{(}\AttributeTok{x =}\NormalTok{ pop\_raster, }
                                   \AttributeTok{by =}\NormalTok{ ukr\_shp, }
                                   \AttributeTok{FUN =}\NormalTok{ sum, }
                                   \AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{) }
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   user  system elapsed 
 64.881   8.473  73.527 
\end{verbatim}

\textbf{Sub-national population}

The chunk code above returns a list of raster data. We want to create a
spatial data frame containing population counts for individual
sub-national areas and years. We achieve this by running the following
code:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a function to bind the population data frame to the shapefile}
\NormalTok{add\_population }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{mutate}\NormalTok{(ukr\_shp, }
                      \AttributeTok{population =}\NormalTok{ x)}

\CommentTok{\# obtain sub{-}national population counts}
\NormalTok{ukr\_eshp }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(popbyarea\_df, add\_population)}

\CommentTok{\# create a dataframe with sub{-}national populations}
\NormalTok{select\_pop }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(x, GID\_2, NAME\_2, population)}
\NormalTok{population\_df }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(ukr\_eshp, select\_pop) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{do.call}\NormalTok{(rbind, .)}
\NormalTok{population\_df}\SpecialCharTok{$}\NormalTok{year }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{2000}\NormalTok{, }\DecValTok{2020}\NormalTok{), }\AttributeTok{times =} \DecValTok{1}\NormalTok{, }\AttributeTok{each =} \FunctionTok{nrow}\NormalTok{(ukr\_shp))}
\FunctionTok{rownames}\NormalTok{(population\_df) }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(population\_df), }\AttributeTok{by=}\DecValTok{1}\NormalTok{), }\AttributeTok{times =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# sub{-}national spatial data frame}
\NormalTok{population\_df }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Simple feature collection with 13209 features and 4 fields (with 21 geometries empty)
Geometry type: GEOMETRY
Dimension:     XY
Bounding box:  xmin: 22.14519 ymin: 44.38681 xmax: 40.21807 ymax: 52.375
Geodetic CRS:  WGS 84
First 10 features:
       GID_2            NAME_2 population                       geometry year
1          ?                 ?  301849.00 POLYGON ((30.59574 50.40547... 2000
2  UKR.1.1_1        Cherkas'ka  280917.39 POLYGON ((32.1715 49.43881,... 2000
3  UKR.1.2_1       Cherkas'kyi   89116.78 POLYGON ((32.03393 49.49881... 2000
4  UKR.1.3_1   Chornobaivs'kyi   50096.24 POLYGON ((32.17991 49.44486... 2000
5  UKR.1.4_1     Chyhyryns'kyi   36646.73 POLYGON ((32.26144 49.20893... 2000
6  UKR.1.5_1       Drabivs'kyi   42467.86 POLYGON ((32.41852 49.83724... 2000
7  UKR.1.6_1 Horodyshchens'kyi   49886.59 POLYGON ((31.56959 49.42509... 2000
8  UKR.1.7_1       Kamians'kyi   35587.28 POLYGON ((32.19797 49.20946... 2000
9  UKR.1.8_1         Kanivs'ka   14406.93 MULTIPOLYGON (((31.4459 49.... 2000
10 UKR.1.9_1        Kanivs'kyi   37495.04 POLYGON ((31.5851 49.62482,... 2000
\end{verbatim}

\textbf{National population}

We also create a data frame providing population counts at the national
level.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# obtain national population counts}
\NormalTok{population\_count }\OtherTok{\textless{}{-}} \FunctionTok{map\_dbl}\NormalTok{(ukr\_eshp, }\SpecialCharTok{\textasciitilde{}}\NormalTok{.x }\SpecialCharTok{\%\textgreater{}\%} 
          \FunctionTok{pull}\NormalTok{(population) }\SpecialCharTok{\%\textgreater{}\%} 
          \FunctionTok{sum}\NormalTok{(}\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{        ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{()}

\CommentTok{\# change labels}
\FunctionTok{colnames}\NormalTok{(population\_count) }\OtherTok{\textless{}{-}}  \FunctionTok{c}\NormalTok{(}\StringTok{"population"}\NormalTok{)}
\FunctionTok{rownames}\NormalTok{(population\_count) }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{2000}\NormalTok{, }\DecValTok{2020}\NormalTok{, }\AttributeTok{by=}\DecValTok{1}\NormalTok{), }\AttributeTok{times =} \DecValTok{1}\NormalTok{)}
\NormalTok{population\_count}\SpecialCharTok{$}\NormalTok{year }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{2000}\NormalTok{, }\DecValTok{2020}\NormalTok{, }\AttributeTok{by=}\DecValTok{1}\NormalTok{), }\AttributeTok{times =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# national annual population counts}
\NormalTok{population\_count}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     population year
2000   47955683 2000
2001   47520197 2001
2002   47094225 2002
2003   46700872 2003
2004   46330322 2004
2005   46011048 2005
2006   45734099 2006
2007   45502336 2007
2008   45286748 2008
2009   45090608 2009
2010   44923112 2010
2011   44744969 2011
2012   44593427 2012
2013   44424702 2013
2014   44250993 2014
2015   44068072 2015
2016   43856852 2016
2017   43622605 2017
2018   43391259 2018
2019   43140679 2019
2020   42880388 2020
\end{verbatim}

\hypertarget{exploratory-data-analysis}{%
\subsection{Exploratory data analysis}\label{exploratory-data-analysis}}

Now we are ready to start analysing the data. Before building complexity
on our analysis, conducting some exploratory data analysis to understand
the data is generally a good starting point, particularly given the
multi-layer nature of the data at hand - capturing space, time and
population levels.

\textbf{National patterns}

We first analyse national population trends. We want to know to what
extent the population of Ukraine has declined over time over the last 20
years. An effective way to do this is to compute summary statistics and
visualise the data. Below we look at year-to-year changes in population
levels and as a percentage change. By using \texttt{patchwork}, we
combine two plots into a single figure.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# visualise national population trends}
\NormalTok{pop\_level\_p }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(population\_count, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ population}\SpecialCharTok{/}\DecValTok{1000000}\NormalTok{ )) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_tufte2}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{48}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Population }\SpecialCharTok{\textbackslash{}n}\StringTok{(million)"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Year"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
i Please use `linewidth` instead.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# visualise percentage change in population}
\NormalTok{pop\_percent\_p }\OtherTok{\textless{}{-}}\NormalTok{ population\_count }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{pct\_change =}\NormalTok{ ( ( population }\SpecialCharTok{{-}} \DecValTok{47955683}\NormalTok{) }\SpecialCharTok{/} \DecValTok{47955683}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ pct\_change )) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_tufte2}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Population }\SpecialCharTok{\textbackslash{}n}\StringTok{percentage change (\%)"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Year"}\NormalTok{) }

\NormalTok{pop\_level\_p }\SpecialCharTok{|}\NormalTok{ pop\_percent\_p}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sequence-analysis_files/figure-pdf/unnamed-chunk-13-1.pdf}

}

\end{figure}

\textbf{Sub-national}

Population losses are likely vary across the country. From previous
research we know that rural and less well connected areas tend to lose
population through the internal migration of young individuals as they
move for work and job opportunities (Rowe, Corcoran, and Bell 2016). We
also know that they tend to move to large, densely populated cities
where these opportunities are concentrated and because they also offer a
wide variety of amenities and activities. Cities tend to work as
accelarators enabling fast career development and occupational
progression (Fielding 1992). Though, we have also seen the shrinkage of
populations in cities, particularly in eastern European countries (Turok
and Mykhnenko 2007).

To examine the patterns of sun-national population losses, we compute
two summary measures: (1) annual percentage change in population; and,
(2) overall percentage change in population between 2000 and 2021. We
start by looking the overall percentage change as it is easier to
visualise. To this end, we categorise our measure of overall percentage
change into seven different classes. Based on previous work by
González-Leonardo, Newsham, and Rowe (2023), we classify changes into
high decline (\(\leq\) -3), decline (\textgreater{} -3 and \(\leq\)
-1.5), moderate decline (\textgreater{} -1.5 and \(\leq\) -0.3), stable
(\textless{} -0.3 and \textless{} 0.3), moderate growth (\(\geq\) 0.3
and \textless{} 1.5), growth (\(\geq\) 1.5 and \textless{} 3) and high
growth (\(\geq\) 3). Let's first create the measures of population
change.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# compute population change metrics}
\NormalTok{population\_df }\OtherTok{\textless{}{-}}\NormalTok{ population\_df }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(GID\_2) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{year, }\AttributeTok{.by\_group =} \ConstantTok{TRUE}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
  \AttributeTok{pct\_change =}\NormalTok{ ( population }\SpecialCharTok{/} \FunctionTok{lead}\NormalTok{(population) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\CommentTok{\# rate of population change}
  \AttributeTok{pct\_change\_2000\_21 =}\NormalTok{ ( population[year }\SpecialCharTok{==} \StringTok{"2020"}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ population[year }\SpecialCharTok{==} \StringTok{"2000"}\NormalTok{] }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\CommentTok{\# overall rate of change}
  \AttributeTok{ave\_pct\_change\_2000\_21 =} \FunctionTok{mean}\NormalTok{(pct\_change, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Let's map the overall percentage change in population between 2000 and
2020. We see a wide spread pattern of population decline across Ukraine.
We observe a large spatial cluster of high population decline across the
country with moderate population decline in some areas. Administrative
areas containing large cities seem to record overall population growth
between 2000 and 2020, potentially absorbing population movements from
the rest of the country. What else do you think may be driving
population growth in cities? And in contrast, what do you think is
contributing to population decline in most of Ukraine?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set colours}
\NormalTok{cols }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"\#7f3b08"}\NormalTok{, }\StringTok{"\#b35806"}\NormalTok{, }\StringTok{"\#e08214"}\NormalTok{, }\StringTok{"\#faf0e6"}\NormalTok{, }\StringTok{"\#8073ac"}\NormalTok{, }\StringTok{"\#542788"}\NormalTok{, }\StringTok{"\#2d004b"}\NormalTok{)}
\CommentTok{\# reverse order}
\NormalTok{cols }\OtherTok{\textless{}{-}} \FunctionTok{rev}\NormalTok{(cols)}

\NormalTok{population\_df }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{( year }\SpecialCharTok{==} \DecValTok{2020}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{drop\_na}\NormalTok{(pct\_change\_2000\_21) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{ove\_pop\_class =} \FunctionTok{case\_when}\NormalTok{( pct\_change\_2000\_21 }\SpecialCharTok{\textless{}=} \SpecialCharTok{{-}}\DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}high\_decline\textquotesingle{}}\NormalTok{,}
\NormalTok{                           pct\_change\_2000\_21 }\SpecialCharTok{\textless{}=} \SpecialCharTok{{-}}\FloatTok{1.5} \SpecialCharTok{\&}\NormalTok{ pct\_change\_2000\_21 }\SpecialCharTok{\textgreater{}} \SpecialCharTok{{-}}\DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}decline\textquotesingle{}}\NormalTok{,}
\NormalTok{                           pct\_change\_2000\_21 }\SpecialCharTok{\textless{}=} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{3} \SpecialCharTok{\&}\NormalTok{ pct\_change\_2000\_21 }\SpecialCharTok{\textgreater{}} \SpecialCharTok{{-}}\FloatTok{1.5} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}moderate\_decline\textquotesingle{}}\NormalTok{,}
\NormalTok{                           pct\_change\_2000\_21 }\SpecialCharTok{\textgreater{}} \SpecialCharTok{{-}}\FloatTok{0.3} \SpecialCharTok{\&}\NormalTok{ pct\_change\_2000\_21 }\SpecialCharTok{\textless{}} \FloatTok{0.3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}stable\textquotesingle{}}\NormalTok{,}
\NormalTok{                           pct\_change\_2000\_21 }\SpecialCharTok{\textgreater{}=} \FloatTok{0.3} \SpecialCharTok{\&}\NormalTok{ pct\_change\_2000\_21 }\SpecialCharTok{\textless{}} \FloatTok{1.5} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}moderate\_growth\textquotesingle{}}\NormalTok{,}
\NormalTok{                           pct\_change\_2000\_21 }\SpecialCharTok{\textgreater{}=} \FloatTok{1.5} \SpecialCharTok{\&}\NormalTok{ pct\_change\_2000\_21 }\SpecialCharTok{\textless{}} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}growth\textquotesingle{}}\NormalTok{,}
\NormalTok{                           pct\_change\_2000\_21 }\SpecialCharTok{\textgreater{}=} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}high\_growth\textquotesingle{}}\NormalTok{),}
    \AttributeTok{ove\_pop\_class =} \FunctionTok{factor}\NormalTok{(ove\_pop\_class, }
         \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"high\_decline"}\NormalTok{, }\StringTok{"decline"}\NormalTok{, }\StringTok{"moderate\_decline"}\NormalTok{, }\StringTok{"stable"}\NormalTok{, }\StringTok{"moderate\_growth"}\NormalTok{, }\StringTok{"growth"}\NormalTok{, }\StringTok{"high\_growth"}\NormalTok{) )}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ ove\_pop\_class)) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{col =} \StringTok{"white"}\NormalTok{, }\AttributeTok{size =}\NormalTok{ .}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =}\NormalTok{ cols,}
                    \AttributeTok{name =} \StringTok{"Population change"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_map\_tufte}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sequence-analysis_files/figure-pdf/unnamed-chunk-15-1.pdf}

}

\end{figure}

Now that we have understanding of population changes over the whole
2000-2020 period. Let's try to understand how different places arrive to
different outcomes. A way to do this is to look at the evolution of
population changes. Different trajectories of population change could
underpin the outcomes of population change that we observe today.
Current outcomes could be the result of a consistent pattern of
population decline over the last 20 years. They could be the result of
acceleration in population loss after a major natural or war event, or
they could reflect a gradual process of erosion. We visualise way to get
an understanding of this is to analyse annual percentage population
changes across individual areas. We use a Hovmöller Plot as illustrated
by Rowe and Arribas-Bel (2022) for the analysis of spatio-temporal data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population\_df }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{( ave\_pct\_change\_2000\_21 }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tail}\NormalTok{(., }\DecValTok{40}\SpecialCharTok{*}\DecValTok{21}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ., }
           \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{ year, }\AttributeTok{y=} \FunctionTok{reorder}\NormalTok{(NAME\_2, pct\_change), }\AttributeTok{fill=}\NormalTok{ pct\_change)) }\SpecialCharTok{+}
  \FunctionTok{geom\_tile}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis}\NormalTok{(}\AttributeTok{name=}\StringTok{"Population"}\NormalTok{, }\AttributeTok{option =}\StringTok{"plasma"}\NormalTok{, }\AttributeTok{begin =}\NormalTok{ .}\DecValTok{2}\NormalTok{, }\AttributeTok{end =}\NormalTok{ .}\DecValTok{8}\NormalTok{, }\AttributeTok{direction =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_tufte2}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=} \FunctionTok{paste}\NormalTok{(}\StringTok{" "}\NormalTok{), }\AttributeTok{x=}\StringTok{"Year"}\NormalTok{, }\AttributeTok{y=}\StringTok{"Area"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{14}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.y =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{8}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sequence-analysis_files/figure-pdf/unnamed-chunk-16-1.pdf}

}

\end{figure}

The Hovmöller Plot shows that most of the selected areas tend to
experience annual population decline, with varying spells of population
growth. Percentage population changes range between 1 and -2.5. We also
observe areas with consistent trajectories of annual population decline,
like Zolochivs'kyi and Barvinkivs'kyi, and areas with strong decline in
the first few years between 2000 and 2005 but moderate decline later on,
such as Novovorontsovk'kyi. Yet, Hovmöller Plots provide a limited
understanding of the annual population changes for a handful of areas at
the time and it is therefore difficult to identify systematic
representative patterns. Here we have selected 40 areas of a total of
629. Displaying the total number of areas in a Hovmöller Plot will not
produce readable results. Even if that was the case, it would be
difficult to identify systematic patterns. As we will seek to persuade
you below, sequence analysis provides a very novel way to define
representative trajectories in the data, identify systematic patterns
and extract distinctive features characterising those trajectories.

\hypertarget{application}{%
\section{Application}\label{application}}

Next, we focus on the application of sequence analysis to identify
representative trajectories of population decline at the sub-national
level between 2000 and 2020 in Ukraine. Intuitively, sequence analysis
can be seen as a four-stage process. First, it requires the definition
of longitudinal categorical outcome. Second, it measures the
dissimilarity of individual sequences via a process known as optimal
matching (OM). Third, it uses these dissimilarity measures to define a
typology of representative trajectories using unsupervised machine
learning clustering techniques. Fourth, trajectories can be visualised
and their distinctive features can be measured. Below we describe the
implementation of each stage to identify representative trajectories of
population decline.

\hypertarget{defining-outcome-process}{%
\subsection{Defining outcome process}\label{defining-outcome-process}}

Sequence analysis requires longitudinal categorical data as an input. We
therefore classify our population count data into distinct categorical
categories, henceforth referred to as \emph{states} of population
change. We compute the annual percentage rate of population change for
individual areas and use these rates to measure the extent and pace of
population change. The annual rate of population change is computed as
follows:

\[
{p(t1) - p(t0) \over p(t0)}*100
\]

where: \(p(t0)\) is the population at year t0 and \(p(t1)\) is the
population at t + 1.

As previously, we differentiate areas of high decline, decline, moderate
decline, stable, moderate growth, growth and high growth. For the
analysis, we focus on areas recording population losses between 2000 and
2020. The histogram shows the magnitude and distribution of population
decline over this period. We observe that most occurrences of decline
are moderate around zero, while very few exceed 5\%.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select areas reporting losses between 2000 and 2020}
\NormalTok{population\_loss\_df }\OtherTok{\textless{}{-}}\NormalTok{ population\_df }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{( pct\_change\_2000\_21 }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{)}
\CommentTok{\# plot distribution of percentage change }
\NormalTok{population\_loss\_df }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(pct\_change  }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{alpha=}\FloatTok{0.8}\NormalTok{, }\AttributeTok{colour=}\StringTok{"black"}\NormalTok{, }\AttributeTok{fill=}\StringTok{"lightblue"}\NormalTok{, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ pct\_change)) }\SpecialCharTok{+}
  \FunctionTok{theme\_tufte2}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sequence-analysis_files/figure-pdf/unnamed-chunk-17-1.pdf}

}

\end{figure}

Next we classify the annual percentage of population change into our
seven states.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# remove 2000 as it has no observations of population change}
\NormalTok{population\_loss\_df }\OtherTok{\textless{}{-}}\NormalTok{ population\_loss\_df }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{( year }\SpecialCharTok{!=} \DecValTok{2000}\NormalTok{)}
\CommentTok{\# clasify data}
\NormalTok{population\_loss\_df }\OtherTok{\textless{}{-}}\NormalTok{ population\_loss\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{pop\_class =} \FunctionTok{case\_when}\NormalTok{( pct\_change }\SpecialCharTok{\textless{}=} \SpecialCharTok{{-}}\DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}high\_decline\textquotesingle{}}\NormalTok{,}
\NormalTok{                           pct\_change }\SpecialCharTok{\textless{}=} \SpecialCharTok{{-}}\FloatTok{1.5} \SpecialCharTok{\&}\NormalTok{ pct\_change }\SpecialCharTok{\textgreater{}} \SpecialCharTok{{-}}\DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}decline\textquotesingle{}}\NormalTok{,}
\NormalTok{                           pct\_change }\SpecialCharTok{\textless{}=} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{3} \SpecialCharTok{\&}\NormalTok{ pct\_change }\SpecialCharTok{\textgreater{}} \SpecialCharTok{{-}}\FloatTok{1.5} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}moderate\_decline\textquotesingle{}}\NormalTok{,}
\NormalTok{                           pct\_change }\SpecialCharTok{\textgreater{}} \SpecialCharTok{{-}}\FloatTok{0.3} \SpecialCharTok{\&}\NormalTok{ pct\_change }\SpecialCharTok{\textless{}} \FloatTok{0.3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}stable\textquotesingle{}}\NormalTok{,}
\NormalTok{                           pct\_change }\SpecialCharTok{\textgreater{}=} \FloatTok{0.3} \SpecialCharTok{\&}\NormalTok{ pct\_change }\SpecialCharTok{\textless{}} \FloatTok{1.5} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}moderate\_growth\textquotesingle{}}\NormalTok{,}
\NormalTok{                           pct\_change }\SpecialCharTok{\textgreater{}=} \FloatTok{1.5} \SpecialCharTok{\&}\NormalTok{ pct\_change }\SpecialCharTok{\textless{}} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}growth\textquotesingle{}}\NormalTok{,}
\NormalTok{                           pct\_change }\SpecialCharTok{\textgreater{}=} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}high\_growth\textquotesingle{}}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{optimal-matching}{%
\subsection{Optimal matching}\label{optimal-matching}}

We measure the extent of dissimilarity between individual sequence of
population decline. To this end, we used a sequence analysis technique,
OM, which computes distances between sequences as a function of the
number of transformations required to make sequences identical. Two sets
of operations are generally used: (1) insertion/deletion (known as
indel) and (2) substitution operations. Both of these operations
represent the cost of transforming one sequence into another. These
costs are challenging to define and below we discuss what is generally
used in empirical work. Intuitively, the idea of OM is to estimate the
cost of transforming one sequence into another so that the greater the
cost to make two sequences identical, the greater the dissimilarity and
\emph{vice versa}.

\emph{Indel operations} involve the addition or removal of an element
within the sequence and substitution operations are the replacement of
one element for another. Each of these operations is assigned a cost,
and the distance between two sequences is defined as the minimum cost to
transform one sequence to another (Abbott and Tsay 2000). By default,
indel costs are set to 1. To illustrate indel operations, let's consider
an example of sequences of annual population change for three areas
during 2000 and 2003. The sequences are identical, except for 2003. In
this case, indel operations involve the cost of transforming the status
\emph{stable} in the sequence for area 1 to \emph{high decline} in the
sequence for area 2, and thus this operation would return a cost is 2.
Why 2? It is 2 because you would need to delete \emph{stable} and add
\emph{high decline}. Now, let's try the cost of transforming the status
\emph{stable} in the sequence for area 1 to the status in the sequence
for area 3 using indel operations. What is the cost? The answer is 1
because we only need to delete \emph{stable} to make it identical.

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Area & 2000 & 2001 & 2002 & 2003 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & decline & decline & decline & stable \\
2 & decline & decline & decline & high decline \\
3 & decline & decline & decline & - \\
\end{longtable}

\emph{Substitution operations} or costs represent transition costs; that
is, the cost for substituting each state with another. Substitution
costs are defined in one of two ways (Salmela-Aro et al. 2011). One
approach is the theory-driven approach. In such approach, substitution
costs are grounded in theory suggesting that, for example, transforming
state 1 to state 2 should have a greater cost than transforming state 1
to state 3, or performing the opposite operation i.e.~transforming state
2 to state 1. An example could be that it is more financially costly to
transition from \emph{full-time employment} to \emph{full-time
education} than transition from \emph{full-time education} to
\emph{full-time employment}.

A second approach and most commonly used in empirical work is a
data-driven approach. In this approach, substitution costs are
empirically derived from transition rates between states. The cost of
substitution is inversely related to the frequency of observed
transitions within the data. This means that infrequent transitions
between states have a higher substitution cost. For example, as we will
see, transitions from the state of high decline to high growth are rarer
than from high growth to high decline in Ukraine. The transition rate
between state \(i\) and state \(j\) is the probability of observing
state \(j\) at time \(t1\) given that the state \(i\) is observed at
time \(t\) for \(i \neq j\). The substitution cost between states \(i\)
and \(j\) is computed as:

\[
2 - {p(i | j) - p(j | i)}
\] where \(p(i | j)\) is the transition rate between state \(i\) and
\(j\).

To implement OM, we first need to rearrange the structure of our data
from long to wide format. You can now see now that individual rows
represent areas (column 1) and columns from 2 to 21 represent years.

\begin{itemize}
\tightlist
\item
  see Rowe and Arribas-Bel (2022) for a description on different
  spatio-temporal data structures and their manipulation using tidyverse
  principles.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# transform from long to wide format}
\NormalTok{wide\_population\_loss\_df }\OtherTok{\textless{}{-}}\NormalTok{ population\_loss\_df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(GID\_2) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(year, }\AttributeTok{.by\_group =} \ConstantTok{TRUE}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(}
  \AttributeTok{id\_cols =}\NormalTok{  GID\_2,}
  \AttributeTok{names\_from =} \StringTok{"year"}\NormalTok{,}
  \AttributeTok{values\_from =} \StringTok{"pop\_class"}
\NormalTok{)}
  
\NormalTok{wide\_population\_loss\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 571 x 21
   GID_2   `2001` `2002` `2003` `2004` `2005` `2006` `2007` `2008` `2009` `2010`
   <chr>   <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr>  <chr> 
 1 UKR.1.~ decli~ moder~ decli~ decli~ decli~ moder~ moder~ moder~ moder~ moder~
 2 UKR.1.~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~
 3 UKR.1.~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~
 4 UKR.1.~ decli~ decli~ moder~ decli~ moder~ moder~ moder~ moder~ moder~ moder~
 5 UKR.1.~ decli~ decli~ moder~ decli~ moder~ moder~ moder~ moder~ moder~ moder~
 6 UKR.1.~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~
 7 UKR.1.~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~
 8 UKR.1.~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~ moder~
 9 UKR.1.~ decli~ decli~ decli~ decli~ decli~ decli~ decli~ decli~ moder~ decli~
10 UKR.1.~ moder~ stable moder~ moder~ stable moder~ moder~ stable decli~ stable
# i 561 more rows
# i 10 more variables: `2011` <chr>, `2012` <chr>, `2013` <chr>, `2014` <chr>,
#   `2015` <chr>, `2016` <chr>, `2017` <chr>, `2018` <chr>, `2019` <chr>,
#   `2020` <chr>
\end{verbatim}

Once the data frame has been reshaped into a wide format, we define the
data as a state sequence object using the R package \texttt{TraMineR}.
Key here is to appropriately define the labels and an appropriate
palette of colours. Depending on the patterns you are seeking to capture
a diverging, sequential or qualitative colour palette may be more
appropriate. For this chapter, we use a diverging colour palette as we
want to effectively represent areas experiencing diverging patterns of
population decline or growth.

\begin{quote}
Note: various types of sequence data representation exist in
\texttt{TraMineR}. These representations vary in the way they capture
states or events. Chapter 4 in Gabadinho et al. (2009) describes the
various representations that \texttt{TraMineR} can handle. In any case,
the state sequence representation used in this chapter is the most
commonly used and internal format used by \texttt{TraMineR}. Hence we
focus on it.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# alphabet}
\NormalTok{seq.alphab }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"high\_growth"}\NormalTok{, }\StringTok{"growth"}\NormalTok{, }\StringTok{"moderate\_growth"}\NormalTok{, }\StringTok{"stable"}\NormalTok{, }\StringTok{"moderate\_decline"}\NormalTok{, }\StringTok{"decline"}\NormalTok{, }\StringTok{"high\_decline"}\NormalTok{)}
\CommentTok{\# labels}
\NormalTok{seq.lab }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"High growth"}\NormalTok{, }\StringTok{"Growth"}\NormalTok{, }\StringTok{"Moderate growth"}\NormalTok{, }\StringTok{"Stable"}\NormalTok{, }\StringTok{"Moderate decline"}\NormalTok{, }\StringTok{"Decline"}\NormalTok{, }\StringTok{"High decline"}\NormalTok{)}
\CommentTok{\# define state sequence object}
\NormalTok{seq.cl }\OtherTok{\textless{}{-}} \FunctionTok{seqdef}\NormalTok{(wide\_population\_loss\_df, }
                 \DecValTok{2}\SpecialCharTok{:}\DecValTok{21}\NormalTok{, }
                 \AttributeTok{alphabet =}\NormalTok{ seq.alphab,}
                 \AttributeTok{labels =}\NormalTok{ seq.lab,}
                 \AttributeTok{cnames =} \FunctionTok{c}\NormalTok{(}\StringTok{"2001"}\NormalTok{, }\StringTok{"2002"}\NormalTok{, }\StringTok{"2003"}\NormalTok{, }\StringTok{"2004"}\NormalTok{, }\StringTok{"2005"}\NormalTok{, }\StringTok{"2006"}\NormalTok{, }\StringTok{"2007"}\NormalTok{, }\StringTok{"2008"}\NormalTok{, }\StringTok{"2009"}\NormalTok{, }\StringTok{"2010"}\NormalTok{, }\StringTok{"2011"}\NormalTok{, }\StringTok{"2012"}\NormalTok{, }\StringTok{"2013"}\NormalTok{, }\StringTok{"2014"}\NormalTok{, }\StringTok{"2015"}\NormalTok{, }\StringTok{"2016"}\NormalTok{, }\StringTok{"2017"}\NormalTok{, }\StringTok{"2018"}\NormalTok{, }\StringTok{"2019"}\NormalTok{, }\StringTok{"2020"}\NormalTok{),}
                 \AttributeTok{cpal =}\FunctionTok{c}\NormalTok{(}\StringTok{"1"} \OtherTok{=} \StringTok{"\#7f3b08"}\NormalTok{, }
                         \StringTok{"2"} \OtherTok{=} \StringTok{"\#b35806"}\NormalTok{,}
                         \StringTok{"3"} \OtherTok{=} \StringTok{"\#e08214"}\NormalTok{,}
                         \StringTok{"4"} \OtherTok{=} \StringTok{"\#faf0e6"}\NormalTok{,}
                         \StringTok{"5"} \OtherTok{=} \StringTok{"\#8073ac"}\NormalTok{,}
                         \StringTok{"6"} \OtherTok{=} \StringTok{"\#542788"}\NormalTok{,}
                         \StringTok{"7"} \OtherTok{=} \StringTok{"\#2d004b"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [>] 7 distinct states appear in the data: 
\end{verbatim}

\begin{verbatim}
     1 = decline
\end{verbatim}

\begin{verbatim}
     2 = growth
\end{verbatim}

\begin{verbatim}
     3 = high_decline
\end{verbatim}

\begin{verbatim}
     4 = high_growth
\end{verbatim}

\begin{verbatim}
     5 = moderate_decline
\end{verbatim}

\begin{verbatim}
     6 = moderate_growth
\end{verbatim}

\begin{verbatim}
     7 = stable
\end{verbatim}

\begin{verbatim}
 [>] state coding:
\end{verbatim}

\begin{verbatim}
       [alphabet]       [label]          [long label] 
\end{verbatim}

\begin{verbatim}
     1  high_growth      high_growth      High growth
\end{verbatim}

\begin{verbatim}
     2  growth           growth           Growth
\end{verbatim}

\begin{verbatim}
     3  moderate_growth  moderate_growth  Moderate growth
\end{verbatim}

\begin{verbatim}
     4  stable           stable           Stable
\end{verbatim}

\begin{verbatim}
     5  moderate_decline moderate_decline Moderate decline
\end{verbatim}

\begin{verbatim}
     6  decline          decline          Decline
\end{verbatim}

\begin{verbatim}
     7  high_decline     high_decline     High decline
\end{verbatim}

\begin{verbatim}
 [>] 571 sequences in the data set
\end{verbatim}

\begin{verbatim}
 [>] min/max sequence length: 20/20
\end{verbatim}

Using the sequence data object, we create a state distribution plot to
get an understanding of the data. The plot shows the distribution of
areas across status of population change in individual years. The
overall picture emerging from the plot is an overall pattern of
population decline between 2000 and 2020, predominantly moderate decline
and limited spells of high population decline or growth. This aligns
with the predominant trajectory of population decline observed in Ukrain
based on more aggregate data at the regional level (Newsham and Rowe
2022a).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{seqplot}\NormalTok{(seq.cl, }
        \AttributeTok{title=}\StringTok{"State distribution plot"}\NormalTok{, }
        \AttributeTok{type =} \StringTok{"d"}\NormalTok{,}
        \AttributeTok{with.legend =} \StringTok{"right"}\NormalTok{,}
        \AttributeTok{border =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sequence-analysis_files/figure-pdf/unnamed-chunk-21-1.pdf}

}

\end{figure}

We now move on to compute the substitution costs for our population
states. From the equation above, you may have realised that transition
costs vary between 0 and 2, with the former indicating zero cost. The
latter indicates the maximum cost of converting one state into another.
The option \texttt{TRATE} in method states to derive costs from observed
transition rates. That is the data-driven approach discussed above. From
the matrix below, you can see that it is more costly to convert a status
``high growth'' to ``moderate decline'' than from ``growth'' to
``moderate''. This makes sense. We expect gradual changes in population
along a given trajectory if they were to occur due to natural causes.

\begin{quote}
Note we are considering a fixed measure of transition rates. That means
that we are using the whole dataset to compute an average transition
rate between states. That assumes that the rate of change between states
does not change over time. Yet, there may be good reasons to believe
they do as areas move across different states. In empirical work, time
varying transition rates are more often considered. That means we use
temporal slices of the data to compute transition rates; for example,
using data from 2001, 2002 and so on. In this way, we end up with
potentially different transition rates for every year.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate transition rates}
\NormalTok{subs\_costs }\OtherTok{\textless{}{-}} \FunctionTok{seqsubm}\NormalTok{(seq.cl, }
                      \AttributeTok{method =} \StringTok{"TRATE"}\NormalTok{,}
                      \CommentTok{\#time.varying = TRUE}
\NormalTok{                      )}

\NormalTok{subs\_costs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                 high_growth   growth moderate_growth   stable moderate_decline
high_growth         0.000000 1.975000        1.940066 1.949203         1.949277
growth              1.975000 0.000000        1.867730 1.867946         1.634772
moderate_growth     1.940066 1.867730        0.000000 1.709425         1.547832
stable              1.949203 1.867946        1.709425 0.000000         1.501723
moderate_decline    1.949277 1.634772        1.547832 1.501723         0.000000
decline             1.847681 1.848052        1.826810 1.915740         1.643018
high_decline        1.288462 1.700000        1.773408 1.917223         1.805088
                  decline high_decline
high_growth      1.847681     1.288462
growth           1.848052     1.700000
moderate_growth  1.826810     1.773408
stable           1.915740     1.917223
moderate_decline 1.643018     1.805088
decline          0.000000     1.844570
high_decline     1.844570     0.000000
\end{verbatim}

To understand better the idea of substitution costs, we can have direct
look at transition rates underpinning these costs. Transition rates can
be computed via \texttt{seqtrate} . By definition, transition rates vary
between 0 and 1, with zero indicating no probability of a transition
occurring. One indicates a 100\% probability of a transition taking
place. Thus, for example, the matrix below tell us that there is a 5\%
probability of observing a transition from ``high growth'' to ``moderate
decline'' in our sample. Examining transition rates could provide very
valuable information about the process in analysis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seq.trate }\OtherTok{\textless{}{-}} \FunctionTok{seqtrate}\NormalTok{(seq.cl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [>] computing transition probabilities for states high_growth/growth/moderate_growth/stable/moderate_decline/decline/high_decline ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(seq.trate, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                      [-> high_growth] [-> growth] [-> moderate_growth]
[high_growth ->]                  0.12        0.03                 0.05
[growth ->]                       0.00        0.05                 0.11
[moderate_growth ->]              0.01        0.02                 0.09
[stable ->]                       0.00        0.00                 0.04
[moderate_decline ->]             0.00        0.00                 0.02
[decline ->]                      0.00        0.01                 0.02
[high_decline ->]                 0.16        0.10                 0.18
                      [-> stable] [-> moderate_decline] [-> decline]
[high_growth ->]             0.05                  0.05         0.15
[growth ->]                  0.13                  0.36         0.15
[moderate_growth ->]         0.25                  0.43         0.15
[stable ->]                  0.46                  0.42         0.05
[moderate_decline ->]        0.08                  0.82         0.08
[decline ->]                 0.03                  0.28         0.65
[high_decline ->]            0.07                  0.19         0.15
                      [-> high_decline]
[high_growth ->]                   0.55
[growth ->]                        0.20
[moderate_growth ->]               0.05
[stable ->]                        0.01
[moderate_decline ->]              0.00
[decline ->]                       0.01
[high_decline ->]                  0.15
\end{verbatim}

Now we focus on the probably most important component of sequence
analysis; that is, the calculation of dissimilarity. Recall our aim is
to identify representative trajectories. To this end, we need a way to
measure how similar or different sequences are - which is known as OM.
Above, we described that we can use indel and substitution operations to
measure the dissimilarity or costs between individual sequences. The
code chunk implements OM based on indel and substitution operations. The
algorithm takes an individual sequence and compares it with all of the
sequences in the dataset, and identifies the sequence with the minimum
cost i.e.~the most similar sequence. The result of this computing
intensive process is a distance matrix encoding the similarity or
dissimilarity between individual sequences.

\begin{quote}
For indel, \texttt{auto} sets the indel as \texttt{max(sm)/2} when sm is
a matrix. For more details, run \texttt{?seqdist} on your console
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate a distance matrix}
\NormalTok{seq.om }\OtherTok{\textless{}{-}} \FunctionTok{seqdist}\NormalTok{(seq.cl,}
                  \AttributeTok{method =} \StringTok{"OM"}\NormalTok{, }\CommentTok{\# specify the method}
                  \AttributeTok{indel =} \StringTok{"auto"}\NormalTok{, }\CommentTok{\# specify indel costs}
                  \AttributeTok{sm =}\NormalTok{ subs\_costs) }\CommentTok{\# specify substitution costs}
\end{Highlighting}
\end{Shaded}

As highlighted above, if you would like to apply varying substitution
costs, you can do this directly here by using the option
\texttt{method\ =\ DHD} .

\hypertarget{clustering}{%
\subsection{Clustering}\label{clustering}}

The resulting distance matrix from OM \texttt{seq.om} indicates the
degree of similarity between individual sequences. To identify
representative trajectories, we then need to a way to group together
similar sequences to produce a typology, in this case of population
decline trajectories. Unsupervised cluster analysis is generally used
for this task. Trusting you have built an understanding of cluster
analysis from the previous chapter, we will not provide an elaborate
description here. If you would like to know more about cluster analysis,
we recommend the introductory book by Kaufman and Rousseeuw (2009). We
use a clustering method called \texttt{k-meloids} . This methods is
known to be more robust to noise and outliers than the conventional
k-means procedure (Backman, Lopez, and Rowe 2020). This is because the
medoid algorithm clusters the data by minimising a sum of pair-wise
dissimilarities (Kaufman and Rousseeuw 2009), rather than a sum of
squared Euclidean distances. We run cluster analyses at different
numbers of \texttt{k} starting from 2 to 20.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# run PAMs}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\DecValTok{20}\NormalTok{)}
\NormalTok{  pam\_sol }\OtherTok{\textless{}{-}} \FunctionTok{pam}\NormalTok{(seq.om, k)}
\end{Highlighting}
\end{Shaded}

We then seek to determine the optimal number of clusters \texttt{k}. We
use silhouette scores, but as we noted Chapter~\ref{sec-chp3}, the
optimal number of clusters is better determined by the user given the
context and use case. It is an art. There is no wrong or right answer.
As can be seen from the results below from the average silhouette score,
two clusters is suggested as the optimal solution. However, we could
argue that we gain very little from such coarse partition of the data.
We suggest to take this as guidance and a starting point to look to
identify an appropriate data partition. We suggest to visualise
different solution and gain an understanding of what data get split and
decide on whether the resulting patterns contribute to the understanding
of the process at hand.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# compute average silhouette scores for all 20 cluster solutions}
\NormalTok{asw }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\DecValTok{20}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\DecValTok{20}\NormalTok{)}
\NormalTok{  asw[k] }\OtherTok{\textless{}{-}} \FunctionTok{pam}\NormalTok{(seq.om, k) }\SpecialCharTok{$}\NormalTok{ silinfo }\SpecialCharTok{$}\NormalTok{ avg.width}
\NormalTok{  k.best }\OtherTok{\textless{}{-}} \FunctionTok{which.max}\NormalTok{(asw)}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"silhouette{-}optimal number of clusters:"}\NormalTok{, k.best, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
silhouette-optimal number of clusters: 2 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  asw}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 0.0000000 0.5729062 0.5363097 0.4923203 0.4629785 0.4425575 0.4417958
 [8] 0.4559577 0.4597152 0.4569222 0.4682526 0.4832332 0.4359832 0.4365490
[15] 0.4249453 0.4193369 0.4317815 0.4323037 0.4333972 0.4463586
\end{verbatim}

We rerun and save the results for a 7k cluster partition. If you inspect
the resulting data frame, it provides an identifier for each cluster.
Each individual area is attributed to a cluster. Next the question that
we seek to answer is what sort of pattern do these clusters capture?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# rerun pam for k=7}
\NormalTok{pam\_optimal }\OtherTok{\textless{}{-}} \FunctionTok{pam}\NormalTok{(seq.om, }\DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualising}{%
\subsection{Visualising}\label{visualising}}

To understand the representative patterns captured in our data
partition, we use visualisation. There is a battery of different
visualisation tools to extract information and identify distinctive
features of the identified trajectories. We start by using
\emph{individual sequence plots} by trajectory type. They provide a
visual representation of how individual areas in each trajectory type
moves between states. Recall that we are capturing representative
trajectories; hence, there is still quite a bit of variability in terms
of the patterns encapsulated in each representative trajectory. Back to
the individual sequence plots, each line in these plots represents an
area. Time is displayed horizontally and colours encode different states
- in our case of population change. Numbers on the y-axis display the
number of areas in each cluster. The figure immediate below relies on
the base \texttt{plot} library, and by default, it is not very visually
appealing.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create individual sequence plots}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\FunctionTok{seqplot}\NormalTok{(seq.cl, }
        \AttributeTok{group =}\NormalTok{ pam\_optimal}\SpecialCharTok{$}\NormalTok{clustering,}
        \AttributeTok{type =} \StringTok{"I"}\NormalTok{,}
        \AttributeTok{border =} \ConstantTok{NA}\NormalTok{, }
        \AttributeTok{cex.axis =} \FloatTok{1.5}\NormalTok{, }
        \AttributeTok{cex.lab =} \FloatTok{1.5}\NormalTok{,}
        \AttributeTok{sortv =}\NormalTok{ seq.om)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sequence-analysis_files/figure-pdf/unnamed-chunk-28-1.pdf}

}

\end{figure}

We therefore switch to the R library \texttt{ggseqplot} which enables
visualisation of sequence data based on \texttt{ggplot} functionalities.
This package may provide more flexibility if we are more familiar with
\texttt{ggplot}.

The figure below offers a clear representation of the systematic
sequencing of states that each trajectory captures. It provides
information on two key features of trajectories: sequencing and size.
For example, trajectory 1 seems to capture a sequencing pattern of
transitions from moderate population decline to stability and back to
moderate decline. Trajectory 2 shows a pattern high population decline
during the first few years and then consistent moderate decline.
Trajectory 3 displays a predominant pattern of moderate population
decline. Trajectory 4 represents patterns of areas experiencing decline
with spells of high population decline. Trajectory 5 shows a pattern of
decline in the first few years followed by moderate decline and decline
again. Trajectory 6 shows a similar pattern with more prevalent spells
of population decline across the entire period. Trajectory 7 displays a
trend of temporary decline, with spells of population growth and
stability. From these plots, you can also identify which trajectories
tend to be more common. In our example, trajectory and 3 accounts the
largest number of areas: 189.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create individual sequence plots based on ggplot}
\FunctionTok{ggseqiplot}\NormalTok{(seq.cl, }
        \AttributeTok{group =}\NormalTok{ pam\_optimal}\SpecialCharTok{$}\NormalTok{clustering,}
        \AttributeTok{sortv =}\NormalTok{ seq.om,}
        \AttributeTok{facet\_ncol =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{rev}\NormalTok{(cols)) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{rev}\NormalTok{(cols)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Scale for fill is already present.
Adding another scale for fill, which will replace the existing scale.
Scale for colour is already present.
Adding another scale for colour, which will replace the existing scale.
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{sequence-analysis_files/figure-pdf/unnamed-chunk-29-1.pdf}

}

\end{figure}

We can also get a better understanding of the resulting trajectories by
analysing \emph{state frequency plots}. They show the number of
occurrences of a given state in individual years. These plots examine
the data from a vertical perspective i.e.~looking at individual years
across areas, rather than at individual areas over time. State frequency
plots reveal that predominant states in each year and changes in their
prevalence. Focusing on trajectory 1, for example, we observe that
moderate decline was the predominant state between 2000 and 2007 and
stability became equally prevalent during 2008 and 2015.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create state frequency plots based on ggplot}
\FunctionTok{ggseqdplot}\NormalTok{(seq.cl, }
        \AttributeTok{group =}\NormalTok{ pam\_optimal}\SpecialCharTok{$}\NormalTok{clustering,}
        \AttributeTok{facet\_ncol =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =}\NormalTok{ cols) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =}\NormalTok{ cols) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Scale for fill is already present.
Adding another scale for fill, which will replace the existing scale.
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{sequence-analysis_files/figure-pdf/unnamed-chunk-30-1.pdf}

}

\end{figure}

We can also examine time spent in individual states in each trajectory.
\emph{Time spent plots} report the average time spent in each state. The
measure of time depends on the original data used in the analysis. We
use years so the y-axis refers to the average number of years that a
given status appears in a representative trajectory type. For example, a
score over 5 for stable in trajectory 1 indicates that the average
number of years that areas in that typology are classified in that
category is over 5.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create time spent plots based on ggplot}
\FunctionTok{ggseqmtplot}\NormalTok{(seq.cl, }
        \AttributeTok{group =}\NormalTok{ pam\_optimal}\SpecialCharTok{$}\NormalTok{clustering,}
        \AttributeTok{facet\_ncol =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{rev}\NormalTok{(cols)) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{rev}\NormalTok{(cols)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_discrete}\NormalTok{(}\AttributeTok{labels=}\FunctionTok{c}\NormalTok{(}\StringTok{"high\_growth"} \OtherTok{=} \StringTok{"HG"}\NormalTok{, }\StringTok{"growth"} \OtherTok{=} \StringTok{"G"}\NormalTok{,}
                              \StringTok{"moderate\_growth"} \OtherTok{=} \StringTok{"MG"}\NormalTok{, }\StringTok{"stable"} \OtherTok{=} \StringTok{"S"}\NormalTok{, }\StringTok{"moderate\_decline"} \OtherTok{=} \StringTok{"MD"}\NormalTok{, }\StringTok{"decline"} \OtherTok{=} \StringTok{"D"}\NormalTok{, }\StringTok{"high\_decline"} \OtherTok{=} \StringTok{"HD"}\NormalTok{ ))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Scale for fill is already present.
Adding another scale for fill, which will replace the existing scale.
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{sequence-analysis_files/figure-pdf/unnamed-chunk-31-1.pdf}

}

\end{figure}

Finally we analyse \emph{entropy index plots}. Entropy is a measure of
diversity. The greater the score, the greater the entropy or diversity
of states. The plot below displays the entropy index computed for
individual trajectories each year. Each line represents the entropy
index for a trajectory in each year. The top yellow line in 2001
indicates that in 2001 areas following a trajectory 7 type were
distributed across a larger number of states than any other trajectory,
reflecting the fact that areas experiencing this trajectory moves
between states of decline, stability and growth. In other word, it
indicates that there was more diversity of states.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create entropy index plots based on ggplot}
\FunctionTok{ggseqeplot}\NormalTok{(seq.cl, }
        \AttributeTok{group =}\NormalTok{ pam\_optimal}\SpecialCharTok{$}\NormalTok{clustering) }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_viridis\_d}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Scale for colour is already present.
Adding another scale for colour, which will replace the existing scale.
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{sequence-analysis_files/figure-pdf/unnamed-chunk-32-1.pdf}

}

\end{figure}

\hypertarget{questions-1}{%
\section{Questions}\label{questions-1}}

For the first assignment, we will continue to focus on London as our
area of analysis. We will use population count estimates from the Office
of National Statistics (ONS). The dataset provides information on area,
population numbers and population density at national, regional and
smaller sub-national area level, including Unitary Authority,
Metropolitan County, Metropolitan District, County, Non-metropolitan
District, London Borough, Council Area and Local Government District for
the period from 2001 to 2020.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pop\_df }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"./data/sequence{-}analysis/population\_uk/population{-}uk{-}2011\_20.csv"}\NormalTok{)}
\NormalTok{pop\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 420 x 44
   Code      Name              Geography   `Area (sq km)` Estimated population~1
   <chr>     <chr>             <chr>                <dbl>                  <dbl>
 1 K02000001 UNITED KINGDOM    Country           242741.                67081234
 2 K03000001 GREAT BRITAIN     Country           228948.                65185724
 3 K04000001 ENGLAND AND WALES Country           151047.                59719724
 4 E92000001 ENGLAND           Country           130310.                56550138
 5 E12000001 NORTH EAST        Region              8581.                 2680763
 6 E06000047 County Durham     Unitary Au~         2226.                  533149
 7 E06000005 Darlington        Unitary Au~          197.                  107402
 8 E06000001 Hartlepool        Unitary Au~           93.7                  93836
 9 E06000002 Middlesbrough     Unitary Au~           53.9                 141285
10 E06000057 Northumberland    Unitary Au~         5020.                  323820
# i 410 more rows
# i abbreviated name: 1: `Estimated population mid-2020`
# i 39 more variables: `2020 people per sq. km` <dbl>,
#   `Estimated Population mid-2019` <dbl>, `2019 people per sq. km` <dbl>,
#   `Estimated Population mid-2018` <dbl>, `2018 people per sq. km` <dbl>,
#   `Estimated Population mid-2017` <dbl>, `2017 people per sq. km` <dbl>,
#   `Estimated Population mid-2016` <dbl>, `2016 people per sq. km` <dbl>, ...
\end{verbatim}

For the assignment, you should only work with smaller sub-national
areas. Filter out country and regional area. You should address the
following questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Use sequence analysis to identify representative trajectories of
  population change and discuss the type of trajectories identified in
  London Boroughs.
\item
  Use individual sequence plot to identify distinctive features in the
  resulting trajectories.
\end{enumerate}

For the analysis, aim to focus on the area of London so you can link
your narrative to the rest of analyses you will be conducting.

Ensure you justify the number of optimal clusters you will use in your
analysis and provide a brief description of the trajectories identified.
Describe how they are unique.

\bookmarksetup{startatroot}

\hypertarget{sec-chp5}{%
\chapter{Network Analysis}\label{sec-chp5}}

Network Analysis is an interdisciplinary analytical framework which
studies the structure, behavior, and dynamics of the connections between
different elements. Specific applications range from the analysis of
social links between people to the analysis of transportation and
migration links between places.

In particular, network analysis has proven to be highly effective for
gaining insights about the behavior of populations, and the spatial
distribution of people and resources (Prieto Curiel, Cabrera-Arnau, and
Bishop 2022). By examining the networks of human interactions,
researchers can gain a better understanding of how social, economic, and
cultural factors influence individual behavior, and how patterns of
migration and communication can shape the development of communities,
regions and cities (Cabrera-Arnau et al. 2022). By the end of this
session, you should be able to describe some of the most fundamental
concepts and tools for network analysis. You should also be able to
understand the potential of this approach to revolutionise the way in
which we study populations.

The chapter is based on the following references:

\begin{itemize}
\item
  Network analysis with R and igraph: NetSci X Tutorial (Ognyanova
  2016).
\item
  \href{https://igraph.org/}{igraph} - The network analysis package.
\end{itemize}

The book Networks by Newman (2018) is an excellent resource to learn
more about network analysis, which covers the basics but also the more
advanced concepts and methods.

\hypertarget{sec-sec_dependencies}{%
\section{Dependencies}\label{sec-sec_dependencies}}

To run the code in the rest of this workbook, we will need to load the
following R packages:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Support for simple features, a standardised way to encode spatial vector data}
\FunctionTok{library}\NormalTok{(sf)}
\CommentTok{\#Data manipulation}
\FunctionTok{library}\NormalTok{(dplyr)}
\CommentTok{\# An R package for network manipulation and analysis}
\FunctionTok{library}\NormalTok{(igraph)}
\CommentTok{\# Provides a number of useful functions for working with character strings in R}
\FunctionTok{library}\NormalTok{(stringr)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-sec_data}{%
\section{Data}\label{sec-sec_data}}

\hypertarget{the-us-census-dataset}{%
\subsection{The US Census dataset}\label{the-us-census-dataset}}

In this Chapter we will be looking at data provided by
\href{https://www.census.gov/data/tables/2019/demo/geographic-mobility/metro-to-metro-migration.html}{US
Census Bureau}. In particular, we have prepared the file
metro\_to\_metro\_2015\_2019\_US\_migration.csv, which contains
migration data between the US Metropolitan Statistical Areas (MSAs) in
two consecutive years. The data is based on the 2015-2019 American
Community Survey (ACS) estimates. For more information on the
methodology for the data collection process, visit this
\href{https://www.census.gov/programs-surveys/acs/methodology.html}{link}.

\hypertarget{import-the-data-1}{%
\subsection{Import the data}\label{import-the-data-1}}

Before importing the data, ensure to set the path to the directory where
you stored it. Please modify the following line of code as needed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"./data/networks/metro\_to\_metro\_2015\_2019\_US\_migration.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let us do a few changes in the names of the fields of the dataset so we
can later manipulate them more easily.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Ensure the MSA code is imported as a character and not as a number}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{MSA\_Current\_Code }\OtherTok{\textless{}{-}} \FunctionTok{as.character}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{MSA\_Current\_Code) }

\CommentTok{\#Include an additional column with the full name of the MSA in the format: Name, State}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{MSA\_Previous\_Name\_State }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{MSA\_Previous\_Name, }\StringTok{\textquotesingle{}, \textquotesingle{}}\NormalTok{, df}\SpecialCharTok{$}\NormalTok{MSA\_Previous\_State)}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{MSA\_Current\_Name\_State }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{MSA\_Current\_Name, }\StringTok{\textquotesingle{}, \textquotesingle{}}\NormalTok{, df}\SpecialCharTok{$}\NormalTok{MSA\_Current\_State)}
\end{Highlighting}
\end{Shaded}

Each row corresponds to an origin-destination pair, so we can obtain the
total number of reported migratory movements with the following command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nrow}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 52930
\end{verbatim}

\hypertarget{sec-sec_create}{%
\section{Creating networks}\label{sec-sec_create}}

Before we start to analyse the data introduced in
Section~\ref{sec-sec_data}, let us first take a step back to consider
the main object of study of this Chapter: the so-called networks. In the
most general sense, a \textbf{network} (also known as a graph) is a
structure formed by a set of objects which may have some connections
between them. The objects are represented by \textbf{nodes} (a.k.a.
vertices) and the connections between these objects are represented by
\textbf{edges} (a.k.a. links). Networks are used as a tool to
conceptualise many real-life contexts, such as the friendships between
the members of a year group at school, the direct airline connections
between cities in a continent or the presence of hyperlinks between a
set of websites. In this session, we will use networks to model the
migratory flows between US cities.

\hypertarget{starting-from-the-basics}{%
\subsection{Starting from the basics}\label{starting-from-the-basics}}

In order to create, manipulate and analyse networks in R, we will use
the igraph package, which we imported in Section~\ref{sec-sec_data}. We
start by creating a very simple network with the code below. The network
contains five nodes and five edges and it is undirected, so the edges do
not have orientations. The nodes and edges could represent,
respectively, a set of cities and the presence of migration flows
between these cities in two consecutive years.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create an undirected network with 5 nodes and 5 edges}
\CommentTok{\# The number of nodes is given by argument n}
\CommentTok{\# In this case, the node labels or IDs are represented by numbers 1 to 5}
\CommentTok{\# The edges are specified as a list of pairs of nodes}
\NormalTok{g1 }\OtherTok{\textless{}{-}} \FunctionTok{graph}\NormalTok{( }\AttributeTok{edges=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{), }\AttributeTok{n=}\DecValTok{5}\NormalTok{, }\AttributeTok{directed=}\NormalTok{F ) }

\CommentTok{\# A simple plot of the network allows us to visualise it}
\FunctionTok{plot}\NormalTok{(g1) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\end{figure}

If the connections between the nodes of a network are non-reciprocal,
the network is called directed. For example, this could correspond to a
situation where there are people moving from city 1 to city 2, but
nobody moving from city 2 to city 1. Note that in the code below we have
not only added directions to the edges, but we have also added a few
additional parameters to the plot function in order to customise the
diagram.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Creates a directed network with 7 nodes and 6 edges }
\CommentTok{\# Note that we now have edge 1,4 and edge 4,1 and that 2 of the nodes are isolated}
\NormalTok{g2 }\OtherTok{\textless{}{-}} \FunctionTok{graph}\NormalTok{( }\AttributeTok{edges=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{), }\AttributeTok{n=}\DecValTok{7}\NormalTok{, }\AttributeTok{directed=}\NormalTok{T ) }

\CommentTok{\# A simple plot of the network with a few extra features}
\FunctionTok{plot}\NormalTok{(g2, }\AttributeTok{vertex.frame.color=}\StringTok{"red"}\NormalTok{,  }\AttributeTok{vertex.label.color=}\StringTok{"black"}\NormalTok{,}
\AttributeTok{vertex.label.cex=}\FloatTok{0.9}\NormalTok{, }\AttributeTok{vertex.label.dist=}\FloatTok{2.3}\NormalTok{, }\AttributeTok{edge.curved=}\FloatTok{0.3}\NormalTok{, }\AttributeTok{edge.arrow.size=}\NormalTok{.}\DecValTok{5}\NormalTok{, }\AttributeTok{edge.color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{vertex.color=}\StringTok{"yellow"}\NormalTok{, }\AttributeTok{vertex.size=}\DecValTok{15}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-6-1.pdf}

}

\end{figure}

The network can also be defined as a list containing pairs of named
nodes. Then, it is not necessary to specify the number of nodes but the
isolated nodes have to be included. The following code generates a
network which is equivalent to the one above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g3 }\OtherTok{\textless{}{-}} \FunctionTok{graph}\NormalTok{( }\FunctionTok{c}\NormalTok{(}\StringTok{"City 1"}\NormalTok{,}\StringTok{"City 2"}\NormalTok{, }\StringTok{"City 2"}\NormalTok{,}\StringTok{"City 3"}\NormalTok{, }\StringTok{"City 1"}\NormalTok{,}\StringTok{"City 4"}\NormalTok{,  }\StringTok{"City 4"}\NormalTok{,}\StringTok{"City 1"}\NormalTok{,  }\StringTok{"City 4"}\NormalTok{,}\StringTok{"City 2"}\NormalTok{, }\StringTok{"City 4"}\NormalTok{,}\StringTok{"City 5"}\NormalTok{), }\AttributeTok{isolates=}\FunctionTok{c}\NormalTok{(}\StringTok{"City 6"}\NormalTok{, }\StringTok{"City 7"}\NormalTok{) ) }
\FunctionTok{plot}\NormalTok{(g3, }\AttributeTok{vertex.frame.color=}\StringTok{"red"}\NormalTok{,  }\AttributeTok{vertex.label.color=}\StringTok{"black"}\NormalTok{,}
\AttributeTok{vertex.label.cex=}\FloatTok{0.9}\NormalTok{, }\AttributeTok{vertex.label.dist=}\FloatTok{2.3}\NormalTok{, }\AttributeTok{edge.curved=}\FloatTok{0.3}\NormalTok{, }\AttributeTok{edge.arrow.size=}\NormalTok{.}\DecValTok{5}\NormalTok{, }\AttributeTok{edge.color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{vertex.color=}\StringTok{"yellow"}\NormalTok{, }\AttributeTok{vertex.size=}\DecValTok{15}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-7-1.pdf}

}

\end{figure}

\hypertarget{adding-attributes}{%
\subsection{Adding attributes}\label{adding-attributes}}

In R, we can add attributes to the nodes, edges and the network. To add
attributes to the nodes, we first need to access them via the following
command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{V}\NormalTok{(g3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
+ 7/7 vertices, named, from bf4a090:
[1] City 1 City 2 City 3 City 4 City 5 City 6 City 7
\end{verbatim}

The node attribute \emph{name} is automatically generated from the node
labels that we manually assigned before.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{V}\NormalTok{(g3)}\SpecialCharTok{$}\NormalTok{name}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "City 1" "City 2" "City 3" "City 4" "City 5" "City 6" "City 7"
\end{verbatim}

But other node attributes could be added. For example, the current
population of the cities represented by the nodes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{V}\NormalTok{(g3)}\SpecialCharTok{$}\NormalTok{population }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{134000}\NormalTok{, }\DecValTok{92000}\NormalTok{, }\DecValTok{549000}\NormalTok{, }\DecValTok{1786000}\NormalTok{, }\DecValTok{74000}\NormalTok{, }\DecValTok{8000}\NormalTok{, }\DecValTok{21000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Similarly, we can access the edges:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{E}\NormalTok{(g3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
+ 6/6 edges from bf4a090 (vertex names):
[1] City 1->City 2 City 2->City 3 City 1->City 4 City 4->City 1 City 4->City 2
[6] City 4->City 5
\end{verbatim}

and add edge attributes, such as the number of people moving from an
origin to a destination city in two consecutive years. We call this
attribute the \emph{weight} of the edge, since if there is a lot of
people going from one city to another, the connection between these
cities has more importance or ``weight'' in the network.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{E}\NormalTok{(g3)}\SpecialCharTok{$}\NormalTok{weight }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2000}\NormalTok{, }\DecValTok{3000}\NormalTok{, }\DecValTok{5000}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{4000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can examine the adjacency matrix of the network, which represents the
presence of edges between different pairs of nodes. In this case, each
row corresponds to an origin city and each column to a destination:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g3[] }\CommentTok{\#The adjacency matrix of network g3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
7 x 7 sparse Matrix of class "dgCMatrix"
       City 1 City 2 City 3 City 4 City 5 City 6 City 7
City 1      .   2000      .   5000      .      .      .
City 2      .      .   3000      .      .      .      .
City 3      .      .      .      .      .      .      .
City 4   1000   1000      .      .   4000      .      .
City 5      .      .      .      .      .      .      .
City 6      .      .      .      .      .      .      .
City 7      .      .      .      .      .      .      .
\end{verbatim}

We can also look at the existing node and edge attributes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vertex\_attr}\NormalTok{(g3) }\CommentTok{\#Node attributes of g3. Use edge\_attr() to access the edge attributes}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$name
[1] "City 1" "City 2" "City 3" "City 4" "City 5" "City 6" "City 7"

$population
[1]  134000   92000  549000 1786000   74000    8000   21000
\end{verbatim}

Finally, it is possible to add network attributes

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g3}\SpecialCharTok{$}\NormalTok{title }\OtherTok{\textless{}{-}} \StringTok{"Network of migration between cities"}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-sec_reading}{%
\section{Reading networks from data files}\label{sec-sec_reading}}

\hypertarget{preparing-the-data-to-create-an-igraph-object}{%
\subsection{Preparing the data to create an igraph
object}\label{preparing-the-data-to-create-an-igraph-object}}

At the beginning of the chapter, we defined a data frame called
\emph{df} based on some imported data from the US Census about migratory
movements between different US cities, or more precisely, between US
Metropolitan Statistical Areas. This is a large data frame containing
52,930 rows, but how can we turn this data frame into a network similar
to the ones that we generated in Section~\ref{sec-sec_create}. The
igraph function \textbf{graph\_from\_data\_frame()} can do this for us.
To find out more about this function, we can run the following command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{help}\NormalTok{(}\StringTok{"graph\_from\_data\_frame"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As we can see, the input data for \textbf{graph\_from\_data\_frame()}
needs to be in a certain format which is different from our migration
data frame. In particular, the function requires three arguments: 1)
\emph{d}, which is a data frame containing an edge list in the first two
columns and any additional columns are considered as edge attributes; 2)
\emph{vertices}, which is either NULL or a data frame with vertex
metadata (i.e.~vertex attributes); and 3) \emph{directed}, which is a
boolean argument indicating whether the network is directed or not. Our
next task is therefore to obtain 1) and 2) from the migration data frame
called \emph{df}.

Let us start with argument 1). Each row in \emph{df} will correspond to
an edge in the migration network since it contains information about a
pair of origin and destination cities for two consecutive years. The
names of the origin and destination cities are given by the columns in
\emph{df} called \emph{MSA\_Previous\_Name} and
\emph{MSA\_Current\_Name}. In addition, the column called
\emph{Movers\_Metro\_to\_Metro\_Flow\_Estimate} gives the number of
people moving between the origin and the destination cities, so this
will be the weight attribute of each edge in the migration network.
Hence, we can define a data frame of edges which we will call
\emph{df\_edges} that conforms with the format required by the argument
1) as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#The pipe operator used below and denoted by \%\textgreater{}\% is a feature of the magrittr package, it takes the output of one function and passes it into another function as an argument}

\CommentTok{\# Creates the df\_edges data frame with data from df and renames the columns as "origin", "destination" and "weight"}
\NormalTok{df\_edges }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{MSA\_Previous\_Name\_State, df}\SpecialCharTok{$}\NormalTok{MSA\_Current\_Name\_State, df}\SpecialCharTok{$}\NormalTok{Movers\_Metro\_to\_Metro\_Flow\_Estimate) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{origin =}\NormalTok{ df.MSA\_Previous\_Name\_State, }\AttributeTok{destination =}\NormalTok{ df.MSA\_Current\_Name\_State, }\AttributeTok{weight =}\NormalTok{ df.Movers\_Metro\_to\_Metro\_Flow\_Estimate) }

\CommentTok{\#Ensure that the weight attribute is stored as a number and not as character }
\NormalTok{df\_edges}\SpecialCharTok{$}\NormalTok{weight }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{gsub}\NormalTok{(}\StringTok{","}\NormalTok{,}\StringTok{""}\NormalTok{,df\_edges}\SpecialCharTok{$}\NormalTok{weight)) }
\end{Highlighting}
\end{Shaded}

For argument 2) we can define a data frame of nodes which we will call
\emph{df\_nodes}, where each row will correspond to a unique node or
city. To obtain all the unique cities from \emph{df}, we can firstly
obtain a data frame of unique origin cities, then a data frame of unique
destinations, and finally, apply the \textbf{full\_join()} function to
these two data frames to obtain their union, which will be
\emph{df\_nodes}. The name of the unique cities in \emph{df\_nodes} is
in the column called \emph{label}, the other columns can be seen as the
nodes metadata.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_unique\_origins }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct}\NormalTok{(MSA\_Previous\_Name\_State) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{name =}\NormalTok{ MSA\_Previous\_Name\_State) }

\NormalTok{df\_unique\_destinations }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{distinct}\NormalTok{(MSA\_Current\_Name\_State) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{name =}\NormalTok{ MSA\_Current\_Name\_State)}

\NormalTok{df\_nodes }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(df\_unique\_origins, df\_unique\_destinations, }\AttributeTok{by =} \StringTok{"name"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, a directed migration network can be obtained with the following
line of code. It should contain 386 nodes and 52,930 edges. You can test
this yourself with the functions that you learnt in
Section~\ref{sec-sec_create}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g\_US }\OtherTok{\textless{}{-}} \FunctionTok{graph\_from\_data\_frame}\NormalTok{(}\AttributeTok{d =}\NormalTok{ df\_edges,}
                                       \AttributeTok{vertices =}\NormalTok{ df\_nodes,}
                                       \AttributeTok{directed =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If we try to plot the network \emph{g3} containing the migratory
movements between all the US cities with the \textbf{plot()} function as
we did before, we obtain a result which is rather undesirable\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(g\_US)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-20-1.pdf}

}

\end{figure}

\hypertarget{filtering-the-data-to-create-a-subgraph}{%
\subsection{Filtering the data to create a
subgraph}\label{filtering-the-data-to-create-a-subgraph}}

We will dedicate the entirety of next section to explore tools that can
help us improve the visualisation of networks, since it is one of the
most important aspects of network analysis. To facilitate the
visualisation in the examples shown in Section~\ref{sec-sec_visualise},
we will work with a subset of the full network called \emph{g\_US}. A
way to create a subnetwork is to filter the original data frame. In
particular, we will filter \emph{df} to only include cities from a
state, in this case, Washington. To filter, we use the \textbf{grepl()}
function, which stands for grep logical. Both \textbf{grep()} and
\textbf{grepl()} allow us to check whether a pattern is present in a
character string or vector of a character string. While the
\textbf{grep()} function returns a vector of indices of the element if a
pattern exists in that vector, the \textbf{grepl()} function returns
TRUE if the given pattern is present in the vector. Otherwise, it
returns FALSE. In this case, we are filtering the dataset so that only
the rows where the field MSA\_Current\_State is WA, which is the
official abbreviation for Washington state.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter the original data frame}
\NormalTok{df\_sub }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(}\FunctionTok{grepl}\NormalTok{(}\StringTok{\textquotesingle{}WA\textquotesingle{}}\NormalTok{, MSA\_Current\_State)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(}\FunctionTok{grepl}\NormalTok{(}\StringTok{\textquotesingle{}WA\textquotesingle{}}\NormalTok{, MSA\_Previous\_State)) }
\end{Highlighting}
\end{Shaded}

Then, we can prepare the data as we did before to create \emph{gUS}.
But, instead of basing the network on \emph{df}, we will generate it
from \emph{df\_sub}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a new data frame containing the columns for the origin, destination, and weight from df\_sub}
\CommentTok{\# Rename the columns to origin, destination, and weight respectively}
\NormalTok{df\_sub\_edges }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(df\_sub}\SpecialCharTok{$}\NormalTok{MSA\_Previous\_Name, df\_sub}\SpecialCharTok{$}\NormalTok{MSA\_Current\_Name, df\_sub}\SpecialCharTok{$}\NormalTok{Movers\_Metro\_to\_Metro\_Flow\_Estimate) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{origin =}\NormalTok{ df\_sub.MSA\_Previous\_Name, }\AttributeTok{destination =}\NormalTok{ df\_sub.MSA\_Current\_Name, }\AttributeTok{weight =}\NormalTok{ df\_sub.Movers\_Metro\_to\_Metro\_Flow\_Estimate)}

\CommentTok{\# Split long names into several lines for better visualization in the resulting graph}
\NormalTok{df\_sub\_edges}\SpecialCharTok{$}\NormalTok{origin }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"{-}"}\NormalTok{, }\StringTok{"{-}}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, df\_sub\_edges}\SpecialCharTok{$}\NormalTok{origin)}
\NormalTok{df\_sub\_edges}\SpecialCharTok{$}\NormalTok{destination }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"{-}"}\NormalTok{, }\StringTok{"{-}}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, df\_sub\_edges}\SpecialCharTok{$}\NormalTok{destination)}

\CommentTok{\# Convert the weight column to numeric, removing any commas that may be present}
\NormalTok{df\_sub\_edges}\SpecialCharTok{$}\NormalTok{weight }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{gsub}\NormalTok{(}\StringTok{","}\NormalTok{,}\StringTok{""}\NormalTok{,df\_sub\_edges}\SpecialCharTok{$}\NormalTok{weight)) }

\CommentTok{\# Create a data frame of unique origins by selecting distinct values of MSA\_Previous\_Name from df\_sub}
\CommentTok{\# Rename the resulting column to "name"}
\NormalTok{df\_sub\_unique\_origins }\OtherTok{\textless{}{-}}\NormalTok{ df\_sub }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct}\NormalTok{(MSA\_Previous\_Name) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{name =}\NormalTok{ MSA\_Previous\_Name) }

\CommentTok{\# Create a data frame of unique destinations by selecting distinct values of MSA\_Current\_Name from df\_sub}
\CommentTok{\# Rename the resulting column to "name"}
\NormalTok{df\_sub\_unique\_destinations }\OtherTok{\textless{}{-}}\NormalTok{ df\_sub }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{distinct}\NormalTok{(MSA\_Current\_Name) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{name =}\NormalTok{ MSA\_Current\_Name)}

\CommentTok{\# Merge the unique origins and unique destinations data frames into one data frame of nodes}
\CommentTok{\# Match the rows based on the "name" column}
\NormalTok{df\_sub\_nodes }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(df\_sub\_unique\_origins, df\_sub\_unique\_destinations, }\AttributeTok{by =} \StringTok{"name"}\NormalTok{)}

\CommentTok{\# Split long names into several lines for better visualization in the resulting graph}
\NormalTok{df\_sub\_nodes}\SpecialCharTok{$}\NormalTok{name }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"{-}"}\NormalTok{, }\StringTok{"{-}}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, df\_sub\_nodes}\SpecialCharTok{$}\NormalTok{name)}

\CommentTok{\# Create a graph object from the edges and nodes data frames}
\NormalTok{g\_sub }\OtherTok{\textless{}{-}} \FunctionTok{graph\_from\_data\_frame}\NormalTok{(}\AttributeTok{d =}\NormalTok{ df\_sub\_edges,}
                                       \AttributeTok{vertices =}\NormalTok{ df\_sub\_nodes,}
                                       \AttributeTok{directed =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-sec_visualise}{%
\section{Network visualisation}\label{sec-sec_visualise}}

\hypertarget{visualisation-with-igraph}{%
\subsection{Visualisation with igraph}\label{visualisation-with-igraph}}

Let us start by generating the most basic visualisation of
\emph{g\_sub}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(g\_sub)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-23-1.pdf}

}

\end{figure}

This plot can be improved by changing adding a few additional arguments
to the \textbf{plot()} function. For example, by just changing the color
and size of the labels, the color and size of the nodes and the arrow
size of the edges, we can already see some improvements.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(g\_sub, }\AttributeTok{vertex.size=}\DecValTok{10}\NormalTok{, }\AttributeTok{edge.arrow.size=}\NormalTok{.}\DecValTok{2}\NormalTok{, }\AttributeTok{edge.curved=}\FloatTok{0.1}\NormalTok{,}
\AttributeTok{vertex.color=}\StringTok{"gold"}\NormalTok{, }\AttributeTok{vertex.frame.color=}\StringTok{"black"}\NormalTok{,}
\AttributeTok{vertex.label=}\FunctionTok{V}\NormalTok{(g\_sub)}\SpecialCharTok{$}\NormalTok{name, }\AttributeTok{vertex.label.color=}\StringTok{"black"}\NormalTok{,}
\AttributeTok{vertex.label.cex=}\NormalTok{.}\DecValTok{65}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-24-1.pdf}

}

\end{figure}

But there are few more things we can do not only to improve the look of
the diagram, but also to include more information about the network. For
example, we can set the size of the nodes so that it reflects the total
number of people that the corresponding cities receive. We can do this
by adding a new node attribute, \emph{inflow}, which is obtained as the
sum of the rows of the adjacency matrix of \emph{g\_sub}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{V}\NormalTok{(g\_sub)}\SpecialCharTok{$}\NormalTok{inflow }\OtherTok{\textless{}{-}} \FunctionTok{rowSums}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(g\_sub[]))}
\end{Highlighting}
\end{Shaded}

Below we set the node size based on the inflow attribute. Note the
formula \(0.4\times(V(gsub)\$inflow)^{0.4}\), where the power of 0.4 is
chosen to scale the size of the nodes in such a way that the largest
ones do not get excessively large and the smallest ones do not get
excessively small. We also set the edge width based on its weight, which
is the total number of people migrating from the origin and destination
cities that it connects.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set node size based on inflow of migrants:}
\FunctionTok{V}\NormalTok{(g\_sub)}\SpecialCharTok{$}\NormalTok{size }\OtherTok{\textless{}{-}} \FloatTok{0.4}\SpecialCharTok{*}\NormalTok{(}\FunctionTok{V}\NormalTok{(g\_sub)}\SpecialCharTok{$}\NormalTok{inflow)}\SpecialCharTok{\^{}}\FloatTok{0.4}
\CommentTok{\# Set edge width based on weight:}
\FunctionTok{E}\NormalTok{(g\_sub)}\SpecialCharTok{$}\NormalTok{width }\OtherTok{\textless{}{-}} \FunctionTok{E}\NormalTok{(g\_sub)}\SpecialCharTok{$}\NormalTok{weight}\SpecialCharTok{/}\DecValTok{1200}
\end{Highlighting}
\end{Shaded}

Run the code below to discover how the aspect of the network has
significantly improved with the modifications that we have introduced
above.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(g\_sub, }\AttributeTok{vertex.size=}\FunctionTok{V}\NormalTok{(g\_sub)}\SpecialCharTok{$}\NormalTok{size, }\AttributeTok{edge.arrow.size=}\NormalTok{.}\DecValTok{15}\NormalTok{, }\AttributeTok{edge.arrow.width=}\NormalTok{.}\DecValTok{2}\NormalTok{, }\AttributeTok{edge.curved=}\FloatTok{0.1}\NormalTok{, }\AttributeTok{edge.width=}\FunctionTok{E}\NormalTok{(g\_sub)}\SpecialCharTok{$}\NormalTok{width, }\AttributeTok{edge.color =}\StringTok{"gray80"}\NormalTok{,}
\AttributeTok{vertex.color=}\StringTok{"gold"}\NormalTok{, }\AttributeTok{vertex.frame.color=}\StringTok{"gray90"}\NormalTok{,}
\AttributeTok{vertex.label=}\FunctionTok{V}\NormalTok{(g\_sub)}\SpecialCharTok{$}\NormalTok{name, }\AttributeTok{vertex.label.color=}\StringTok{"black"}\NormalTok{,}
\AttributeTok{vertex.label.cex=}\NormalTok{.}\DecValTok{65}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-27-1.pdf}

}

\end{figure}

\hypertarget{visualisation-of-spatial-networks}{%
\subsection{Visualisation of spatial
networks}\label{visualisation-of-spatial-networks}}

Firstly, we will import geographical data for the metropolitan and
micropolitan statistical areas in the whole of the US, using the sf
package. Here, we are only interested in the metropolitan areas so we
will filter the data frame \emph{cbsa\_us} to keep only the metropolitan
areas, i.e.~those entries with value M1 for the column named
\emph{LSAD.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Import core{-}based statistical areas https://www.census.gov/geographies/mapping{-}files/time{-}series/geo/cartographic{-}boundary.2020.html\#list{-}tab{-}YXS5CUH5MBYOZ7MJLN}
\NormalTok{cbsa\_us }\OtherTok{\textless{}{-}} \FunctionTok{st\_read}\NormalTok{(}\StringTok{"./data/networks/cb\_2020\_us\_cbsa\_500k/cb\_2020\_us\_cbsa\_500k.shp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Reading layer `cb_2020_us_cbsa_500k' from data source 
  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/networks/cb_2020_us_cbsa_500k/cb_2020_us_cbsa_500k.shp' 
  using driver `ESRI Shapefile'
Simple feature collection with 939 features and 9 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: -178.3347 ymin: 17.88328 xmax: -65.56427 ymax: 65.45352
Geodetic CRS:  NAD83
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter the original data frame to obtain only metro areas}
\NormalTok{msa\_us }\OtherTok{\textless{}{-}}\NormalTok{ cbsa\_us }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(}\FunctionTok{grepl}\NormalTok{(}\StringTok{\textquotesingle{}M1\textquotesingle{}}\NormalTok{, LSAD)) }
\end{Highlighting}
\end{Shaded}

We will now find the centroid of each MSA polygon and add columns to
\emph{msa\_us} for the longitude and latitude of each centroid.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add longitude and latitude corresponding to centroid of each MSA polygon}
\NormalTok{msa\_us}\SpecialCharTok{$}\NormalTok{lon\_centroid }\OtherTok{\textless{}{-}} \FunctionTok{st\_coordinates}\NormalTok{(}\FunctionTok{st\_centroid}\NormalTok{(msa\_us}\SpecialCharTok{$}\NormalTok{geometry))[,}\StringTok{"X"}\NormalTok{]}
\NormalTok{msa\_us}\SpecialCharTok{$}\NormalTok{lat\_centroid }\OtherTok{\textless{}{-}} \FunctionTok{st\_coordinates}\NormalTok{(}\FunctionTok{st\_centroid}\NormalTok{(msa\_us}\SpecialCharTok{$}\NormalTok{geometry))[,}\StringTok{"Y"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Since we are focusing on Washington state, let us filter \emph{msa\_us}
so that it only includes data from Washington. This requires some data
manipulation via the library stringr:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a new column with the name of the state taken from the last two characters of entries in column NAME}
\NormalTok{msa\_us}\SpecialCharTok{$}\NormalTok{NAME\_ONLY }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{",.*$"}\NormalTok{, }\StringTok{""}\NormalTok{, msa\_us}\SpecialCharTok{$}\NormalTok{NAME) }

\CommentTok{\# Long names of MSAs are split into lines for visualisation purposes}
\NormalTok{msa\_us}\SpecialCharTok{$}\NormalTok{NAME\_ONLY }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"{-}"}\NormalTok{, }\StringTok{"{-}}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, msa\_us}\SpecialCharTok{$}\NormalTok{NAME\_ONLY) }

\CommentTok{\# Create a new column with the name of the state taken from the last two characters of entries in column NAME}
\NormalTok{msa\_us}\SpecialCharTok{$}\NormalTok{STATE }\OtherTok{\textless{}{-}} \FunctionTok{substr}\NormalTok{(msa\_us}\SpecialCharTok{$}\NormalTok{NAME, }\FunctionTok{nchar}\NormalTok{(msa\_us}\SpecialCharTok{$}\NormalTok{NAME)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FunctionTok{nchar}\NormalTok{(msa\_us}\SpecialCharTok{$}\NormalTok{NAME)) }

\CommentTok{\# Filter to keep the metro areas belonging to Washington state only}
\NormalTok{msa\_sub }\OtherTok{\textless{}{-}}\NormalTok{ msa\_us }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(}\FunctionTok{grepl}\NormalTok{(}\StringTok{\textquotesingle{}WA\textquotesingle{}}\NormalTok{, STATE)) }
\end{Highlighting}
\end{Shaded}

We can now plot the polygons for the MSA belonging to Washington state
as well as the centroids:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{st\_geometry}\NormalTok{(msa\_sub))}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{st\_centroid}\NormalTok{(msa\_sub}\SpecialCharTok{$}\NormalTok{geometry), }\AttributeTok{add=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{cex=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{pch=}\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-31-1.pdf}

}

\end{figure}

However, we still need to link this data to the network data that we
obtained before. In order to incorporate the geographic information to
the nodes of the migration subnetwork, we can join data from two data
frames: \emph{msa\_sub}, which contains the geographic data, and
\emph{df\_sub\_nodes}, which contains the names of the nodes. To do
this, we can use the function \textbf{left\_join()} and then, select
only the columns of interest. For more information on this magical
function, check
\href{https://dplyr.tidyverse.org/reference/mutate-joins.html}{this
link}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Join the data frame of nodes df\_sub\_nodes with the geographic information of the centroid of each MSA}
\NormalTok{df\_sub\_spatial\_nodes }\OtherTok{\textless{}{-}}\NormalTok{ df\_sub\_nodes }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{left\_join}\NormalTok{(msa\_sub, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"name"} \OtherTok{=} \StringTok{"NAME\_ONLY"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"name"}\NormalTok{, }\StringTok{"lon\_centroid"}\NormalTok{, }\StringTok{"lat\_centroid"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lo }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(df\_sub\_spatial\_nodes[,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{st\_geometry}\NormalTok{(msa\_sub), }\AttributeTok{border=}\FunctionTok{adjustcolor}\NormalTok{(}\StringTok{"gray50"}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(g\_sub, }\AttributeTok{layout=}\NormalTok{lo, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{rescale =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{vertex.size=}\FunctionTok{V}\NormalTok{(g\_sub)}\SpecialCharTok{$}\NormalTok{size, }\AttributeTok{edge.arrow.size=}\NormalTok{.}\DecValTok{1}\NormalTok{, }\AttributeTok{edge.arrow.width=}\FloatTok{1.}\NormalTok{, }\AttributeTok{edge.curved=}\FloatTok{0.1}\NormalTok{, }\AttributeTok{edge.width=}\FunctionTok{E}\NormalTok{(g\_sub)}\SpecialCharTok{$}\NormalTok{width, }\AttributeTok{edge.color=}\FunctionTok{adjustcolor}\NormalTok{(}\StringTok{"gray80"}\NormalTok{, }\AttributeTok{alpha.f =}\NormalTok{ .}\DecValTok{6}\NormalTok{), }\AttributeTok{vertex.color=}\StringTok{"gold"}\NormalTok{, }\AttributeTok{vertex.frame.color=}\StringTok{"gray90"}\NormalTok{,}
\AttributeTok{vertex.label=}\FunctionTok{V}\NormalTok{(g\_sub)}\SpecialCharTok{$}\NormalTok{name, }\AttributeTok{vertex.label.color=}\StringTok{"black"}\NormalTok{,}
\AttributeTok{vertex.label.cex=}\NormalTok{.}\DecValTok{45}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-34-1.pdf}

}

\end{figure}

\hypertarget{alternative-visualisations}{%
\subsection{Alternative
visualisations}\label{alternative-visualisations}}

In this session we have based our visualisations on igraph, however,
there exist a variety of packages that would also allow us to generate
nice plots of networks.

For example, migration networks are particularly well-suited to be
represented as a chord diagram. If you want to explore this type of
visualisation, you can find further information on the
\href{https://www.rdocumentation.org/packages/circlize/versions/0.4.15/topics/chordDiagram}{official
R documentation} and also, for example, on this other link
\href{https://r-graph-gallery.com/122-a-circular-plot-with-the-circlize-package.html}{link}.

\hypertarget{sec-sec_metrics}{%
\section{Network metrics}\label{sec-sec_metrics}}

Here we define some of the most important metrics that help us quantify
different characteristics of a network. We will use the migration
network for the whole of the US again, \emph{g\_US}. It has more nodes
and edges than \emph{g\_sub} and consequently, its behaviour is richer
and helps us illustrate better the concepts that we introduce in this
section.

\hypertarget{density}{%
\subsection{Density}\label{density}}

The network \textbf{density} is defined as the proportion of existing
edges out of all the possible edges. In a network with \(n\) nodes, the
total number of possible edges is \(n\times(n-1)\), i.e.~the number of
edges if each node was connected to all the other nodes. A density equal
to \(1\) corresponds to a situation where \(n\times(n-1)\) edges are
present. A network with no edges at all would have density equal to
\(0\). The line of code below tells us that the density of \emph{g\_sub}
is approximately 0.33, meaning that about 33\% of all the possible edges
are present, or in other words, that there are migratory movements
between almost a third of every pair of cities.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{edge\_density}\NormalTok{(g\_US, }\AttributeTok{loops=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3283458
\end{verbatim}

\hypertarget{reciprocity}{%
\subsection{Reciprocity}\label{reciprocity}}

The \textbf{reciprocity} in a directed network is the proportion of
reciprocated connections between nodes (i.e.~number of pairs of nodes
with edges in both directions) from all the existing edges.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{reciprocity}\NormalTok{(g\_US)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6067259
\end{verbatim}

From this result, we conclude that about 62\% of the pairs of nodes that
are connected have edges in both directions.

\hypertarget{degree}{%
\subsection{Degree}\label{degree}}

The \textbf{total degree} of a node refers to the number of edges that
emerge from or point at that node. The \textbf{in-degree} of a node in a
directed network is the number of edges that point at it whereas the
\textbf{out-degree} is the number of edges that emerge from it. The
\textbf{degree()} functions, allows us to compute the degree of one or
more nodes and allows us to specify if we are interested in the total
degree, the in-degree or the out-degree.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute degree of the nodes given by v belonging to graph g\_US, in this case the in{-}degree}
\NormalTok{deg }\OtherTok{\textless{}{-}} \FunctionTok{degree}\NormalTok{(g\_US, }\AttributeTok{v=}\FunctionTok{V}\NormalTok{(g\_US), }\AttributeTok{mode=}\StringTok{"in"}\NormalTok{)}

\CommentTok{\# Produces histogram of the frequency of nodes with a certain in{-}degree}
\FunctionTok{hist}\NormalTok{(deg, }\AttributeTok{breaks =} \DecValTok{30}\NormalTok{, }\AttributeTok{main=}\StringTok{"Histogram of node in{-}degree"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-37-1.pdf}

}

\end{figure}

As we can see in the histogram, many cities receive immigrants from
60-70 different cities. Very few cities receive immigrants from 300 or
above cities. We can check which is the city with the maximum in-degree.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{V}\NormalTok{(g\_US)}\SpecialCharTok{$}\NormalTok{name[}\FunctionTok{degree}\NormalTok{(g\_US, }\AttributeTok{mode=}\StringTok{"in"}\NormalTok{)}\SpecialCharTok{==}\FunctionTok{max}\NormalTok{(}\FunctionTok{degree}\NormalTok{(g\_US, }\AttributeTok{mode=}\StringTok{"in"}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Phoenix-Mesa-Chandler, AZ"                   
[2] "Washington-Arlington-Alexandria, DC-VA-MD-WV"
\end{verbatim}

We actually obtain a tie between two: the MSA containing Phoenix in
Arizona and the MSA containing Washington DC, which actually spans over
four states. Their in-degree is 354 as we can see below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{degree}\NormalTok{(g\_US, }\AttributeTok{v=}\FunctionTok{c}\NormalTok{(}\StringTok{"Phoenix{-}Mesa{-}Chandler, AZ"}\NormalTok{), }\AttributeTok{mode=}\StringTok{"in"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Phoenix-Mesa-Chandler, AZ 
                      354 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{degree}\NormalTok{(g\_US, }\AttributeTok{v=}\FunctionTok{c}\NormalTok{(}\StringTok{"Washington{-}Arlington{-}Alexandria, DC{-}VA{-}MD{-}WV"}\NormalTok{), }\AttributeTok{mode=}\StringTok{"in"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Washington-Arlington-Alexandria, DC-VA-MD-WV 
                                         354 
\end{verbatim}

Note that the fact that these two cities have the largest in-degree does
not necessarily mean that they are the ones receiving the largest number
of migrants.

\hypertarget{distances}{%
\subsection{Distances}\label{distances}}

A \textbf{path} in a network between node \(A\) and node \(B\) is a
sequence of edges which joins a sequence of distinct nodes, starting at
node \(A\) and terminating at node \(B\). In a \textbf{directed path}
there is an added restriction: the edges must be all directed in the
same direction.

The \textbf{length of a path} between nodes \(A\) and \(B\) is normally
defined as the number of edges that form the path. The \textbf{shortest
path} is the minimum number of edges that need to be traversed to travel
from \(A\) to \(B\).

The length of a path can also be defined in other ways. For example, if
the edges are weighted, it can be defined as the sum of the weights of
the edges that form the path.

In R, we can use the function \textbf{shortest\_paths()} to find the
shortest path between a given pair of nodes and its length. For example,
below we can see that the shortest path between the MSA containing New
York and the MSA containing Los Angeles is one. This is not surprising
since we would expect an edge connecting these two MSAs representing the
fact that there is people migrating from one to the other in two
consecutive years.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shortest\_paths}\NormalTok{(g\_US, }
\AttributeTok{from =} \FunctionTok{V}\NormalTok{(g\_US)}\SpecialCharTok{$}\NormalTok{name}\SpecialCharTok{==}\StringTok{"New York{-}Newark{-}Jersey City, NY{-}NJ{-}PA"}\NormalTok{,}
\AttributeTok{to =} \FunctionTok{V}\NormalTok{(g\_US)}\SpecialCharTok{$}\NormalTok{name}\SpecialCharTok{==}\StringTok{"Los Angeles{-}Long Beach{-}Anaheim, CA"}\NormalTok{,}
\AttributeTok{weights=}\ConstantTok{NA}\NormalTok{, }\CommentTok{\#If weights=NULL and the graph has a weight edge attribute, then the weigth attribute is used. If this is NA then no weights are used (even if the graph has a weight attribute)}
\AttributeTok{output =} \StringTok{"both"}\NormalTok{) }\CommentTok{\# outputs both path nodes and edges}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$vpath
$vpath[[1]]
+ 2/402 vertices, named, from 0770de7:
[1] New York-Newark-Jersey City, NY-NJ-PA Los Angeles-Long Beach-Anaheim, CA   


$epath
$epath[[1]]
+ 1/52930 edge from 0770de7 (vertex names):
[1] New York-Newark-Jersey City, NY-NJ-PA->Los Angeles-Long Beach-Anaheim, CA


$predecessors
NULL

$inbound_edges
NULL
\end{verbatim}

Of all shortest paths in a network, the length of the longest one is
defined as the \textbf{diameter} of the network. In this case, the
diameter is 3 meaning that the longest of all shortest paths in
\emph{g\_US} has 3 edges.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{diameter}\NormalTok{(g\_US, }\AttributeTok{directed=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{weights=}\ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3
\end{verbatim}

The mean distance is defined as the average length of all shortest paths
in the network. The mean distance will always be smaller or equal than
the diameter.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean\_distance}\NormalTok{(g\_US, }\AttributeTok{directed=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{weights=}\ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.663335
\end{verbatim}

\hypertarget{centrality}{%
\subsection{Centrality}\label{centrality}}

Centrality metrics assign scores to nodes (and sometimes also edges)
according to their position within a network. These metrics can be used
to identify the most influential nodes.

We have already explored some concepts which can be regarded as
centrality metrics, for example, the \textbf{degree} of a node or the
weighted degree of a node, also known as the \textbf{strength of a
node}, which is the sum of edge weights that link to adjacent nodes or,
in other words, the in-flow or out-flow associated with each node. As we
can see from the code below, many nodes in \emph{g\_US} have an in-flow
of less than 100 immigrants. Note that we have set mode to c(``in'') for
inflow and we have set the weights parameter to NULL since we want to
know the sum of weights for the incoming edges and not just the total
number of incoming edges.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute strength of the nodes belonging to graph g\_US, in this case the in{-}flow}
\NormalTok{strength\_US }\OtherTok{\textless{}{-}} \FunctionTok{strength}\NormalTok{(g\_US, }\CommentTok{\#The input graph}
  \AttributeTok{vids =} \FunctionTok{V}\NormalTok{(g\_US), }\CommentTok{\# The vertices for which the strength will be calculated.}
  \AttributeTok{mode =} \FunctionTok{c}\NormalTok{(}\StringTok{"in"}\NormalTok{), }\CommentTok{\#“in” for in{-}degree}
  \AttributeTok{loops =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\#whether the loop edges are also counted}
  \AttributeTok{weights =} \ConstantTok{NULL} \CommentTok{\#If the graph has a weight edge attribute, then this is used by default when weights=NULL. If the graph does not have a weight edge attribute and this argument is NULL, then a warning is given and degree is called.}
\NormalTok{)}
  
\CommentTok{\#Produce histogram of the frequency of nodes with a certain strength}
\FunctionTok{hist}\NormalTok{(strength\_US, }\AttributeTok{breaks =} \DecValTok{50}\NormalTok{, }\AttributeTok{main=}\StringTok{"Histogram of node strength"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-43-1.pdf}

}

\end{figure}

We can check which is the city with the maximum strength:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{V}\NormalTok{(g\_US)}\SpecialCharTok{$}\NormalTok{name[}\FunctionTok{strength}\NormalTok{(g\_US, }\AttributeTok{vids =} \FunctionTok{V}\NormalTok{(g\_US), }\AttributeTok{mode =} \FunctionTok{c}\NormalTok{(}\StringTok{"in"}\NormalTok{), }\AttributeTok{loops =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{weights =} \ConstantTok{NULL}\NormalTok{)}\SpecialCharTok{==}\FunctionTok{max}\NormalTok{(}\FunctionTok{strength}\NormalTok{(g\_US, }\AttributeTok{vids =} \FunctionTok{V}\NormalTok{(g\_US), }\AttributeTok{mode =} \FunctionTok{c}\NormalTok{(}\StringTok{"in"}\NormalTok{), }\AttributeTok{loops =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{weights =} \ConstantTok{NULL}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "New York-Newark-Jersey City, NY-NJ-PA"
\end{verbatim}

We will look at another two important centrality metrics that are based
on the structure of the network. Firstly, \textbf{closeness centrality}
which is a measure of the length of the shortest path between a node and
all the other nodes. For a given node, it is computed as the inverse of
the average shortest paths between that node and every other node in the
network. So, if a node has closeness centrality close to \(1\), it means
that on average, it is very close to the other nodes in the network. A
closeness centrality of exactly \(0\) corresponds to an isolated node.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{close\_centr }\OtherTok{\textless{}{-}} \FunctionTok{closeness}\NormalTok{(g\_US, }\AttributeTok{mode=}\StringTok{"in"}\NormalTok{, }\AttributeTok{weights=}\ConstantTok{NA}\NormalTok{) }\CommentTok{\#using unweighted edges}
\FunctionTok{hist}\NormalTok{(close\_centr, }\AttributeTok{breaks =} \DecValTok{50}\NormalTok{, }\AttributeTok{main=}\StringTok{"Histogram of closeness centrality"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-45-1.pdf}

}

\end{figure}

The other metric is known as \textbf{betweenness centrality}. For a
given node, it is a measure of the number of shortest paths that go
through that node. Therefore, nodes with high values of betweenness
centrality are those that play a very important role in the connectivity
of the network. Betweenness can also be computed for edges.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{between\_centr }\OtherTok{\textless{}{-}} \FunctionTok{betweenness}\NormalTok{(g\_US, }\AttributeTok{v =} \FunctionTok{V}\NormalTok{(g\_US), }\AttributeTok{directed =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{weights =} \ConstantTok{NA}\NormalTok{)}
\FunctionTok{hist}\NormalTok{(between\_centr, }\AttributeTok{breaks =} \DecValTok{50}\NormalTok{, }\AttributeTok{main=}\StringTok{"Histogram of betweenness centrality"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-46-1.pdf}

}

\end{figure}

\hypertarget{hubs-and-authorities}{%
\subsection{Hubs and authorities}\label{hubs-and-authorities}}

We call hubs or authorities those nodes with a higher-than-average
degree. Normally, the name hub is reserved to nodes with high out-degree
whereas authority is reserved to nodes with high in-degree. An algorithm
to detect hubs and authorities was developed by Jon Kleinberg, although
it was initially used to examine web pages. Like we did for other
network metrics, we can compute the hub score and then plot a histogram
to see how this metric is distributed across the nodes of the network.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hs }\OtherTok{\textless{}{-}} \FunctionTok{hub\_score}\NormalTok{(g\_US, }\AttributeTok{weights=}\ConstantTok{NULL}\NormalTok{)}\SpecialCharTok{$}\NormalTok{vector }\CommentTok{\#In this case, we use the weighted edges}
\FunctionTok{hist}\NormalTok{(hs, }\AttributeTok{breaks =} \DecValTok{50}\NormalTok{, }\AttributeTok{main=}\StringTok{"Histogram of hub score"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-47-1.pdf}

}

\end{figure}

Similarly, we can explore the authority score for each node:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{as }\OtherTok{\textless{}{-}} \FunctionTok{authority\_score}\NormalTok{(g\_US, }\AttributeTok{weights=}\ConstantTok{NULL}\NormalTok{)}\SpecialCharTok{$}\NormalTok{vector}
\FunctionTok{hist}\NormalTok{(as, }\AttributeTok{breaks =} \DecValTok{50}\NormalTok{, }\AttributeTok{main=}\StringTok{"Histogram of authority score"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-48-1.pdf}

}

\end{figure}

\hypertarget{questions-2}{%
\section{Questions}\label{questions-2}}

In this set of questions, we will use internal migration data
corresponding to the London boroughs. The original data can be found on
the UK Office for National Statistics
\href{https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/migrationwithintheuk/datasets/internalmigrationbyoriginanddestinationlocalauthoritiessexandsingleyearofagedetailedestimatesdataset}{website}.
The data set that we use below corresponds for the year ending in June
2019 and has already been cleaned for you. You can import it with the
following line of code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"./data/networks/LA\_to\_LA\_2019\_London\_clean.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Essay questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create a network with igraph that represents the migratory movements
  between the London boroughs in the year ending in June 2019. The nodes
  of this network will represent the different boroughs and the edges
  will represent the migration flows between them. Make sure the network
  object is directed.
\item
  Produce histograms for the unweighted in-degree and out-degree of the
  nodes in the network. Produce histograms for the weighted in-degree
  and out-degree (also known as strength of inflows and outflows) as
  well. Combine all four histograms in one plot. Additionally, compute
  the edge density of the network (with no loops). Comment on all your
  observations. Finally, based on network metrics, what is the code for
  the London borough with the largest incoming population as well as the
  London borough with the largest outgoing population?
\item
  BONUS QUESTION. This question \textbf{will not be assessed} but it is
  an excellent exercise for those of you who are keen on networks.
  Create a visualisation of a migration network showing the flows
  between different boroughs. You can get as creative as you like. You
  may use igraph or other tools that have not been discussed in this
  workbook but that you may want to explore by yourself.

  You are most welcome to show your figure to one of the lecturers for
  an opportunity to get feedback. However, since it is not assessed, you
  are \textbf{not supposed to include it in the essay.}

  For your visualisation, you may want to use geographical data for the
  London boroughs. To access it, you should run the code below. It loads
  the necessary data for the geographical boundaries of the Local
  Authority Districts (LADs) in England and Wales and then, it filters
  the dataset so only the London boroughs remain, i.e.~those LADs
  starting with E09. The geographical boundaries can be downloaded from
  the ONS Open Geography Portal
  \href{https://geoportal.statistics.gov.uk/search?collection=Dataset\&sort=name\&tags=all(BDY_LAD\%2CDEC_2022)}{website}
  but we have also included a copy of the dataset in the GitHub
  repository associated with this module.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Import boundaries for local authorities in England and Wales}
\NormalTok{LA\_UK }\OtherTok{\textless{}{-}} \FunctionTok{st\_read}\NormalTok{(}\StringTok{"./data/networks/Local\_Authority\_Districts\_(December\_2022)\_Boundaries\_UK\_BFC/LAD\_DEC\_2022\_UK\_BFC.shp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Reading layer `LAD_DEC_2022_UK_BFC' from data source 
  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/networks/Local_Authority_Districts_(December_2022)_Boundaries_UK_BFC/LAD_DEC_2022_UK_BFC.shp' 
  using driver `ESRI Shapefile'
Simple feature collection with 374 features and 10 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: -116.1928 ymin: 5336.966 xmax: 655653.8 ymax: 1220302
Projected CRS: OSGB36 / British National Grid
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter the original data frame to obtain only London boroughs}
\NormalTok{LND\_boroughs }\OtherTok{\textless{}{-}}\NormalTok{ LA\_UK }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(}\FunctionTok{grepl}\NormalTok{(}\StringTok{\textquotesingle{}E09\textquotesingle{}}\NormalTok{, LAD22CD)) }

\FunctionTok{plot}\NormalTok{(}\FunctionTok{st\_geometry}\NormalTok{(LND\_boroughs))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{network_files/figure-pdf/unnamed-chunk-50-1.pdf}

}

\end{figure}

\bookmarksetup{startatroot}

\hypertarget{sec-chp6}{%
\chapter{Sentiment Analysis}\label{sec-chp6}}

This chapter illustrates the use of sentiment analysis and social media
data to measure and monitor public opinion sentiment about migration on
Twitter. Sentiment analysis, also known as opinion mining or emotion
artificial intelligence, refers to the use of natural language
processing to systematically identify, measure and analyse emotional
states and subjective information. It computationally identifies the
polarity of text, that is, whether the underpinning semantics of an
opinion is positive, negative or neutral. It allows deriving
quantitative scores to identify the attitude or position on the
distribution of negative or positive terms in a given piece of text.

This chapters focuses on migration sentiment. Immigration has
consistently been identified as one of the most divisive social issues
globally (European Commision 2019). Immigration sentiment shapes
migration policy formulation and political outcomes. Anti-immigration
sentiment has spurred attention towards more restrictive migration
policies, and has been linked to an increasing prominence of right-wing
affiliation, particularly in Western European countries and the United
States (e.g. Bail et al. 2018). Immigration sentiment also influences
the capacity of migrants to integrate into receiving communities. Acts
of discrimination, intolerance and xenophobia can impair immigrants'
ability to secure employment, housing and achieve a sense of belonging
in local communities, contributing to more polarised societies (Cheong
et al. 2007).

Traditionally, data on public perception on attitudes towards
immigration have been collected through qualitative sources, namely
ethnographies, interviews and surveys. Yet, qualitative methods rely on
small samples and normally suffer from sample bias (Rowe 2021a).
Similarly, while surveys can provide a reliable national representation,
they are expensive, infrequent, offer low population coverage, lack
statistically validity at fine geographical scales and become available
with a lag of one or two years after they have been collected. Social
media, particularly microblogging, has been identified as a key source
of data to overcome these limitations. Social media offers a dynamic and
open space which provides a unique window to better understand public
opinion about immigration. As stated previously, this chapter aims to
illustrate how Twitter data and sentiment analysis can be used to
measure sentiment about migration.

The chapter is based on the following references:

\begin{itemize}
\item
  R for Data Science: \href{https://r4ds.had.co.nz/strings.html}{Working
  with strings (Chapter 14)}.
\item
  Text Mining with R:
  \href{https://www.tidytextmining.com/tidytext.html}{The tidy text
  format}.
\item
  Hutto and Gilbert (2014) on a widely used sentiment analysis algorithm
  for Twitter data.
\item
  Rowe, Mahony, Graells-Garrido, et al. (2021), Rowe, Mahony, Sievers,
  et al. (2021) illustrate the use of Twitter data and sentiment
  analysis to monitor migration sentiment during the COVID-19 pandemic.
\end{itemize}

\hypertarget{dependencies-1}{%
\section{Dependencies}\label{dependencies-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# text data manipulation}
\FunctionTok{library}\NormalTok{(tidytext) }\CommentTok{\# text data tidy approach}
\FunctionTok{library}\NormalTok{(tm) }\CommentTok{\# creating data corpus}
\FunctionTok{library}\NormalTok{(SnowballC) }\CommentTok{\# stemming text}
\FunctionTok{library}\NormalTok{(tidyverse) }\CommentTok{\# manipulate data}

\CommentTok{\# sentiment analysis}
\FunctionTok{library}\NormalTok{(vader)}

\CommentTok{\# download twitter data (no used)}
\FunctionTok{library}\NormalTok{(rtweet)}

\CommentTok{\# design plots}
\FunctionTok{library}\NormalTok{(patchwork)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-1}{%
\section{Data}\label{data-1}}

We will use a sample of Twitter data on public opinion about migration
originated in the United States during December 1st 2019 and May 1st
2020. They data were collected by Rowe, Mahony, Graells-Garrido, et al.
(2021) to analyse changes in public opinion related to migration during
the early stages of the COVID-19 pandemic. During this period, a rising
number of anti-immigration sentiment incidents were reported across the
world ({``Stop the Coronavirus Stigma Now''} 2020). Acts and displays of
intolerance, discrimination, racism, xenophobia and violent extremism
emerged linking individuals of Asian descendent and appearance to
COVID-19 Coates (2020). In the United States, President Donald Trump
repeatedly used the terms ``Chinese Virus,'' ``China Virus,'' and ``Fung
Flu'' in reference to COVID-19. Fear mongering and racial stereotyping
spread on social media and rapidly spilled onto the streets (Cowper
2020). In the United Kingdom, the government reported a 21\% increase in
hate crime incidents against Asian communities between January and
March, and Chinese businesses reported a notorious reduction in footfall
during Chinese celebrations (Home Affairs Committee 2020).

Data were collected via an application programming interface (API) using
random sampling strategy. A set of key search terms were defined,
including words, Twitter accounts and hashtags to collect the data.
These search terms were developed in collaboration with the United
Nations' International Organization for Migration. See Rowe, Mahony,
Graells-Garrido, et al. (2021) for details on the search strategy. For
this chapter, Twitter users' handles have been anonymised.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets\_df }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"./data/sentiment{-}analysis/usa\_tweets\_01122019\_01052020.rds"}\NormalTok{)}
\FunctionTok{glimpse}\NormalTok{(tweets\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 224,611
Columns: 4
$ id         <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1~
$ created_at <dttm> 2019-12-01 00:00:18, 2019-12-01 00:00:23, 2019-12-01 00:00~
$ status_id  <dbl> 1.200928e+18, 1.200928e+18, 1.200928e+18, 1.200928e+18, 1.2~
$ text       <chr> "Another example @anonymous are the TRUE protectors of sanc~
\end{verbatim}

\hypertarget{text-data-structures}{%
\subsection{Text data structures}\label{text-data-structures}}

Different approaches to storing and manipulating text data exist.
General formats include:

String: Text can be stored as strings, i.e., character vectors, within
R, and often text data is first read into memory in this form.

Corpus: These types of objects typically contain raw strings annotated
with additional metadata and details.

Document-term matrix: This is a sparse matrix describing a collection
(i.e., a corpus) of documents with one row for each document and one
column for each term. The value in the matrix is typically word count.

TidyText: This is a novel approach developed by Sielge and Robinson
(2022). The tidytext approach defines a representation in the form of a
table with one-token-per-row. A token is a meaningful unit of text for
analysis, such as a word, a sentence or a tweet. Tokenisation is the
process of splitting text into tokens. Tidy data has a specific
structure. Each variable is a column. Each observation is a row and each
type of observational unit is a table, The R \texttt{tidytext} package
provides full functionality to implement the tidytext approach and
manipulate text with the standard \texttt{tidyverse} ecosystem.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# converting to a tidytext data representation}
\NormalTok{tidy\_tweets\_df }\OtherTok{\textless{}{-}}\NormalTok{ tweets\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(created\_at, text) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{unnest\_tokens}\NormalTok{(}\StringTok{"word"}\NormalTok{, text)}
\end{Highlighting}
\end{Shaded}

In numeric data analysis, we normally look at the distribution of
variables to gain a first understanding of the data. In text analysis,
we explore the distribution of words. Typically we analyse the top
words.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ranking words}
\NormalTok{tidy\_tweets\_df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 129,554 x 2
   word             n
   <chr>        <int>
 1 anonymous   449611
 2 the         215293
 3 to          163434
 4 http        143377
 5 url_removed 136774
 6 and         112320
 7 of          106244
 8 in           93216
 9 a            92763
10 is           75237
# i 129,544 more rows
\end{verbatim}

\hypertarget{basic-text-data-principles}{%
\subsection{Basic text data
principles}\label{basic-text-data-principles}}

Working with text data is complex. There are various important concepts
and procedures that we need to introduce to get you familiar with,
before we can get you rolling with text data mining. In this section, we
introduce key concepts using example to illustrate the main ideas and
basic code.

\textbf{Character encoding}

Character encoding is the numeric representation of graphical characters
that we use to represent human language. Character encoding, such as
UTF-8, ASCII and -ISO-8859-1 enables characters to be stored,
transmitted and transformed using digital computers. For example, we use
the English alphabet and understand differences between lower, upper
case letters, numerals and punctuation. Computers encode and understand
these characters as binary numeric combinations. There is no unique
system of character representation.

Various systems exist and vary according to the type of information,
when it was stored and geographic context. Additionally, different
character encoding representations are used with varying levels of
popularity according to the operating system, language and software, and
this level of popularity changes over time. Currently UTF-8 is one of
the most popular character encoding systems used on the web according to
\href{https://googleblog.blogspot.com/2012/02/unicode-over-60-percent-of-web.html}{Google}.

Sometimes we may need to standardise text data before they can be
combined based on different character encoding systems. Generally R
recognises and read different character encoding representations. But,
if you notice an error of invalid string or an unusual character, this
may mean you are using a dataset based on a character encoding
representation which has not been globally integrated in R. This tends
to occur with characters in various languages and emojis.

There is not quick way to standardise two different encoding systems, to
our knowledge. Two handy functions in R that would help you with this
task is the \texttt{iconv} and \texttt{encoding}. Both functions require
you to know the character encoding representation of the data.

\textbf{Regular expressions}

Regular expressions are patterns that occur in a group of strings. Often
when you work with text data, you are likely to find unusual character
expressions, particularly if you are working with data scrapped from a
website. Strings such as \texttt{tab} and \texttt{return} are normally
represented by \texttt{\textbackslash{}t} and \texttt{\textbackslash{}r}
so you may want to remove these expressions. To deal with regular
patterns, we can use the \texttt{grep} base R library. In our dataset,
we do have various occurrences of the expression \texttt{\&gt} which
stands for \texttt{\textgreater{}} - see example below. Let's assume we
want to remove this expression from our tweet sample. We first could use
\texttt{grepl} from the \texttt{grep} library to identify tweets
containing this expression. The \texttt{grepl} function asks if the
first expression before the comma is present in the data object after
the comma, and returns Boolean result i.e.~\texttt{TRUE} or
\texttt{FALSE}.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, leftrule=.75mm, bottomrule=.15mm, titlerule=0mm, opacitybacktitle=0.6, left=2mm, colframe=quarto-callout-note-color-frame, rightrule=.15mm, colback=white, bottomtitle=1mm, toprule=.15mm, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, toptitle=1mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white]

You can also use the function \texttt{str\_detect} from the
\texttt{stringr} package to identify regular patterns or expressions in
text

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{grepl}\NormalTok{(}\StringTok{"\&gt"}\NormalTok{, tweets\_df}\SpecialCharTok{$}\NormalTok{text[}\DecValTok{40}\SpecialCharTok{:}\DecValTok{50}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE
\end{verbatim}

Let's see one of those tweets:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets\_df}\SpecialCharTok{$}\NormalTok{text[}\DecValTok{48}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Job Lead! ---&gt; LIFE Program Assistant, International Institute of Los Angeles http://url_removed#nonprofit #jobs #Immigration #multitasking"
\end{verbatim}

We may now want to remove the regular expression \texttt{\&gt}. We can
do this by using \texttt{gsub} from the \texttt{grep} library. In
\texttt{gsub}, you add the pattern you want to replace
(i.e.~\texttt{-\/-\/-\&gt}), followed by the expression you want to use
to replace this pattern with (nothing), and the data object (the
tweet:\texttt{tweets\_df\$text{[}48{]}}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gsub}\NormalTok{(}\StringTok{"{-}{-}{-}\&gt"}\NormalTok{, }\StringTok{""}\NormalTok{, tweets\_df}\SpecialCharTok{$}\NormalTok{text[}\DecValTok{48}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Job Lead! ; LIFE Program Assistant, International Institute of Los Angeles http://url_removed#nonprofit #jobs #Immigration #multitasking"
\end{verbatim}

\texttt{grep} functions can also be very helpful in identifying a word
and prefix in a string of words. We recommend consulting the
\href{https://github.com/rstudio/cheatsheets/blob/main/regex.pdf}{Cheat
Sheet} for basic regular expressions in R put together by Ian Kopacka,
to get familiar with the various character expressions and functions.

\textbf{Unit of analysis}

A key component in text data mining is the unit of analysis. We could
focus our analysis on single words, individual sentences, paragraphs,
sections, chapters or a larger corpus of text. The concept of here is
relevant. The process of splitting text into units is known as
\emph{tokenisation}. Tokens are the resulting units of analysis. In the
context of text data mining, \emph{n-grams} is a popular tokenisation
concept. This refers to a sequence of words of length n.~A unigram is
one word (e.g.~migration). A bigram is a sequence of two words
(migration can). A trigram is a sequence of three words (migration can
have) and so on. n-grams can be useful when you want to capture the
meaning of a sequence of words; for example, identifying ``The United
Kingdom'' in a sequence of words. In R, we can use \texttt{tidytext} to
organise the data according to n-grams.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, leftrule=.75mm, bottomrule=.15mm, titlerule=0mm, opacitybacktitle=0.6, left=2mm, colframe=quarto-callout-note-color-frame, rightrule=.15mm, colback=white, bottomtitle=1mm, toprule=.15mm, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, toptitle=1mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white]

Note that the size of the chunk of text that we use to add up unigram
sentiment scores can have an effect on an analysis. A text the size of
many paragraphs can often have positive and negative sentiment averaged
out to about zero, while sentence-sized or paragraph-sized text often
works better. Similarly, small sections of text may not contain enough
words to accurately estimate sentiment, while sentiment in very large
sections may be difficult to identify.

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets\_df[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{,] }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(created\_at, text) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{unnest\_tokens}\NormalTok{(ngram, text, }\AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{, }\AttributeTok{n =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,296 x 2
   created_at          ngram                
   <dttm>              <chr>                
 1 2019-12-01 00:00:18 another example      
 2 2019-12-01 00:00:18 example anonymous    
 3 2019-12-01 00:00:18 anonymous are        
 4 2019-12-01 00:00:18 are the              
 5 2019-12-01 00:00:18 the true             
 6 2019-12-01 00:00:18 true protectors      
 7 2019-12-01 00:00:18 protectors of        
 8 2019-12-01 00:00:18 of sanctuary         
 9 2019-12-01 00:00:18 sanctuary communities
10 2019-12-01 00:00:18 communities when     
# i 1,286 more rows
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, leftrule=.75mm, bottomrule=.15mm, titlerule=0mm, opacitybacktitle=0.6, left=2mm, colframe=quarto-callout-tip-color-frame, rightrule=.15mm, colback=white, bottomtitle=1mm, toprule=.15mm, breakable, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, toptitle=1mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white]

Task

Try changing \texttt{n} to 4. What changes do you observe?

\end{tcolorbox}

\textbf{Text pre-processing}

We also need to think about the words we want to include in our
analysis. Normally we focus on a selection of words conveying a
particular conceptual representation. Some words may not convey much
meaning, enrich our analysis or may distort the meanings we want to
capture. So carefully thinking of the words we want to include in our
analysis is important. Below we go through key character concepts which
are often considered in text data mining and natural language
processing. These concepts imply the removal of certain characters such
as stop words, punctuation and numbers. The need to remove these
characters will vary on the aim of the study, context and algorithm used
to analyse the data.

\emph{\texttt{Stop\ words}}

Stop words are commonly used words in a language. They tend to comprise
articles, prepositions, pronouns and conjunctions. Examples of stop
words in English are ``a'', ``the'', ``is'' and ``are''. Stop words are
essential to communicate in every day language. Yet, in the text data
mining and NLP, stop words are often removed as they are considered to
carry limited useful information. Stop words differ across languages and
different lists of words exist to remove stop words from analysis. We
use the \texttt{tidytext} approach to remove stop words by running:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# remove stop words}
\FunctionTok{data}\NormalTok{(}\StringTok{"stop\_words"}\NormalTok{) }\CommentTok{\# call stop word list}
\NormalTok{ tidy\_tweets\_df }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_tweets\_df }\SpecialCharTok{\%\textgreater{}\%} 
   \FunctionTok{anti\_join}\NormalTok{(stop\_words)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(word)`
\end{verbatim}

Let's see what are the top words now:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_tweets\_df[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{,] }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 9 x 2
  word            n
  <chr>       <int>
1 trust           2
2 anonymous       1
3 citizens        1
4 communities     1
5 ice             1
6 police          1
7 protectors      1
8 sanctuary       1
9 true            1
\end{verbatim}

We can see words, such as ``t.co'' and ``https'' which may not add much
to our analysis and we may consider to remove them using \texttt{grep}
functions. We will illustrate this below.

\emph{\texttt{Punctuation}}

We may also want to remove punctuation. Again, while punctuation may be
very important in written language to communicate and understand the
meaning of text. Punctuation by itself does not convey helpful meaning
in the context of text analysis as we often take the text out of
sequential order. Punctuation removal is another reason to prefer
\texttt{tidytext} as punctuation marks are removed automatically.

\emph{\texttt{Numbers}}

We may want to remove numbers. Sometimes numbers, such as 9/11 or 2016
may provide very relevant meaning in the terrorist attacks in the US
context or Brexit referendum in the UK. However, they generally do not
add much meaning. We can use grep to remove numbers. The
\texttt{"\textbackslash{}\textbackslash{}b\textbackslash{}\textbackslash{}d+\textbackslash{}\textbackslash{}b"}
text tells R to remove all numeric digits. \texttt{d} stands for digits.
The \texttt{-} sign tells \texttt{grep} to exclude these digits.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# remove numbers}
\NormalTok{tidy\_tweets\_df }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_tweets\_df[ }\SpecialCharTok{{-}}\FunctionTok{grep}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b"}\NormalTok{, }
\NormalTok{                                        tidy\_tweets\_df}\SpecialCharTok{$}\NormalTok{word),]}
\end{Highlighting}
\end{Shaded}

\emph{\texttt{Word\ case}}

An additional element to consider is word case. Often all text is forced
into lower case in quantitative text analysis as we do not want words
starting with an upper case word such as ``Migration'' to be counted as
a different word than ``migration''. In this example, the difference
between using lower or upper case words may not matter. Semantically, on
other occasions (e.g.~in a sentiment analysis context), this distinction
may make all the difference. Consider ``HAPPY'' or ``happy''. The former
may emphasise that a person is much happier than in the latter case and
we may want to capture the intensity of this emotion in our analysis. In
such cases, we may be better off preserving the original text.

The \texttt{unnest\_tokens} in the \texttt{tidytext} package
automatically forces all words into lower case. To preserve upper case
words, you will need to change the default options for
\texttt{to\_lower} to \texttt{FALSE}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets\_df[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{,] }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(created\_at, text) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{unnest\_tokens}\NormalTok{(}\StringTok{"word"}\NormalTok{, }
\NormalTok{                  text, }
                  \AttributeTok{to\_lower =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# preserve upper case}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 282 x 2
   created_at          word       
   <dttm>              <chr>      
 1 2019-12-01 00:00:18 Another    
 2 2019-12-01 00:00:18 example    
 3 2019-12-01 00:00:18 anonymous  
 4 2019-12-01 00:00:18 are        
 5 2019-12-01 00:00:18 the        
 6 2019-12-01 00:00:18 TRUE       
 7 2019-12-01 00:00:18 protectors 
 8 2019-12-01 00:00:18 of         
 9 2019-12-01 00:00:18 sanctuary  
10 2019-12-01 00:00:18 communities
# i 272 more rows
\end{verbatim}

\emph{\texttt{White\ spaces}}

White spaces can also be concern. Often white spaces may also be
considered as words. In \texttt{tidytext} language, white spaces can be
removed using the \texttt{gsub} function identifying white spaces with
\texttt{s+}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gsub}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s+"}\NormalTok{,}
     \StringTok{""}\NormalTok{,}
\NormalTok{     tidy\_tweets\_df}\SpecialCharTok{$}\NormalTok{word[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "anonymous"   "true"        "protectors"  "sanctuary"   "communities"
 [6] "citizens"    "trust"       "police"      "trust"       "ice"        
[11] "politics"    "ahead"       "public"      "safety"      "http"       
[16] "url_removed" "218th"       "illegal"     "immigrant"   "child"      
\end{verbatim}

\emph{\texttt{Stemming}}

A common step in text-preprocessing is stemming. Stemming is the
processing of lowering inflection in words to their root forms. For
example, the stem of the word ``monitoring'' is ``monitor''. This step
aids in the pre-processing of text, words, and documents for text
normalisation. Stemming is common practice because we do not want the
words, such as ``monitoring'' and ``monitor'' to convey different
meanings to algorithms, as such topic modelling algorithms, that we use
to extract latent themes from unstructured texts.

For stemming words, we can use the function \texttt{wordStem} from the
\texttt{SnowballC} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_tweets\_df[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{,] }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{mutate\_at}\NormalTok{(}\StringTok{"word"}\NormalTok{, }
                \FunctionTok{funs}\NormalTok{(}\FunctionTok{wordStem}\NormalTok{((.), }
                              \AttributeTok{language=}\StringTok{"en"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: `funs()` was deprecated in dplyr 0.8.0.
i Please use a list of either functions or lambdas:

# Simple named list: list(mean = mean, median = median)

# Auto named with `tibble::lst()`: tibble::lst(mean, median)

# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))
\end{verbatim}

\begin{verbatim}
# A tibble: 20 x 2
   created_at          word     
   <dttm>              <chr>    
 1 2019-12-01 00:00:18 anonym   
 2 2019-12-01 00:00:18 true     
 3 2019-12-01 00:00:18 protector
 4 2019-12-01 00:00:18 sanctuari
 5 2019-12-01 00:00:18 communiti
 6 2019-12-01 00:00:18 citizen  
 7 2019-12-01 00:00:18 trust    
 8 2019-12-01 00:00:18 polic    
 9 2019-12-01 00:00:18 trust    
10 2019-12-01 00:00:18 ice      
11 2019-12-01 00:00:18 polit    
12 2019-12-01 00:00:18 ahead    
13 2019-12-01 00:00:18 public   
14 2019-12-01 00:00:18 safeti   
15 2019-12-01 00:00:18 http     
16 2019-12-01 00:00:18 url_remov
17 2019-12-01 00:00:23 218th    
18 2019-12-01 00:00:23 illeg    
19 2019-12-01 00:00:23 immigr   
20 2019-12-01 00:00:23 child    
\end{verbatim}

\emph{\texttt{Coding\ characters}}

Typically we also want to remove coding characters, including links to
websites. Words such as ``https'', ``t.co'' and ``amp'' are common in
webscrapped or social media text. The strings ``https'' and ``t.co''
appear as the top two more frequent words in our data. Generally such
words do not convey relevant meaning for the analysis so they are
removed. To do this, we can use \texttt{grep}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_tweets\_df[}\SpecialCharTok{{-}}\FunctionTok{grep}\NormalTok{(}\StringTok{"https|t.co|amp"}\NormalTok{,}
\NormalTok{                              tidy\_tweets\_df}\SpecialCharTok{$}\NormalTok{word),] }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 123,886 x 2
   word             n
   <chr>        <int>
 1 anonymous   449611
 2 http        143377
 3 url_removed 136774
 4 immigration  51480
 5 people       19523
 6 ice          19397
 7 immigrant    18898
 8 trump        17861
 9 virus        16162
10 illegals     14392
# i 123,876 more rows
\end{verbatim}

We could also use \texttt{str\_detect} from the \texttt{stringr}
package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coding\_words }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"https|http|t.co|amp|anonymous|url\_removed|http://url\_removed"}\NormalTok{)}

\NormalTok{tidy\_tweets\_df }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_tweets\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{str\_detect}\NormalTok{(word, coding\_words)) }

\NormalTok{tidy\_tweets\_df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 123,191 x 2
   word            n
   <chr>       <int>
 1 immigration 51480
 2 people      19523
 3 ice         19397
 4 immigrant   18898
 5 trump       17861
 6 virus       16162
 7 illegals    14392
 8 china       12750
 9 coronavirus 12318
10 chinese     11258
# i 123,181 more rows
\end{verbatim}

Analysing word frequencies is often the first stop in text analysis. We
can easily do this using \texttt{ggplot}. Let's visualise the 20 most
common words used on Twitter to express public opinions about
migration-related topics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_tweets\_df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=} \FunctionTok{reorder}\NormalTok{(word, n), }\AttributeTok{y=}\NormalTok{ n}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ n}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{( }\AttributeTok{position=}\StringTok{"stack"}\NormalTok{, }
            \AttributeTok{stat =} \StringTok{"identity"}
\NormalTok{            ) }\SpecialCharTok{+}
  \FunctionTok{theme\_tufte2}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_gradient}\NormalTok{(}\AttributeTok{low =} \StringTok{"white"}\NormalTok{, }
                      \AttributeTok{high =} \StringTok{"darkblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{90}\NormalTok{, }
                                   \AttributeTok{hjust =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Number of occurrences (\textquotesingle{}000)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{fill =} \StringTok{"Word occurrences"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sentiment-analysis_files/figure-pdf/unnamed-chunk-18-1.pdf}

}

\end{figure}

\hypertarget{sentiment-analysis}{%
\section{Sentiment Analysis}\label{sentiment-analysis}}

After pre-processing our text, we can focus on the key of this chapter;
that is, measuring migration sentiment. We do this by using sentiment
analysis, which as described in the introduction of this chapter,
enables identifying, measuring and analysing emotional states and
subjective information. It computationally infers the polarity of text,
that is, whether the underpinning semantics of an opinion is positive,
negative or neutral. A variety of methods and dictionaries for
evaluating the opinion or emotion in text exists. We will explore four
different lexicon-based approaches:
\href{http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html}{\texttt{AFFIN}},
\href{https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html}{\texttt{bing}},
\href{http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm}{\texttt{nrc}}
and \href{https://github.com/cjhutto/vaderSentiment}{\texttt{VADER}}.

\hypertarget{dictionary-based-methods}{%
\subsection{Dictionary-based methods}\label{dictionary-based-methods}}

\texttt{AFFIN}, \texttt{bing} and \texttt{nrc} are dictionary- or
lexicon-based approaches. Sentiment lexicons include key words which are
typically used to express emotions or feelings, and are assigned a
numeric score for positive and negative sentiment. They may also contain
scores for emotions, such as joy, anger and sadness. Sentiment lexicons
can thus be used to measure the valence of a given text by searching for
words that describe affect or opinion. Dictionaries can be created by
examining text-based evaluations of products in online forums to ratings
systems from a variety of sources. They can also be created via
systematic observations about different emotions in the field of
psychology or related fields.

The package \texttt{tidytext} provides access to all three lexicons. The
\texttt{nrc} lexicon classifies words in a binary way into categories of
positive, negative, anger, anticipation, disgust, fear, joy, sadness,
surprise and trust. \texttt{nrc} constructed via Amazon Mechanical Turk
(i.e.~people manually labelling the emotional valence of words). The
\texttt{bing} lexicon classifies words in a binary classification of
positive and negative, and is based on words identified on online
forums. The \texttt{AFINN} lexicon assigns words with a score ranging
between -5 and 5 to capture the intensity of negative and positive
sentiment. \texttt{AFINN} includes a list of sentiment-laden words used
during discussions about climate change on Twitter.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, leftrule=.75mm, bottomrule=.15mm, titlerule=0mm, opacitybacktitle=0.6, left=2mm, colframe=quarto-callout-note-color-frame, rightrule=.15mm, colback=white, bottomtitle=1mm, toprule=.15mm, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, toptitle=1mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white]

Note that not all words are in the lexicons and they only contain words
in the English language.

\end{tcolorbox}

In R, we can browse the content of each lexicon using the
\texttt{get\_sentiment} function from \texttt{tidytext}.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, leftrule=.75mm, bottomrule=.15mm, titlerule=0mm, opacitybacktitle=0.6, left=2mm, colframe=quarto-callout-note-color-frame, rightrule=.15mm, colback=white, bottomtitle=1mm, toprule=.15mm, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, toptitle=1mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white]

Note that you may need to authorise the download of the lexicons on your
console

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_sentiments}\NormalTok{(}\StringTok{"nrc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 13,872 x 2
   word        sentiment
   <chr>       <chr>    
 1 abacus      trust    
 2 abandon     fear     
 3 abandon     negative 
 4 abandon     sadness  
 5 abandoned   anger    
 6 abandoned   fear     
 7 abandoned   negative 
 8 abandoned   sadness  
 9 abandonment anger    
10 abandonment fear     
# i 13,862 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_sentiments}\NormalTok{(}\StringTok{"bing"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6,786 x 2
   word        sentiment
   <chr>       <chr>    
 1 2-faces     negative 
 2 abnormal    negative 
 3 abolish     negative 
 4 abominable  negative 
 5 abominably  negative 
 6 abominate   negative 
 7 abomination negative 
 8 abort       negative 
 9 aborted     negative 
10 aborts      negative 
# i 6,776 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_sentiments}\NormalTok{(}\StringTok{"afinn"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2,477 x 2
   word       value
   <chr>      <dbl>
 1 abandon       -2
 2 abandoned     -2
 3 abandons      -2
 4 abducted      -2
 5 abduction     -2
 6 abductions    -2
 7 abhor         -3
 8 abhorred      -3
 9 abhorrent     -3
10 abhors        -3
# i 2,467 more rows
\end{verbatim}

We can easily employ sentiment lexicons using the
\texttt{get\_sentiment} function from \texttt{tidytext}. Let's first
create a date variable to analyse fluctuations in sentiment by day and
compute sentiment scores.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_tweets\_df }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_tweets\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{date =} \FunctionTok{as.Date}\NormalTok{(}\FunctionTok{substr}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(created\_at),}
                          \DecValTok{1}\NormalTok{,}
                          \DecValTok{10}\NormalTok{))}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nrc\_scores }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_tweets\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(}\FunctionTok{get\_sentiments}\NormalTok{(}\StringTok{"nrc"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
                 \FunctionTok{filter}\NormalTok{(sentiment }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"positive"}\NormalTok{, }
                                         \StringTok{"negative"}\NormalTok{))}
\NormalTok{             ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{method =} \StringTok{"nrc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(word)`
\end{verbatim}

\begin{verbatim}
Warning in inner_join(., get_sentiments("nrc") %>% filter(sentiment %in% : Detected an unexpected many-to-many relationship between `x` and `y`.
i Row 102 of `x` matches multiple rows in `y`.
i Row 812 of `y` matches multiple rows in `x`.
i If a many-to-many relationship is expected, set `relationship =
  "many-to-many"` to silence this warning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nrc\_scores }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  created_at          word      date       sentiment method
  <dttm>              <chr>     <date>     <chr>     <chr> 
1 2019-12-01 00:00:18 true      2019-12-01 positive  nrc   
2 2019-12-01 00:00:18 sanctuary 2019-12-01 positive  nrc   
3 2019-12-01 00:00:18 police    2019-12-01 positive  nrc   
4 2019-12-01 00:00:18 ahead     2019-12-01 positive  nrc   
5 2019-12-01 00:00:18 public    2019-12-01 positive  nrc   
6 2019-12-01 00:00:23 illegal   2019-12-01 negative  nrc   
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bing\_scores }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_tweets\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(}\FunctionTok{get\_sentiments}\NormalTok{(}\StringTok{"bing"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{method =} \StringTok{"bing"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(word)`
\end{verbatim}

\begin{verbatim}
Warning in inner_join(., get_sentiments("bing")): Detected an unexpected many-to-many relationship between `x` and `y`.
i Row 106476 of `x` matches multiple rows in `y`.
i Row 6177 of `y` matches multiple rows in `x`.
i If a many-to-many relationship is expected, set `relationship =
  "many-to-many"` to silence this warning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bing\_scores }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  created_at          word    date       sentiment method
  <dttm>              <chr>   <date>     <chr>     <chr> 
1 2019-12-01 00:00:18 trust   2019-12-01 positive  bing  
2 2019-12-01 00:00:18 trust   2019-12-01 positive  bing  
3 2019-12-01 00:00:23 illegal 2019-12-01 negative  bing  
4 2019-12-01 00:00:23 abuse   2019-12-01 negative  bing  
5 2019-12-01 00:00:41 threats 2019-12-01 negative  bing  
6 2019-12-01 00:00:41 protect 2019-12-01 positive  bing  
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{afinn\_scores }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_tweets\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(}\FunctionTok{get\_sentiments}\NormalTok{(}\StringTok{"afinn"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{method =} \StringTok{"afinn"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(word)`
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{afinn\_scores }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  created_at          word    date       value method
  <dttm>              <chr>   <date>     <dbl> <chr> 
1 2019-12-01 00:00:18 true    2019-12-01     2 afinn 
2 2019-12-01 00:00:18 trust   2019-12-01     1 afinn 
3 2019-12-01 00:00:18 trust   2019-12-01     1 afinn 
4 2019-12-01 00:00:18 safety  2019-12-01     1 afinn 
5 2019-12-01 00:00:23 illegal 2019-12-01    -3 afinn 
6 2019-12-01 00:00:23 arrests 2019-12-01    -2 afinn 
\end{verbatim}

As you can see, the output of the various algorithm differs.
\texttt{nrc} and \texttt{bing} provides sentiment scores classified into
positive and negative. \texttt{afinn} returns a value from -5 to 5. An
important feature of the three approaches explored so far is that they
are based on unigrams; that is, single words. As a result, these methods
do not take into account qualifiers before a word, such as in ``no
good'' or ``not true''. Additionally, these methods cannot appropriately
handle negations, contractions, slang, emoticons, emojis, initialisms,
acronyms, punctuation and word-shape (e.g., capitalization) as a signal
of sentiment polarity and intensity (Hutto and Gilbert 2014) . Most
commonly, lexicon-based approaches only capture differences in sentiment
polarity (i.e., positive or negative) but do not identify differences in
sentiment intensity (strongly positive vs.~moderately positive) or
contrasting statements. We note that accurate identification and scoring
of sarcastic statements remain a key challenge in natural language
processing.

We could subtracting positive and negative score to obtain an estimate
of sentiment for each day based on three lexicon approaches. The
resulting data frames could be binned and used to visualise how the
predominant pattern of migration sentiment changes over time. We
recalculate the sentiment scores by reusing the code above and adding
lines for counting, pivoting, summing and subtracting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nrc\_scores }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_tweets\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(}\FunctionTok{get\_sentiments}\NormalTok{(}\StringTok{"nrc"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
                 \FunctionTok{filter}\NormalTok{(sentiment }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"positive"}\NormalTok{, }
                                         \StringTok{"negative"}\NormalTok{))}
\NormalTok{             ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(date, sentiment) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ sentiment,}
              \AttributeTok{values\_from =}\NormalTok{ n,}
              \AttributeTok{values\_fill =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sentiment =}\NormalTok{ positive }\SpecialCharTok{{-}}\NormalTok{ negative) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{method =} \StringTok{"nrc"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(date, sentiment, method)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(word)`
\end{verbatim}

\begin{verbatim}
Warning in inner_join(., get_sentiments("nrc") %>% filter(sentiment %in% : Detected an unexpected many-to-many relationship between `x` and `y`.
i Row 102 of `x` matches multiple rows in `y`.
i Row 812 of `y` matches multiple rows in `x`.
i If a many-to-many relationship is expected, set `relationship =
  "many-to-many"` to silence this warning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bing\_scores }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_tweets\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(}\FunctionTok{get\_sentiments}\NormalTok{(}\StringTok{"bing"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(date, sentiment) }\SpecialCharTok{\%\textgreater{}\%} 
   \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ sentiment,}
              \AttributeTok{values\_from =}\NormalTok{ n,}
              \AttributeTok{values\_fill =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sentiment =}\NormalTok{ positive }\SpecialCharTok{{-}}\NormalTok{ negative) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{method =} \StringTok{"bing"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(date, sentiment, method)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(word)`
\end{verbatim}

\begin{verbatim}
Warning in inner_join(., get_sentiments("bing")): Detected an unexpected many-to-many relationship between `x` and `y`.
i Row 106476 of `x` matches multiple rows in `y`.
i Row 6177 of `y` matches multiple rows in `x`.
i If a many-to-many relationship is expected, set `relationship =
  "many-to-many"` to silence this warning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{afinn\_scores }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_tweets\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(}\FunctionTok{get\_sentiments}\NormalTok{(}\StringTok{"afinn"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(date) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{sentiment =} \FunctionTok{sum}\NormalTok{(value)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{method =} \StringTok{"afinn"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(word)`
\end{verbatim}

Once we have daily sentiment scores, we bin them together and visualise
them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bind\_rows}\NormalTok{(nrc\_scores,}
\NormalTok{          bing\_scores,}
\NormalTok{          afinn\_scores) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date, }\AttributeTok{y =}\NormalTok{ sentiment, }\AttributeTok{fill =}\NormalTok{ method)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_tufte2}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{method, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sentiment-analysis_files/figure-pdf/unnamed-chunk-27-1.pdf}

}

\end{figure}

All there lexicons display the same predominant pattern of increasing
negative migration sentiment between March and April. They differ in the
representation they provide during earlier months. While \texttt{afinn}
and \texttt{bing} lexicons concur in suggesting that a negative
sentiment was the prevalent pattern of sentiment towards migration
during these months, \texttt{nrc} paints a different picture of a
predominantly positive sentiment.

\hypertarget{vader}{%
\subsection{VADER}\label{vader}}

We move on to explore VADER. VADER is a lexicon and rule-based sentiment
analysis tool which is tailored to the analysis of sentiments expressed
in social media, and stands for Valence Aware Dictionary and sEntiment
Reasoner (Hutto and Gilbert 2014). VADER has been shown to perform
better than 11 typical state-of-practice sentiment algorithms at
identifying the polarity expressed in tweets (Hutto and Gilbert 2014),
and has remained one of the most widely used sentiment analysis methods
for social media data (e.g. Elbagir and Yang 2020) ). See Ghani et al.
(2019) and Rosa et al. (2019) for recent comprehensive reviews of social
media analytics.

VADER overcomes limitations of existing approaches (Hutto and Gilbert
2014). It also captures differences in sentiment intensity, contrasting
statements, and can handle complex sentences, including typical
negations (e.g.~``not good''), contractions (e.g.~``wasn't very good''),
conventional use of punctuation to signal increased sentiment intensity
(e.g.~``Good!!!''), use of word-shape to signal emphasis (e.g.~using ALL
CAPS), using degree modifiers to alter sentiment intensity
(e.g.~intensity boosters (e.g.~``very'') and intensity dampeners
(e.g.''kind of''), sentiment-laden slang (e.g.~`sux'), slang words as
modifiers (e.g.~`uber' or `friggin' or `kinda'), emoticons (:) and :D),
translating utf-8 encoded emojis (💘, 💋 and 😁), initialisms and
acronyms (e.g.~`lol'). VADER can also handle entire sentences or ngrams,
rather than only unigrams. See some examples below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vader\_df}\NormalTok{(}\StringTok{"wasn\textquotesingle{}t very good"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              text      word_scores compound pos   neu   neg but_count
1 wasn't very good {0, 0, -1.62282}   -0.386   0 0.433 0.567         0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vader\_df}\NormalTok{(}\StringTok{"not good"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      text word_scores compound pos   neu   neg but_count
1 not good {0, -1.406}   -0.341   0 0.294 0.706         0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vader\_df}\NormalTok{(}\StringTok{"good"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  text word_scores compound pos neu neg but_count
1 good       {1.9}     0.44   1   0   0         0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vader\_df}\NormalTok{(}\StringTok{"Good!!!"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     text word_scores compound pos neu neg but_count
1 Good!!!       {1.9}    0.583   1   0   0         0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vader\_df}\NormalTok{(}\StringTok{"VERY good!!!"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          text word_scores compound   pos   neu neg but_count
1 VERY good!!!  {0, 2.926}    0.701 0.828 0.172   0         0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vader\_df}\NormalTok{(}\StringTok{"wasn\textquotesingle{}t bad but very good"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                      text              word_scores compound   pos   neu neg
1 wasn't bad but very good {0, 0.925, 0, 0, 3.2895}    0.736 0.674 0.326   0
  but_count
1         1
\end{verbatim}

The output is a vector with the following entries: *
\emph{word\_scores}: a string that contains an ordered list with the
matched scores for each of the words in the text. For the first example,
you can see three scores i.e.~a 0 score for ``wasn't'' and ``very'' and
a negative score for ``good'' reflecting the meaning of ``good'' in the
text. * \emph{compound}: the resulting valence compound of VADER for the
entire text after applying modifiers and aggregation rules. * \emph{pos,
neg, and neu}: the parts of the compound for positive, negative, and
neutral content. These take into account modifiers and are combined when
calculating the compound score * \emph{but\_count}: an additional count
of ``but'' since it can complicate the calculation of sentiment.

Let's think about the results from the examples above, what is the piece
of text with the most negative and positive sentiment score? Why? How do
modifiers, amplifiers and negators influence the meaning of text?

Now we will use VADER to explore the sentiment towards migration during
the wake of the COVID-19 pandemic. To reduce computational requirements,
we will work with a sub-sample of our data to obtain sentiment scores at
the tweet level. This is unlike our previous analysis which returned
word-level scores. Obtaining sentiment scores via VADER may take some
time. So do not panic, relax and wait. If this is taking too long, you
can use the `tweet\_vader\_scores.rds` in the data folder for this
chapter.

Note that for this example we will use the tweet text as VADER can
handle various of the issues that would be a problem using the previous
three approaches (as we have described above). Nonetheless, for your our
work you may want to explore the influence of regular expressions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# tweet level }
\NormalTok{tweet\_vader\_scores }\OtherTok{\textless{}{-}} \FunctionTok{vader\_df}\NormalTok{(tweets\_df}\SpecialCharTok{$}\NormalTok{text)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# combine vader scores,tweet ids and dates}
\NormalTok{tweets\_scores\_df }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(tweets\_df}\SpecialCharTok{$}\NormalTok{id, tweets\_df}\SpecialCharTok{$}\NormalTok{created\_at, tweet\_vader\_scores) }

\CommentTok{\# rename vars and extract day var}
\NormalTok{tweets\_scores\_df }\OtherTok{\textless{}{-}}\NormalTok{ tweets\_scores\_df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}
    \AttributeTok{id =} \StringTok{"tweets\_df$id"}\NormalTok{,}
    \AttributeTok{created\_at =} \StringTok{"tweets\_df$created\_at"}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}  
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{date =} \FunctionTok{as.Date}\NormalTok{(}\FunctionTok{substr}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(created\_at),}
                          \DecValTok{1}\NormalTok{,}
                          \DecValTok{10}\NormalTok{))}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\emph{Concentration}

We have done the hard work of computing sentiment scores. We can now
start analysing the results. As any exploratory analysis, a first
feature you may want to analyse if the overall distribution of sentiment
scores. Applied to public opinion data, such analysis may give you an
idea of how socially polarised is a discussion on social media. To this
end, we can create a histogram.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ tweets\_scores\_df) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ compound, }
                 \AttributeTok{binwidth =} \FloatTok{0.05}\NormalTok{), }
                 \AttributeTok{fill =} \StringTok{"\#440154FF"}\NormalTok{,}
                 \AttributeTok{color=}\StringTok{"\#440154FF"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_tufte2}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=} \StringTok{"Tweet sentiment score"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Density"}\NormalTok{)}

\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(tweets\_scores\_df, }\FunctionTok{aes}\NormalTok{(compound)) }\SpecialCharTok{+} 
  \FunctionTok{stat\_ecdf}\NormalTok{(}\AttributeTok{geom =} \StringTok{"step"}\NormalTok{,}
            \AttributeTok{size =} \DecValTok{2}\NormalTok{,}
            \AttributeTok{colour =} \StringTok{"\#440154FF"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_tufte2}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=} \StringTok{"Tweet sentiment score"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Cumulative density"}\NormalTok{)}

\NormalTok{p1 }\SpecialCharTok{|}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sentiment-analysis_files/figure-pdf/unnamed-chunk-32-1.pdf}

}

\end{figure}

We produce two plots exploring the frequency and cumulative distribution
of migration sentiment scores. The results indicate that a concentration
around zero and also at both extremes i.e.~below -0.5 and over 0.5,
suggesting that migration is very polarising social issue.

\emph{Temporal evolution}

We can also explore the temporal evolution of sentiment towards
migration over time. The results indicate that migration sentiment
remained slighly negative but stable during March to April 2020.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot sentiment scores by day}
\NormalTok{p3 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(tweets\_scores\_df, }
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date, }\AttributeTok{y =}\NormalTok{ compound)) }\SpecialCharTok{+}
 \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{colour =} \StringTok{"gray"}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{shape=}\StringTok{"."}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{, }\AttributeTok{size =}\NormalTok{ .}\DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"loess"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{size=}\DecValTok{2}\NormalTok{, }\AttributeTok{span =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{color=}\StringTok{"\#440154FF"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_tufte2}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=} \StringTok{"Day"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Tweet sentiment score"}\NormalTok{)  }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{p3}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sentiment-analysis_files/figure-pdf/unnamed-chunk-33-1.pdf}

}

\end{figure}

\emph{Composition}

Analysing the coumpound conceals the composition of sentiment in tweets,
particularly potential rises in strongly negative sentiment. We then
analyse the composition of tweets classifying our sentiment score into
``Strongly Negative'', ``Negative'', ``Neutral'', ``Positive'' and
``Strongly Positive'' as shown below. The results indicate that the
composition remained largely stable over time with 50\% of all tweets
being negative, of which around 25\% were strongly negative.In contrast,
less than 20\% of all tweets were strongly positive. These results
suggest that anti-migration sentiment tends to use a stronger rethoric
than pro-migration sentiment.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# sentiment categories}
\NormalTok{tweets\_scores\_df }\OtherTok{\textless{}{-}}\NormalTok{ tweets\_scores\_df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stance\_group =}
           \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{           compound }\SpecialCharTok{\textgreater{}=} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{05} \SpecialCharTok{\&}\NormalTok{ compound }\SpecialCharTok{\textless{}=}\NormalTok{ .}\DecValTok{05} \SpecialCharTok{\textasciitilde{}} \DecValTok{3}\NormalTok{,}
\NormalTok{           compound }\SpecialCharTok{\textless{}} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{5} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
\NormalTok{           compound }\SpecialCharTok{\textless{}} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{05} \SpecialCharTok{\&}\NormalTok{ compound }\SpecialCharTok{\textgreater{}=} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{5} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{,}
\NormalTok{           compound }\SpecialCharTok{\textgreater{}}\NormalTok{ .}\DecValTok{05} \SpecialCharTok{\&}\NormalTok{ compound }\SpecialCharTok{\textless{}=}\NormalTok{ .}\DecValTok{5} \SpecialCharTok{\textasciitilde{}} \DecValTok{4}\NormalTok{,}
\NormalTok{           compound }\SpecialCharTok{\textgreater{}}\NormalTok{ .}\DecValTok{5} \SpecialCharTok{\textasciitilde{}} \DecValTok{5}\NormalTok{,}
\NormalTok{           )}
\NormalTok{         )}

\CommentTok{\# count in each sentiment category by day}
\NormalTok{composition\_tab }\OtherTok{\textless{}{-}}\NormalTok{ tweets\_scores\_df }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(date) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{count}\NormalTok{(date, stance\_group) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{spread}\NormalTok{(stance\_group, n)}

\CommentTok{\# percentage in each sentiment category by day}
\NormalTok{composition\_percent\_tab }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(composition\_tab[,}\DecValTok{1}\NormalTok{], (composition\_tab[,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{6}\NormalTok{] }\SpecialCharTok{/} \FunctionTok{rowSums}\NormalTok{(composition\_tab[,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{6}\NormalTok{]) }\SpecialCharTok{*} \DecValTok{100}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  .[, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)] }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{gather}\NormalTok{(stance, percent, }\SpecialCharTok{{-}}\NormalTok{date)}

\CommentTok{\# composition of sentiment score by day}
\NormalTok{p4 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(composition\_percent\_tab , }
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ stance, }\AttributeTok{y =}\NormalTok{ percent, }\AttributeTok{x =}\NormalTok{ date)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position=}\StringTok{"stack"}\NormalTok{, }\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{theme\_tufte2}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"darkred"}\NormalTok{,}\StringTok{"\#d7191c"}\NormalTok{, }\StringTok{"\#f7f7f7"}\NormalTok{, }\StringTok{"\#2c7bb6"}\NormalTok{, }\StringTok{"darkblue"}\NormalTok{),}
                      \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Strongly Negative"}\NormalTok{, }\StringTok{"Negative"}\NormalTok{, }\StringTok{"Neutral"}\NormalTok{, }\StringTok{"Positive"}\NormalTok{, }\StringTok{"Strongly Positive"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=} \StringTok{"Day"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Percent"}\NormalTok{)}

\NormalTok{p4}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sentiment-analysis_files/figure-pdf/unnamed-chunk-34-1.pdf}

}

\end{figure}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, opacityback=0, leftrule=.75mm, bottomrule=.15mm, titlerule=0mm, opacitybacktitle=0.6, left=2mm, colframe=quarto-callout-note-color-frame, rightrule=.15mm, colback=white, bottomtitle=1mm, toprule=.15mm, breakable, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, toptitle=1mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white]

Natural language processing is a rapidly evolving field and various new
sentiment analysis algorithms emerge over the last five years. They are
often tailored to address specific tasks and their performance can vary
widely across datasets. So before deciding on a particular algorithm,
consult the literature on what has been previously used to identify
standard approaches their limitations and strengths.

\end{tcolorbox}

\hypertarget{questions-3}{%
\section{Questions}\label{questions-3}}

For the second assignment, we will focus on the United Kingdom as our
geographical area of analysis. We will use a dataset of tweets about
migration posted by users in the United Kingdom during February 24th
2021 to July 1st 2022. This period coincides with the start of the war
in Ukraine and is expected to capture changes in migration sentiment.
The dataset contains the following information:

\begin{itemize}
\item
  tweet\_id: unique tweet identifier
\item
  created\_at: date tweet were posted
\item
  place\_name: name of place linked to tweet
\item
  lat: latitude
\item
  long: longitude
\item
  text: text content of tweet
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tweets\_qdf }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"./data/sentiment{-}analysis/uk\_tweets\_24022021\_01072022.rds"}\NormalTok{)}
\FunctionTok{glimpse}\NormalTok{(tweets\_qdf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 34,490
Columns: 6
$ tweet_id   <dbl> 1.364707e+18, 1.364694e+18, 1.364692e+18, 1.364684e+18, 1.3~
$ created_at <dttm> 2021-02-24 22:40:21, 2021-02-24 21:47:44, 2021-02-24 21:42~
$ place_name <chr> "Westhumble", "Rushden", "Birmingham", "Cardiff", "Alexandr~
$ lat        <dbl> -0.3302450, -0.6038262, -1.8906405, -3.1797998, -4.5748715,~
$ long       <dbl> 51.25447, 52.28951, 52.49397, 51.49700, 55.98702, 53.64739,~
$ text       <chr> "@post_liberal Voting for Griffin was part of this working ~
\end{verbatim}

Using VADER:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Obtain sentiment scores and create plots to visualise the overall
  distribution of sentiment scores;
\item
  Create plots to analyse the temporal evolution of sentiment over time.
\item
  Visualise the geographical patterns of sentiment scores - see the
  spatial autocorrelation section on Rowe (2022a) to map sentiment
  scores using \texttt{ggplot}.
\end{enumerate}

Analyse and discuss: a) the extent of anti-immigration sentiment in the
United Kingdom; b) how it has changes over time in intensity and
composition; and, c) the degree of spatial concentration in
anti-immigration sentiment in the country.

\bookmarksetup{startatroot}

\hypertarget{sec-chp7}{%
\chapter{Topic Modelling}\label{sec-chp7}}

\textbf{Topic modelling} is part of the larger topic of text data
mining. Text mining is the process of transforming unstructured text
into a structured format to identify meaningful patterns. In text
mining, we often have collections of documents, such as news articles,
blog posts, academic papers and much more. We often want to divide these
documents into natural groups so that we can understand them separately.
\textbf{Topic modeling} is a method for unsupervised classification of
such documents, similar to clustering on numeric data, which finds
natural groups of items - instead of counting them individually - even
when we are not sure what we are looking for. Topic modeling is not the
only method that does this-- cluster analysis, latent semantic analysis,
and other techniques have also been used to identify clustering within
texts
(\href{https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html}{Bail,
2020}).

Topic models offer two significant advantages over simple forms of
cluster analysis such as k-means clustering. Unlike k-means clustering,
which assigns each document to only one cluster, topic models are
mixture models that assign a probability to each document indicating its
likelihood of belonging to a latent theme or topic.

Additionally, topic models use more advanced iterative Bayesian
techniques to determine the probability of each document being
associated with a particular theme or topic. Initially, documents are
assigned a random probability of topic assignment, but the accuracy of
these probabilities improves as more data is processed.

Topic Modelling has been used in population studies on various occasions
to analyse demographic processes such as fertility and migration.
Marshall (2013) for example, uses topic modelling to show the set of
concepts relevant to the study of fertility was defined differently in
France and Great Britain. Findings indicate that bith cultural and
institutional differences were present in the research agendas around
the understandings of fertility decline. This chapter will illustrate
Topic Modelling with
\href{https://www.reddit.com/r/unitedkingdom/}{Reddit data}.

Specifically it will investigate what reddit data can tell us about
discussions around fertility and the pandemic's influence on fertility
rates. Does the data shed any light on theories on the increase in
fertility associated with rising wage inequality Bar et al. (2018)? Or
on the
\href{https://www.stlouisfed.org/on-the-economy/2021/november/pandemic-influence-us-fertility-rates}{negative
impact of the pandemic} on the fertility of women of prime childbearing
age---30- to 34-year-olds?

This chapter is based on:

\begin{itemize}
\item
  The \href{https://www.tidytextmining.com/topicmodeling.html}{Topic
  modelling} chapter in Silge, J. and Robinson, D, 2022.
  \href{https://www.tidytextmining.com/}{Text Mining with R: A Tidy
  Approach}
\item
  Bail, C. 2020's
  \href{https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html}{Topic
  Modelling} chapter in \emph{Text as DATA}. Computational Social
  Science
\end{itemize}

\textbf{Latent Dirichlet Allocation}

A widely used approach for creating a topic model is Latent Dirichlet
Allocation (LDA). LDA considers

\begin{itemize}
\item
  Each document as a combination of topics. We imagine that each
  document may contain words from several topics in particular
  proportions. For instance, in a two-topic model we could say Document
  1 is 80\% about \emph{migration} and 20\% about \emph{refugees}, while
  Document 2 is 40\% about \emph{migration} and 60\% about
  \emph{increased work-force}.
\item
  Each topic as a blend of words. For example, we could imagine a
  two-topic model of a Twitter-feed, with one topic for ``migration''
  and one for ``refugees.'' The most common words in the migration topic
  might be ``migrant'', ``origin'', and ``destination'', while the
  refugee topic may be made up of words such as ``armed conflict'',
  ``persecution'', and ``camp''. Importantly, words can be shared
  between topics; a word like ``destination'' might appear in both
  equally.
\end{itemize}

This approach allows for documents to share content and overlap with one
another, as opposed to being isolated into distinct groups. This mimics
how natural language is typically used. Blei, Ng, and Jordan (2003)
describe LDA's in detail.

For more on LDA's see Bail's chapter on
\href{https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html}{Topic
Modelling}.

\hypertarget{dependencies-2}{%
\section{Dependencies}\label{dependencies-2}}

As shown in the Figure below by we can use tidy text principles to
approach topic modeling with the same set of tidy tools used for other
data analysis in R. In this chapter, we'll learn to work with LDA
objects from the \texttt{topicmodels} package, tidying such models so
that they can be analysed with the help of \texttt{ggplot2} and
\texttt{dplyr}.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{./figs/chp7/tmwr_0601.png}

}

\caption{Silge \& Robinson 2022: A flowchart of a text analysis that
incorporates topic modeling. The topicmodels package takes a
Document-Term Matrix as input and produces a model that can be tided by
tidytext, such that it can be manipulated and visualized with dplyr and
ggplot2.}

\end{figure}

We use the libraries below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Topic models package that allows tidying such models with ggplot2 and dplyr}
\FunctionTok{library}\NormalTok{(topicmodels)}
\CommentTok{\#A framework for text mining applications within R.}
\FunctionTok{library}\NormalTok{(tm)}
\CommentTok{\#The Life{-}Changing Magic of Tidying Text}
\FunctionTok{library}\NormalTok{(tidytext)}
\CommentTok{\# Snowball Stemmers Based on the C \textquotesingle{}libstemmer\textquotesingle{} UTF{-}8 Library}
\FunctionTok{library}\NormalTok{(SnowballC)}
\CommentTok{\# Data manipulation}
\FunctionTok{library}\NormalTok{(tidyverse)}
\CommentTok{\#Create Elegant Data Visualisations Using the Grammar of Graphics}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(ggthemes)}
\CommentTok{\# Reddit Data Extraction Toolkit}
\FunctionTok{library}\NormalTok{(RedditExtractoR)}
\CommentTok{\# Flexibly Reshape Data}
\FunctionTok{library}\NormalTok{(reshape2)}
\CommentTok{\# Estimation of Structural Topic Models}
\FunctionTok{library}\NormalTok{ (stm)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-2}{%
\section{Data}\label{data-2}}

Reddit is a social news website where you can find posts about almost
anything. Reddit has a huge user base and is increasingly used One of
the most interesting aspects of Reddit is the comments that accompany
posts. Redditors are known for their brutal honesty and often provide
interesting opinions that you wouldn't otherwise find. Reddit also has a
\textbf{plug-n-play} R package which makes is very easy to \texttt{get}
Reddit data via API.

The key is to find URLs to Reddit threads of interest. There are 2
available search strategies: by keywords and by home page. Using a set
of \emph{keywords} can help you narrow down your search to a topic of
interest that crosses multiple subreddits whereas searching by
\emph{home page} can help you find, for example, top posts within a
specific subreddit.

If you want to source your own Reddit data, comment out the code below
and change keywords.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#This function takes a collection of URLs and returns a list with 2 data frames: 1. a data frame containing meta data describing each thread 2. a data frame with comments found in all threads}
\CommentTok{\#urls \textless{}{-} find\_thread\_urls(keywords = "fertility", sort\_by = "top", subreddit = NA, period = "year" )}

\CommentTok{\#This function GETs the data. }
\CommentTok{\#fertility\_data \textless{}{-} get\_thread\_content(urls$url)}

\CommentTok{\# The below code simply creates data frames for the threads and the comments and saves them as csvs for later use. }

\CommentTok{\#threads \textless{}{-} pandemic\_babies\_data$threads}
\CommentTok{\#comments \textless{}{-} pandemic\_babies\_data$comments}
\CommentTok{\#write.csv(threads, "data/topic{-}modelling/threads.csv", row.names = FALSE)}
\CommentTok{\#write.csv(comments, "data/topic{-}modelling/comments.csv", row.names = FALSE)}
\end{Highlighting}
\end{Shaded}

First we import the data we will be working with. You can either use the
same data as used below or find other data sourced from reddit
\href{https://github.com/fcorowe/r4ps/tree/main/data/topic_modelling}{here}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{comments }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/topic{-}modelling/comments.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{head}\NormalTok{(comments)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                                                                             url
1 https://www.reddit.com/r/PrequelMemes/comments/zjcuzy/begun_the_clone_war_has/
2 https://www.reddit.com/r/PrequelMemes/comments/zjcuzy/begun_the_clone_war_has/
3 https://www.reddit.com/r/PrequelMemes/comments/zjcuzy/begun_the_clone_war_has/
4 https://www.reddit.com/r/PrequelMemes/comments/zjcuzy/begun_the_clone_war_has/
5 https://www.reddit.com/r/PrequelMemes/comments/zjcuzy/begun_the_clone_war_has/
6 https://www.reddit.com/r/PrequelMemes/comments/zjcuzy/begun_the_clone_war_has/
             author       date  timestamp score upvotes downvotes golds
1    LaLiLuLeLo9001 2022-12-11 1670802198   223     223         0     0
2 Obiwan-Kenobi-Bot 2022-12-11 1670802222   113     113         0     0
3    LaLiLuLeLo9001 2022-12-11 1670802288    75      75         0     0
4 Obiwan-Kenobi-Bot 2022-12-11 1670802305    63      63         0     0
5         Chung_Soy 2022-12-12 1670821362    37      37         0     0
6 Obiwan-Kenobi-Bot 2022-12-12 1670821387    35      35         0     0
                                                                                                                                                                                                                                             comment
1                                                                                                                                                   I'm sorry, are you implying Obi-Wan is just okay looking? That's pure blasphemy, he's beautiful.
2                                                                          That is kind of you to say. But, beauty is an opinion, and I cannot agree with yours. Thank you for your kind thoughts, though.\n\n^This ^Response ^Generated ^by ^OpenAI
3                                                                                                                                                                                                   Damn, and I thought *I*  had self esteem issues.
4  We all have difficult times in our lives. It is not easy to maintain a positive self-image but, with patience and focus, it is possible to find inner peace and contentment. May the Force be with you.\n\n^This ^Response ^Generated ^by ^OpenAI
5                                                                                                                                                                                                         Damn Obi-wan, you going through some shit?
6                                                         Difficult times are part of life, but it is how we respond to them that truly reveals our strength. May the Force guide you through these times.\n\n^This ^Response ^Generated ^by ^OpenAI
   comment_id
1           1
2         1_1
3       1_1_1
4     1_1_1_1
5   1_1_1_1_1
6 1_1_1_1_1_1
\end{verbatim}

We now have a data frame with authors, dates and comments.

\hypertarget{text-data-structures-1}{%
\subsection{Text data structures}\label{text-data-structures-1}}

However, we need to create a Corpus style object to preserve both both
the full text of our Reddit comments and the metadata to eventually move
to a Document-Term matrix used Topic Modelling. We are going to be using
the package \texttt{tidytext}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_fertility\_reddit }\OtherTok{\textless{}{-}}\NormalTok{ comments }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Takes comments dataframe}
  \FunctionTok{select}\NormalTok{(timestamp, comment) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Breaks out the timestamp (like a unique idenified) and the text variables }
  \FunctionTok{unnest\_tokens}\NormalTok{(}\StringTok{"word"}\NormalTok{, comment) }\CommentTok{\# Passes the "word" token and the name of the variable which is \textquotesingle{}comment\textquotesingle{}}

\FunctionTok{head}\NormalTok{(tidy\_fertility\_reddit) }\CommentTok{\# Checks out the first 5 words and the dataframe format}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   timestamp     word
1 1670802198      i'm
2 1670802198    sorry
3 1670802198      are
4 1670802198      you
5 1670802198 implying
6 1670802198      obi
\end{verbatim}

The \texttt{tidytext} format is very useful because once the text has
been tidy-ed, regular R functions can be used to analyze it instead of
the specialized functions. For example, to count the most popular words
in our Reddit, we can can un-comment the following:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#tidy\_fertility\_reddit \%\textgreater{}\%}
\CommentTok{\#  count(word) \%\textgreater{}\%}
\CommentTok{\#  arrange(desc(n))}
\end{Highlighting}
\end{Shaded}

\hypertarget{basic-text-data-principles-1}{%
\subsection{Basic text data
principles}\label{basic-text-data-principles-1}}

Before we can run any type of analysis, we first need to decide
precisely which type of text should be included in our analyses. For
example, as the code above showed, common words such as ``the'', ``and''
and ``that'' are most likely not very informative. Usually, words such
as ``the'' will not be informative for our quantitative text analysis,
but how many times reddit comments use the word ``abortion'' might be
very relevant to an analysis about pro-choice discourses.

\texttt{Stopwords}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"stop\_words"}\NormalTok{) }\CommentTok{\# Stopwords in tidytext package }
\NormalTok{tidy\_fertility\_reddit\_clean }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_fertility\_reddit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(stop\_words) }\CommentTok{\#using anti{-}join to remove words}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(word)`
\end{verbatim}

\texttt{Punctuation\ and\ numbers}

An advantage of \texttt{tidytext} is that it removes punctuation
automatically. There is also very easy in \texttt{tidytext} to remove
all numeric digits. We can use basic grep commands (note the
``\textbackslash b\textbackslash d+\textbackslash b'' text here tells R
to remove all numeric digits and the `-' sign means grep excludes them
rather than includes them). Grep (Global Regular Expression print)
commands are used in searching and matching text files contained in the
regular expressions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_fertility\_reddit\_clean}\OtherTok{\textless{}{-}}\NormalTok{tidy\_fertility\_reddit\_clean[}\SpecialCharTok{{-}}\FunctionTok{grep}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b"}\NormalTok{, tidy\_fertility\_reddit\_clean}\SpecialCharTok{$}\NormalTok{word),]}

\CommentTok{\# Eliminate some specific words}
\NormalTok{tidy\_fertility\_reddit\_clean }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_fertility\_reddit\_clean }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{(word }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"https"}\NormalTok{, }\StringTok{"www.reddit.com"}\NormalTok{, }\StringTok{"comments"}\NormalTok{, }\StringTok{"gt"}\NormalTok{, }\StringTok{"don"}\NormalTok{, }\StringTok{"roe"}\NormalTok{, }\StringTok{"post"}\NormalTok{, }\StringTok{"didn"}\NormalTok{, }\StringTok{"oop"}\NormalTok{, }\StringTok{"ve"}\NormalTok{, }\StringTok{"x200b"}\NormalTok{, }\StringTok{"op"}\NormalTok{, }\StringTok{"nta"}\NormalTok{, }\StringTok{"fuck"}\NormalTok{, }\StringTok{"yeah"}\NormalTok{))) }

\CommentTok{\# Replace some words with others (manual cleaning)}
\NormalTok{tidy\_fertility\_reddit\_clean }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_fertility\_reddit\_clean }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{word =} \FunctionTok{if\_else}\NormalTok{(word }\SpecialCharTok{==} \StringTok{"children"}\NormalTok{, }\StringTok{"child"}\NormalTok{, word)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{word =} \FunctionTok{if\_else}\NormalTok{(word }\SpecialCharTok{==} \StringTok{"kids"}\NormalTok{, }\StringTok{"child"}\NormalTok{, word)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{word =} \FunctionTok{if\_else}\NormalTok{(word }\SpecialCharTok{==} \StringTok{"pregnancy"}\NormalTok{, }\StringTok{"pregnant"}\NormalTok{, word))}
\end{Highlighting}
\end{Shaded}

We could always do more cleaning.

\texttt{Stemming}

Stemming reduces words to most basic forms. A final common step in
text-pre processing is stemming. Stemming a word refers to replacing it
with its most basic conjugate form. For example the stem of the word
``typing'' is ``type.'' Stemming is common practice because we don't
want the words ``type'' and ``typing'' to convey different meanings to
algorithms that we will soon use to extract latent themes from
unstructured texts. \texttt{Tidytext} includes the \texttt{wordStem}
function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  tidy\_fertility\_reddit\_clean}\OtherTok{\textless{}{-}}\NormalTok{tidy\_fertility\_reddit\_clean }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{mutate\_at}\NormalTok{(}\StringTok{"word"}\NormalTok{, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{wordStem}\NormalTok{(., }\AttributeTok{language =} \StringTok{"en"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Analysing word frequencies is often the first stop in text analysis. We
can easily do this using ggplot. Like in sentiment analysis, let's
visualise the 20 most common words used on reddit regarding
fertility-related topics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_fertility\_reddit\_clean }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=} \FunctionTok{reorder}\NormalTok{(word, n), }\AttributeTok{y=}\NormalTok{ n}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ n}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{( }\AttributeTok{position=}\StringTok{"stack"}\NormalTok{, }
            \AttributeTok{stat =} \StringTok{"identity"}
\NormalTok{            ) }\SpecialCharTok{+}
  \FunctionTok{theme\_tufte2}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_gradient}\NormalTok{(}\AttributeTok{low =} \StringTok{"white"}\NormalTok{, }
                      \AttributeTok{high =} \StringTok{"darkblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{90}\NormalTok{, }
                                   \AttributeTok{hjust =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Number of occurrences (\textquotesingle{}000)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{fill =} \StringTok{"Word occurrences"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{topic-modelling_files/figure-pdf/unnamed-chunk-2-1.pdf}

}

\end{figure}

\texttt{The\ Document-Term\ Matrix\ (DTM)}

Finally, we transform our data into a document-term matrix which is the
format we will be needing for quantitative text analysis. This is a
matrix where each word is a row and each column is a document. The
number within each cell describes the number of times the word appears
in the document. Many of the most popular forms of text analysis, such
as topic models, require a document-term matrix.

To create a DTM in \texttt{tidytext} we can use the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_fertility\_DTM}\OtherTok{\textless{}{-}}
\NormalTok{  tidy\_fertility\_reddit\_clean }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(timestamp, word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{cast\_dtm}\NormalTok{(timestamp, word, n)}

\FunctionTok{inspect}\NormalTok{(tidy\_fertility\_DTM[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{3}\SpecialCharTok{:}\DecValTok{8}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<<DocumentTermMatrix (documents: 5, terms: 6)>>
Non-/sparse entries: 6/24
Sparsity           : 80%
Maximal term length: 7
Weighting          : term frequency (tf)
Sample             :
            Terms
Docs         act aren awkward dis normal wish
  1645984812   0    0       0   0      0    0
  1645984815   0    0       0   1      0    0
  1645984839   1    1       0   0      1    1
  1645984869   0    0       1   0      0    0
  1645984938   0    0       0   0      0    0
\end{verbatim}

\hypertarget{topic-modelling}{%
\section{Topic Modelling}\label{topic-modelling}}

After pre-processing out text, we can focus on the key of this chapter:
discussions around fertility and the pandemic's influence on fertility
rates. We do this by using topic modelling.

To start, we will use the DTM we created from the reddit data and the
\texttt{LDA()} function from the \texttt{topicmodels} package, setting k
= 5, to create a five-topic LDA model. Almost any topic model in
practice will use a larger k, but we will soon see that this analysis
approach extends to a larger number of topics.

This function returns an object containing the full details of the model
fit, such as how words are associated with topics and how topics are
associated with documents.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set a seed so that the output of the model is predictable}

\NormalTok{Reddit\_topic\_model }\OtherTok{\textless{}{-}} \FunctionTok{LDA}\NormalTok{(tidy\_fertility\_DTM,}
              \AttributeTok{k =} \DecValTok{5}\NormalTok{, }\CommentTok{\# number of presumed topics}
              \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{seed =} \DecValTok{541}\NormalTok{)) }\CommentTok{\# important if you want this to be reproducible, (321)}

\NormalTok{Reddit\_topic\_model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
A LDA_VEM topic model with 5 topics.
\end{verbatim}

Fitting the model was the ``easy part'': the rest of the analysis will
involve exploring and interpreting the model using \texttt{tidy}
functions from the \texttt{tidytext} package.

\hypertarget{word-topic-probabilities}{%
\subsection{Word-topic probabilities}\label{word-topic-probabilities}}

We can use the \texttt{tidy()} function, originally from the broom
package
\href{https://cran.r-project.org/web/packages/broom/index.html}{Robinson
2017}, for tidying model objects. The \texttt{tidytext} package provides
this method for extracting the per-topic-per-word probabilities, called
β (``beta''), from the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ap\_topics }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{(Reddit\_topic\_model, }\AttributeTok{matrix =} \StringTok{"beta"}\NormalTok{)}
\NormalTok{ap\_topics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 162,320 x 3
   topic term       beta
   <int> <chr>     <dbl>
 1     1 carri 0.000567 
 2     2 carri 0.00115  
 3     3 carri 0.0000683
 4     4 carri 0.000402 
 5     5 carri 0.00117  
 6     1 deal  0.000830 
 7     2 deal  0.00164  
 8     3 deal  0.00203  
 9     4 deal  0.000326 
10     5 deal  0.000681 
# i 162,310 more rows
\end{verbatim}

This has turned the model into a one-topic-per-term-per-row format. For
each combination, the model computes the probability of that term being
generated from that topic. CHANGE: For example, the term ``aaron'' has a
1.686917 × 10−12 probability of being generated from topic 1, but a
3.8959408 × 10−5 probability of being generated from topic 2.

Then there are different options. We could use dplyr's slice\_max() to
find the 10 terms that are most common within each topic. As a tidy data
frame, this lends itself well to a ggplot2 visualization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ap\_top\_terms }\OtherTok{\textless{}{-}}\NormalTok{ ap\_topics }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(topic) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_max}\NormalTok{(beta, }\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(topic, }\SpecialCharTok{{-}}\NormalTok{beta)}

\NormalTok{ap\_top\_terms }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{term =} \FunctionTok{reorder\_within}\NormalTok{(term, beta, topic)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(beta, term, }\AttributeTok{fill =} \FunctionTok{factor}\NormalTok{(topic))) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ topic, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_reordered}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_tufte2}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{topic-modelling_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\end{figure}

Have we defined too many topics? Do we need to increase the number of
words per topic. We can see that the Topic 1 focuses on ``pregancy'' and
``adoption'', while Topic 3 is probably addressing ``legal'' questions
around fertility. We would need to clean the data further to identify
better patterns.

\hypertarget{greatest-differences-in-beta}{%
\subsection{\texorpdfstring{Greatest differences in
\(\beta\)}{Greatest differences in \textbackslash beta}}\label{greatest-differences-in-beta}}

We can consider the terms that had the greatest difference in β between
Topic 1 and Topic 3. This can be estimated based on the log ratio of the
two: \(\log_2(\frac{\beta_2}{\beta_1})\) (a log ratio is useful because
it makes the difference symmetrical: \(\beta_2\) being twice as large
leads to a log ratio of 1, while \(\beta_1\) being twice as large
results in -1). To make sure we pick up relevant words, we can filter
for relatively common words, such as those that have a \(\beta\) greater
than 1/1000 in at least one topic.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ap\_topics }\OtherTok{\textless{}{-}}\NormalTok{ ap\_topics }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(topic }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{|}\NormalTok{ topic }\SpecialCharTok{==} \DecValTok{5}\NormalTok{) }\CommentTok{\# Keeping just the topics of interest}

\NormalTok{beta\_wide }\OtherTok{\textless{}{-}}\NormalTok{ ap\_topics }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{topic =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"topic"}\NormalTok{, topic)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ topic, }\AttributeTok{values\_from =}\NormalTok{ beta) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(topic1 }\SpecialCharTok{\textgreater{}}\NormalTok{ .}\DecValTok{001} \SpecialCharTok{|}\NormalTok{ topic5 }\SpecialCharTok{\textgreater{}}\NormalTok{ .}\DecValTok{001}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Beta greater than 1/1000 in at least one topic}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{log\_ratio =} \FunctionTok{log2}\NormalTok{(topic5 }\SpecialCharTok{/}\NormalTok{ topic1)) }\CommentTok{\# Calculate Log ratio}

\NormalTok{beta\_wide}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 275 x 4
   term     topic1    topic5 log_ratio
   <chr>     <dbl>     <dbl>     <dbl>
 1 carri  0.000567 0.00117       1.05 
 2 lot    0.00574  0.000850     -2.76 
 3 run    0.00134  0.0000847    -3.98 
 4 child  0.0206   0.00159      -3.70 
 5 enjoy  0.000929 0.00102       0.131
 6 fertil 0.00430  0.00362      -0.249
 7 grow   0.00193  0.00236       0.289
 8 hous   0.00166  0.000110     -3.92 
 9 life   0.00923  0.00209      -2.14 
10 mean   0.000899 0.00118       0.396
# i 265 more rows
\end{verbatim}

The words with the greatest differences between the Topic 1 and Topic 3:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_wide }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(}\AttributeTok{direction =}\NormalTok{ log\_ratio }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice\_max}\NormalTok{(}\FunctionTok{abs}\NormalTok{(log\_ratio), }\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{term =} \FunctionTok{reorder}\NormalTok{(term, log\_ratio)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(log\_ratio, term)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Log2 ratio of beta in topic 5 / topic 1"}\NormalTok{, }\AttributeTok{y =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_tufte2}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{topic-modelling_files/figure-pdf/beta_difference-1.pdf}

}

\end{figure}

We can see that the words more common in topic 3 include words such as
``sperm'', ``embryo'' and ``response'' suggesting we may be picking up
medical discussion around fertility. Whereas Topic 1 is more centred
around ``pregnancy'', ``parents'' and ``divorce'' suggesting
socio-economic \ldots. More exploration would be warranted here\ldots{}

\hypertarget{structural-topic-modelling}{%
\subsection{Structural Topic
Modelling}\label{structural-topic-modelling}}

The \texttt{stm} package has some text pre-processing functions
integrated in it. Similar to the steps we did manually in the previous
section. The \texttt{textProcessor} function automatically removes a)
punctuation; b) stop words; c) numbers, and d) stems each word. The
function requires us to specify the part of the dataframe where the
documents we want to analyze are documents), and requires us to name the
dataset where the rest of the meta data live (pandemic\_threads). Notice
what happens in your console while function \texttt{textProcessor}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pandemic\_threads }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/topic{-}modelling/pandemicthreads.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{head}\NormalTok{(pandemic\_threads)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                                                                                                       url
1                              https://www.reddit.com/r/entitledparents/comments/v2kalc/am_i_overreacting/
2        https://www.reddit.com/r/BBBY/comments/1144ioj/today_21623_sue_gove_said_bbby_has_a_new_supplier/
3                    https://www.reddit.com/r/JUSTNOMIL/comments/uizf4z/the_other_son_is_the_golden_child/
4 https://www.reddit.com/r/hiphopheads/comments/10c2n82/album_of_the_year_25_kendrick_lamar_mr_morale_the/
5                https://www.reddit.com/r/collapse/comments/y4mqrd/last_week_in_collapse_october_814_2022/
6     https://www.reddit.com/r/bridezillas/comments/yyda0z/slavedriver_bridezilla_starved_my_mother_while/
                author       date  timestamp
1     user_not_found01 2022-06-01 1654099230
2      DroppingVittles 2023-02-16 1676590184
3 Fair_Personality_122 2022-05-05 1651762956
4   freshsupreme_acist 2023-01-14 1673735518
5   LastWeekInCollapse 2022-10-15 1665837036
6           Azazeru921 2022-11-18 1668753411
                                                                         title
1                                                           Am I overreacting?
2  Today (2/16/23) Sue Gove said BBBY has a new "supplier promise" and more...
3                                            The other son is the golden child
4    Album of the Year #25 : Kendrick Lamar - Mr Morale &amp; The Big Steppers
5                                    Last Week in Collapse: October 8-14, 2022
6 Slavedriver Bridezilla, starved my mother while she was staying at her place
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       text
1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\*Sorry for the long/rant post\\*\n\n&amp;#x200B;\n\nSo, back story I had my son at 17 with a boy who was not ready to be a father. Fine. I lived with my son's father's grandparents (my son's great grandparents) for a time because my parents were divorcing, and they had space for us. It was all well and good until my son was about one, and I met my now fianc\xe9. We started to do things as a family and the great grandparents started to get mad that the baby wasn't with them as much. They about lost it when I moved out on my own. My son was 4, hadn't heard from his dad in over 3 years, and I got served papers from his father asking for full custody of my son. The paperwork was all filled out in the great grandmother's handwriting \\*eye roll\\*. They even went so far as to write a letter to the judge detailing every time they agreed to watch my son, and twisted it seem as though I was a flighty, irresponsible mother. Furthest thing from the truth.\n\nShow up for court and in the end they got every other weekend and a day during the week, and a week in the summer. I say they because the father would quite literally just drop my son off with the great grandparents and leave. He is an alcoholic and would often not show up because of that reason, even lost his license for a time and was still driving my son around. ugh. They would try to manipulate my son (who was 4/5 years old at the time) into thinking I was the bad guy because they would wait until the last second before pick up and get him involved in an art project, or bring out a big new shiny toy, and I would have to tell my son we cant stay and play, or take it with us because we had a small apartment, he would cry and they would comfort him. This went on for a some years until the pandemic hit (my son was then 9/10 years old) and my son's father up and fucked off, again. The great grandmother herself suggested my son not be around his father at that time because his father was/is not taking precautions to be safe. He hasn't even texted me asking about my son since May 2020.\n\nHowever, the great grandparents have wanted to see my son, who is now 13, and I was allowing it for a time, but these people just show over and over their complete lack of respect for me as a parent. We would establish a time for the visit and I would ask my son come home at a certain time. They would wait till the last possible moment and call me to say they started a movie and would be late, or ask if he could spend the night, and if I said no they would be like "your mom said you cant stay for more fun". She would text him and make these plans and then not ask me until the last minute and when we had plans already it would be the same deal. I asked them to ask me first, not my son with plans. They then talked about a birthday party for a cousin in front of my son so that it was my son asking me about it, then again they wait till the last minute to make the plans. I tell them we as a family with a child too young to be vaccinated (my daughter) are avoiding large crowds, they take my son to the mall at peak Christmas time. I tell them my son has a fever he cant attend a basketball game, they call him and tell him to "have his vaccine card ready" because they are going to try to convince me to let him go any way, then they tell him all the fun things he is going to do there (of course I didn't let him go he was sick). This woman SHOWS UP AT MY HOUSE with out telling me and my sister was home with my daughter, scared the crap out of her, and when I told her not to just show up at my house she said "well I didn't see any cars in the drive way so I thought it was fine".  They get on my case all the time saying they "don't know what my problem is", but they are my problem and the lack of respect they have for me. The last time they picked my son up they told me they were going to take him to a place near me, but then I find out they took him about 45 min to an hour away and didn't think that that was something they should have clued me in on.\n\nI really don't want to send my son with them any more, its not like he is asking me to go there or see them. Am I overreacting?\n\n&amp;#x200B;\n\n\\*\\*\\* EDIT to clarify, we are no longer following the every other weekend court order, haven't since my son's father walked out again. The my son seeing his great grandparents is purely voluntary, because I have no legal obligation to them. Thankfully.
2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Take from this as you will...\n\nSource: [https://businessofhome.com/articles/bed-bath-beyond-bought-some-time-will-it-be-enough](https://businessofhome.com/articles/bed-bath-beyond-bought-some-time-will-it-be-enough)\n\nFull article: (boldface is mine)\n\nLet\031s go with the assumption\024as much of a stretch as it may be\024that Bed Bath &amp; Beyond manages to pull off its Houdini-like magic trick and gets the financing to continue to stay in business (and out of bankruptcy). Then what?\n\nIf you consider the company\031s original store format in the 1970s and \03180s\024back then it was just \034Bed &amp; Bath\035\024as 1.0, and its 1987 expansion into the \034Beyond\035 hard goods categories as 2.0, then the private brand era of the past two years was 3.0. On tap is the next version: BBB 4.0.\n\nThe beleaguered retailer\024which has secured new funds with the expectation that more are on the way\024has sketched out a new strategy that is long on optimism but short on specifics. In a statement last week, CEO Sue Gove focused on the big picture: \034This transformative transaction will provide runway to execute our turnaround plan. We continue to put our customers at the center of every decision, positioning Bed Bath &amp; Beyond to meet and exceed their expectations, while resetting our foundation for near- and long-term success.\035\n\n**On Thursday afternoon, BOH was able to access two calls during which BBB executives provided further details to its suppliers\024one call was with conventional vendors, and the other one was with direct-to-consumer brands. Most important, they indicated BBB would begin to pay for merchandise \034in advance or COD\035 depending on the vendor\031s preference. They also said DTC suppliers would start getting paid immediately after BBB was paid by shoppers for orders that are fulfilled directly by the vendor.**\n\n**Executives also said they would be moving to net pricing terms, which would eliminate many of the special charges the store has used in the past to reduce its payments to its suppliers. It\031s all part of what Gove called the company\031s new \034supplier promise.\035 New interim CFO Holly Etlin was blunter when she said BBB was taking the next few months \034to clean up its act.\035**\n\nBeyond new payment terms, here\031s what we know so far about what management has in mind going forward:\n\nStores: From its high of some 1,500 stores just a few years ago, Bed Bath &amp; Beyond is radically slashing its fleet and says it will end up with about 360 stores, plus another 120 BuyBuy Baby locations. All stores in Canada\024about 65 between the two nameplates\024are closing as the company exits the country entirely. (Curiously, it has said nothing about its joint venture in Mexico, where it has a handful of locations.) The new U.S. footprint will represent a drastic reduction, with broad swaths of the country having few if any stores left.\n\nE-commerce: The company has not shared what percentage of its overall business is done online, but the last time it did, in the first quarter of 2021, it was 38 percent, albeit during the pandemic conditions when everyone was shopping online. All Bed Bath &amp; Beyond will say now is that \034the digital channel is expected to rise to a higher proportion of sales with improved channel profitability.\035 Yet none of its statements on how the new funding will be used make mention of investment in its subpar digital operation.\n\nMerchandising: Here\031s where it gets really vague. \034We are prioritizing availability of leading national and emerging direct-to-consumer brands our customers know and love,\035 said Gove in a press release, which probably means the company will continue to de-emphasize private label merchandise and bring in more DTC brands. But these are the same national brands Bed Bath &amp; Beyond previously carried in its 2.0 era\024the same period of decline that led to the pivot toward private label goods. On the DTC front, the retailer already carries Casper and has started to bring in some other digitally native players, but all of these brands are pursuing physical store distribution on their own as well, and it will be hard for the company to stand out.\n\nPhysical format and fulfillment: This one is intriguing. **\034The Company will also be pursuing asset-light inventory management strategies to drive growth, including vendor-direct-to-consumer, marketplace and the potential for innovative collaborations,\035** said the release. The vendor-direct strategy for online orders is something BBB has been doing for years, as have most other retailers. (Wayfair, in particular, takes very little ownership of goods, relying on its suppliers to handle fulfillment.) But what does that approach mean for stores? Is the company suggesting leased or consignment departments that vendors run and staff? While that is a component of department store retailing for luxury brands and beauty, there\031s very little of that in the home space. Only ABC Carpet &amp; Home in New York pursued that strategy in its heyday, and it has never been scaled up to any degree anywhere. **\034Innovative collaborations\035 could mean just about anything\024or, frankly, nothing.**\n\nOperations: Bed Bath &amp; Beyond says it expects to achieve significant cost savings as it drastically reduces the size of the company. \034Supply chain, technology, expense structure and business processes will continue to be streamlined as the company realigns its operational foundation\035 was how Gove put it. **That process has already begun with serious layoffs at corporate headquarters** in Union, New Jersey, and with hundreds of store-closing sales underway.\n\nWill all of this be enough? And will it be in time? The new financial lifeline investors have extended is not all that long, and in the meantime, current sales have likely been severely diminished\024at least one independent source reported that many stores are managing inventory levels off 40 percent from optimum.\n\nAll of those bankruptcy headlines can\031t have helped customer enthusiasm\024or worker morale, for that matter. Bed Bath &amp; Beyond 4.0 is most certainly a work in progress. Grove has to hope it will indeed result in progress.
3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               MIL has always been a pain in the butt. When my kids were babies, she was the MIL who refused to honor my wishes re: no kissing the baby, no "fake biting" the baby. Yes, that was her thing. She wanted the right to fake bite my baby's arm. It was her way to bond. She declined an invitation to attend a birthday party in the park, because celebrating outdoors is beneath her.\n\nFast forward to the pandemic. As soon as it hit, MIL was isolated due to the fear of covid. She is healthy but age is definitely a factor. So, we respected her wishes and we didn't see her for 18 months, until she was fully vaccinated. Then, she only wanted to see us outdoors! And her excuse was that our children weren't vaccinated yet (at that time, vaccines weren't offered to children).\n\n We saw her that one time on our deck, where she sat and ate and barely interacted with the kids. \n\nSince then, my kids got their full vaccines but MIL changed her tune and said the risk to see them was too high.\n\nA few weeks ago, my husband's brother's father-in-law passed away. There was a memorial service held in a tiny hotel reception area. Quite obviously indoors. Packed with people Food was served and masks were only worn when people weren't helping themselves to the buffet. Yes, there was a buffet. But my covid-conscious MIL attended. And she took off her mask to help herself to the buffet. We attended as well. She saw my husband and squeezed his cheeks with her dirty hands, pointed out his bald spot, and asked if the suit he was wearing is the same one from high school (20 years ago. It wasn't). She ran around quacking about her now she's fully vaccinated, times 4, she needs to move on with her life. She's done everything she could.\n\nIn the 2+ years since covid hit, my husband had surgery  and had to be off his feet for months. I worked from home, took care of the kids' virtual schooling, and did ALL of the errands that my husband used to help out with. No help from MIL. No concern.\n\nSo, my kids have a cello recital. They both worked had. They've won awards in the past. This is important to them. We invited MIL and she declined. It is a much less covid-hazardous event. But my MIL has seen her other son in person, has been inside his home, has attended his father-in-law memorial service. My kids get nothing. \n\nI am from Ukraine. As an immigrant, though not recent, I have no extended family. My parents died when I was a teen. This is it for grandparents for my kids.\n\nThat's it. I just wanted to vent. Give it to me straight. Maybe I'm just delusional.
4 Artist : Kendrick Lamar\n\nAlbum : \034Mr. Morale &amp; The Big Steppers\n\nApple download link :\n\n[https://music.apple.com/us/album/mr-morale-the-big-steppers/1626195790](https://music.apple.com/us/album/mr-morale-the-big-steppers/1626195790)\n\nSpotify link : [https://open.spotify.com/album/3OqPkYVDzHKistrI9exrjR?si=ha-Ln-\\_wSleWQa1Lo9MgMQ](https://open.spotify.com/album/3OqPkYVDzHKistrI9exrjR?si=ha-Ln-_wSleWQa1Lo9MgMQ)\n\nYouTube : [YouTubehttps://m.youtube.com : playlistMr. Morale &amp; The Big Steppers](https://m.youtube.com/playlist?list=PLjB_8hSS2lEMY-ap4zdPv0-mbTwxtN7KW)\n\nWho is **Kendrick Lamar**????? I can\031t imagine this is worth answering in 2022, but he\031s a good kid from a MAAD city (a.k.a Section 80) and he taught us how to pimp a butterfly while the whole world said DAMN to even his most untitled and unmastered hits. Do ya googles\n\nAs I get a little older (literally today is my birthday) I realize life is about perspective. That one word, changes how you and i give and receive information.\n\nSo my perspective is that i heard of Kendrick early. I can\031t say exactly when. It was the infamous mixtape era for sure. But that time is honestly just a blur of datpiff cover arts and cigarillos minimally filled with weed hoping for a maximum high. I do remember downloading both of his mixtapes. Not on purpose hilariously enough, no i was a music junkie so anything that looked dope got downloaded in mass. It was the *Kendrick Lamar Ep* and *Overly Dedicated.* It just didn\031t get played. When i finally did, to add more weirdness, i didn\031t care for overly dedicated. I really don\031t have to explain, i do think theres too much of that going on these days, but in this case i want to. I vaguely remember why i didn\031t care for it, but it was mainly that the songs like Michael Jordan etc that i just wasn\031t as into at the time. Not bad music by any stretch, but at that time i was super backpack. Im talking Ghostface Killah, Mos Def, Lupe, Mood Musik times. So it wasn\031t often that i played more up tempo or even braggadocios music. I went to college somewhat knowing who he was, but still not really playing his music. I don\031t think i ever played the Kendrick Lamar Ep, which truly makes me think things happen for a reason because if i had, he would have been one of my favorites from them.\n\nIn college i made one really good friend early. He was from California and we were in Kentucky where i was actually born. Somehow we just got each other more than most. The kind of friendship i never ask you do you have weed, only do you wanna smoke when i do. He actually brought up Kendrick again, BUT, and maybe I\031m the only one that this has happened to, but he played some of the O.D mixtape in a different light and i loved it. Im not sure how but by the time i doubled back to the intro, i was in awe. Im not sure how i missed it, as the display of lyricism and content was enough for me to fully start to appreciate his music. In this same year span, he not only was featured on Mac Miller (another person who i didn\031t like at first, but once he started talking about drugs and the life i lived i loved it), but Section 80 came out. So, perspective. Section 80 is my good kid MAAD city, if i had to vote for a perfect kendrick album it would be TPAB, and my favorite album to listen to by Kendrick as of now is Mr. Morale and the Big Steppers. Ill explain\n\n***United in Grief*** is an interesting opener. While this is one of my favorite albums of the year, this record in particular may be my least favorite. When I FIRST heard the album I listened to every record no skips. There is no bad music on this album. But this album does introduce an interesting concept. Should music be forever? Pineapple Express is one of my favorite movies, I\031ve seen it a thousand times and I can quote the entire movie if I wanted to. So by that definition is an amazing movie. But on the other hand, seven pounds changed my life. I have only seen it maybe 6 or 7 times because I love tragedy. But with such a deep impact, it\031s also great. That is what Kendrick is introducing to music. He knew we would all tune into track one at least. So he said all of the things he felt he needed to in a run on sentence type fashion. A line that sticks out to me is \034**I grieve different**\035. I never really related to Kendrick. Loved his albums, have been a fan since around 2010, but I didn\031t relate. This felt like one of those universal lines. Some people take off work and grieve. I bought more weed and cried on my way to work about my grandmother for a week. We ALL grieve different. While he was \034quiet\035 through the pandemic, he like all of us learned something. In this case, this is the first time I\031ve heard Kendrick talk about the other side of being who he is. So the statement hits extremely hard because you know Kendrick watched everything that happened, and one of the first thoughts he wanted to share with us when he came back is \034**I grieve different**\035.\n\n***N95*** is my album opener. When I pull up this album, I\031m clicking on this to start my run. I don\031t listen to the entire album anymore, as I respect it as a piece of art that doesn\031t need replay value. Yet another example of Kendrick being ahead of the curve. Art doesn\031t have a singular definition, yet sometimes rap makes it seem like it does. Albums like Testing are crucified for straying too far away from the artist\031s discography. I lived through Yeezus and let me tell you, nobody liked it originally. Don\031t even get me started on 808s, I specifically recall being ridiculed for liking that \034weird shit\035. So in a world where most people are putting on so many different things to fit in, here Kendrick is saying take it off. Not one to waste a moment, the moment he\031s told you to take off every single thing that you\031ve put on without it really being you, he tells you \034**ughhh, you ugly as fuck**\035. Clearly he\031s had enough time to analyze himself as well as the world 4 times over. It could easily be one of the deepest lines of the record if you think about it. Yes you\031re ugly. So am I. That\031s what makes us beautiful. You could surely take that line at face value and say the cool kid from the mad city is back on his bragging, but I disagree. This record may have more of the traditional Kendrick markers from his albums pre hiatus, but it seems like he\031s bridging us to something new. Even with all his success in the first half of his career, he still changes and pushes boundaries. \034**Ohh you worried about a critic, that ain\031t protocol**\035 says it best. With lines like that, it\031s clear to see that if Nas isn\031t relevant, the kids are being raised wrong.\n\nI told you that I didn\031t really relate to Kendrick. He was always a cool kid to me, not just because of his music, but the way he\031s been perceived as well. Ever single person from that era has had at least one negative narrative except Kendrick. I\031ve spent hours arguing this exact thing with friends. J Cole was quote unquote boring. Big Sean was called corny. Childish was seen as soft. Not saying any of these are true, just proving my point. I\031ve always debated Kendrick is a mastermind that watched the complaints his classmates received and did extra curricular work to be sure he was never the victim. I was unpopular so I related more to Childish. But when Kendrick said \034 **I don\031t know how to feel, like the first time I fucked a white bitch**\035 on ***Worldwide Steppers*** I was instantly transported back to that dorm room when i didn\031t know how to feel. In was in college, which was the first time i had ever really interacted with white girls. Here i was, asking myself if this was worth the perception that would follow, if i was actually wrong for doing it, could i still be a black leader once people found out all crossed my mind before i even had the condom on. Most people will hear that and not think much of it. But to me, especially with the fact that kendrick had a exact recollection of each time he crossed that line, i believe he too sees it as somewhat of a traumatic event. Not the act itself, but the self questions and self blaming that comes with that for a black man isn\031t something a wise man can just shrug off.\n\n***Die Hard*** would be the record that i originally started at after several, and i do mean plus 15 listens of the full album from the day prior to release on. Once digested, i started to pick my favorites and Die Hard just captures everything i didn\031t know that i wanted from Kendrick. This is the first song I knew I\031d be playing for the foreseeable future due to it containing mantras like \034**I hope I\031m not too late, to set my demons straight/I know I made you wait, but how much can you take**?\035. For a person who has admittedly not achieved as much as I want in life, words like this are golden. Who can\031t relate to wanting just a bit more time to get everything right? I\031m 31 and I\031m learning more from adopting two huskies at once than I can recall in most of my years of school. It\031s becoming clear he has tapped into what could be seen as universal truths, whereas before you could argue Kendrick was speaking to a specific audience with each album, regardless of how well the album did. It\031s like Jay-Z\031s album Reasonable Doubt. If you judge by the lingo and the content on the album, you could argue it was only for high level players and very established drug dealers. Kind of like a memo among a company. But with this album, and maybe for the rest of his career, Kendrick isn\031t just talking to Compton. Or to black people. Or to the youth of the lost age. He\031s talking to the world, which to me, is another mark of his genius. \034**I hope you see the god in me, I hope you can see, and if it\031s up stay down for me**\035 isn\031t a complex scheme, but as well all know you can be looked down upon for things you said or done. Here Kendrick is pleading for you to see the divine root in him despite human actions that take place daily. \034**I get emotional about life/the lost ones keeping me up at night/the world be reminding me it\031s danger/I still risk it all for a stranger/if I told you who I was would you use it against me?/right or wrong, no stone, just love to send me**\035 is extremely profound. I will not use extreme examples here as that could be too controversial and ultimately not worth the metaphor, but let\031s take Dave Chappelle. As a member of his Reddit for a while I was proud to see how people didn\031t let media narratives dictate who he was to them. Then the Elon musk incident happened. There were quite a few people who were quick to bash and denounce him because it \034was cool to do\035 over a single joke he made they didn\031t care for. Kendrick\031s lyrics make me ask the same question again, is there no kind of system where we can properly account of the all the good deeds someone has done when we finally find a bad one? Will smith was safe. Like for a black guy safe lol. Up until that slap not one public incident or outburst. I think we can all recall how many \034he should be in jail\035 phrases were thrown around. What is the point of telling people who you are if they\031ll use it against you? When something happens most people quickly say how they never cared for you anyways. I\031m not saying that you should blindly agree with things you don\031t, and neither is Kendrick. No singular action makes you good, nor evil. It is the sum of all actions multiplied by your intentions. But even then, should we as humans be able to definitively say either way? Slink Johnson said it best, let ye WITHOUT sin cast the first stone. \034**I wonder when I lost my way/been waiting on your call all day/tell me you\031re in my corner right now/when I fall short I\031m leaning on you to cry out**\035 further pushes this point. Maybe we all are just one phone call from a fan or loved one away from finding our path again.\n\nWith the opening of ***Father Time***, I find myself chuckling. I\031ve just never really known much of anything about Kendrick. More of an enigma. But for him to say what I\031ve said to my (ex) girlfriends countless times was hilarious. It serves as yet another glimpse into situations he\031s been in. \034I **got daddy issues, that\031s on me**\035 is a perfect example of the accountability that\031s present all over the album. In reality, most of us have some issues with how our fathers did, or didn\031t, raise us. Even though this is an old concept, if people like Kendrick don\031t reiterate it, it\031s usually lost from one generation to the next, even as far as him identifying to still have those problems at his age is on him. One of my absolute favorite lyrics from the album is \034**When Kanye got back with Drake, I was slightly confused/guess I\031m not mature as I think, got some healing to do**\035 and it\031s for good reason. This was something I had only talked about with close homies and hadn\031t heard too much publicly about. If my understand of that beef and the events that took place (like Drake allegedly sleeping with Ye\031s mother in law as well as Ye saying he found Kim in the bed with Chris Paul and there being videos of Drake wearing his jersey shooting shots) I can\031t understand why he would become \034cool\035 with a guy like that. Drake literally said on his new album, \034**linking with the ops, bitch I did that shit for j prince/I did it for the mob ties/feels like seventeen, two Percs, frog eyes/and I\031ve never been the one to go apologize/me I\031d rather hit \030em up one more time**\035 so I was just as confused that anyone would allow someone who\031s openly duplicitous in their friend circle. Maybe I\031m not as mature as I should be either because after a beef like that, we don\031t gotta be enemies. It\031s enough people dying as it is. But we surely don\031t have to be friends. And for Kendrick to touch on that it was a huge moment, the kind where you realize even in your most outlandish takes, you\031re not alone.\n\nI have not listened to Kodak Black, ever. If I\031m not mistaken when he came out I wasn\031t as musically accepting, as I was a backpack kid. As open as I am with music, I still have not downloaded a single album of his. So for someone like Kendrick to put his record ***Rich (interlude)*** on his platform, it\031s major. I sat and listened to every bar of Kodaks at least 15 times, which is much more than I would have when left to my own devices. I think tracks like this are important, not just to expand horizons but also build more brotherhood in a business that we invented but do not control. It didn\031t make me a fan, but it did make me realize he definitely has something to say, which I\031d argue is more valuable. Fans have expectations but if I think you have something to say I may give you multiple chances to do that.\n\n***Rich Spirit*** is another example of what Kendrick brings this offering. A super catchy hook, with the new mind state courtesy of everything he\031s been through as well as talks with his therapist. \034**Rich n\\*\\*\\*a, broke phone/tryna keep the balance, im staying strong**\035 sounds like the words that got him thru the last 5 years of not releasing any music. It shouldn\031t be undervalued to be the type of artist that can make music other artists will gravitate towards. Kendrick floats effortlessly all over this track, which is absolutely typical. I feel like some things are worth mentioning and some aren\031t. In 2022, if you\031re not aware of the typical things that Kendrick does, this review probably isn\031t for you. I\031m of the beliefs two type of people are coming to this review. People who want to see their favorite album of the year celebrated, or people who didn\031t like it and want to see why others celebrated it. By the end I hope to answer both. This is the Kendrick that I will take with me for the rest of my life. Realistically this album covers a lot more themes and issues I\031ll be dealing with moving forward, where as songs like Backseat Freestyle don\031t hold as much weight as they did when I was in college. This is inevitable. Huge Jay Z fan growing up, but I play only 3-5 of his albums now. In my opinion there should be mutual growth. Human nature is to evolve. Rich spirit showcases some of what we can expect from Kendrick at this stage in his life, and content wise I believe this is as good as it gets. Personal confessions over minimal production will always have a warm place in my heart anyways so maybe I\031m biased, but when someone is vulnerable and can show their flaws as well as their triumphs, I don\031t think there\031s anything better when it comes to music. The original purpose of music was to convey a message, so its great to see thats not lost here\n\n***We Cry Together*** has to be in the top 10 of all time for hip hop love songs. Yes, I said it. Let me be clear. Is this a toxic relationship on display? Absolutely. It\031s hard to argue that saying \034fuck you\035 constantly back and forth isn\031t toxic. BUT. Who is really able to fully pass judgment? Romeo and Juliet is an amazing love story. But how many times have you seen a guy/girl willing to die rather than live without their love? Now how many love stories have you heard of like We Cry Together? Much more common. It reminds me of a 2pac line \034**they say Jesus is a kind man, but we should understand times in this crime land**\035. To me, there should always be a variable when you add up any equation that should represent how things change. So if you take away all the judgment, this song is beautiful. It\031s on the list of songs that I don\031t often play anymore, which throws back to my original point that Kendrick is introducing two different types of music here. The kind you\031ll keep with you in your pocket, as well as a song you could hear once and it will change your life (Kim anyone?). The dialogue here is also TOP NOTCH. He didn\031t just do a toxic love song, he did THE toxic love song that anybody could play and get a good glimpse of the gender relations of today. It\031s even more appropriate that their \034f**uck yous**\035 turn to \034**fuck** **mes**\035 once the couple has vented all their negative emotions. Even with the title you have to respect his intelligence to take the typical phrase \034we love together\035 or \034we pray together\035 to one of its natural opposites. That\031s the truth of life that not every artist touches on. To throw back to an earlier record, he\031s just \034**tryna keep the balance**\035 and i couldn\031t appreciate it more\n\nIm just gonna say this and leave it here. \034**Shut the fuck up when you hear love talking**\035 is one of the best lines on the album. Hell, in the last 10 years of rap its one of the best rap lines. Why? Everybody remembers, \034**i am the beast, feed me rappers or feed me beats**\035 right? To me, this line has an equal amount of staying power. Lil wayne was capturing a whole mixtape era with two sentences. So with Kendrick\031s line, i feel like post pandemic, this is equally as important. Personally i think thats why subreddits like r/wholesome or r/brosbeingbros are flourishing. People want to sit silently and watch moments of humanity being as great as we can, because we have all watched the ugly side of humanity for at least the last two years. When i heard this song it instantly put me in such a good mood. Kendrick and summer and two vocals i wouldn\031t have requested to be overlapped before, but now hearing their chemistry i can\031t be the only one hoping summer doesn\031t show up again, especially considering the days when Kendrick, or most of TDE showing up with Jhene or Sza are over. Ghostface showing up is everything to me. He delivers a very potent verse, but its more important to me that a kid from Compton recognizes one of the Wu Tang greats. While the first half of the CD definitely had some moments for me that made it worth the wait for Kendrick, in all honesty i was a little let down my first listen. Not because its bad music, not at all. I just by the highs. *Die Hard*, *N95*, *Rich Spirit*, and ***Purple Hearts*** are such high highs that in comparison every track didn\031t match up to me. I know what you\031re thinking, how on earth could this be one of his favorite albums of the year if he\031s said he only really likes 40% of the first disc? Well&\n\nIn case you didn\031t know, ***Count Me Out*** is one of the best intros in years. Not only that, but this is that record that your favorite, and your favorite, and his favorite all recognize as a kind of \034hey, this is how well i rap\035 record. Its just top tier lyricism. The way the choir in the background just kind of ushers you down the aisle as Kendrick spills in-between claps feels like an opening to a Devil May Cry or an Elden Ring type game (if they knew how to pick rap music that is). This is one of the rare occasions where, even for Kendrick Lamar, someone who has always had a good flow on any song, showcases one of his best flows ever. @ somebody you want to debate that with as long as it isn\031t me. Maybe its the way the bass hits underneath Kendrick saying \034**fuck** **it** **up**\035. Maybe its the song structure that sounds more like a two verses with a million bridges in-between. \034**i made a decision, never give you my feelings, fuck with you from..fuck with you from a distance**\035 is a prime example. The way it feels, its like watching the million man march form person by person as Martin Luther King speaks slowly rising in tone until the entire town can hear him. But I\031m a pot smoking imagineer don\031t mind me lol. There is no wasted bar here period. \034**When you was at your lowest tell me where the hoes was at/when you was at your lowest tell me where the bros was at**\035 is definitely not the typical statement from a rapper on top. This kind of self awareness is present all over the album, but not in the way that you feel you\031ve attended a college lecture, but more like you met a wino who had 4:42 minutes of game from all the mistakes he\031s made to give you. If you can play this record for day ones and \034just heard him todays\035 then it his the universal chord. The same reason Micheal Jackson could reach the whole world with just his voice. Rich or poor, i doubt theres a single person who can\031t relate to \034**you said id feel better if i just worked hard without lifting my head up/that left me fed up/you made me worry/i wanted my best version but you ignored me/then changed the story/then changed the story/**\034. Thats the beauty of a talent like Kendrick. Even i made the mistake of counting out Kendrick when my friend told me whenever he comes back the world will listen. This record made me realize i couldn\031t have been more wrong.\n\nOne thing I\031ve always been curious about, is how Kendrick sees himself. I know how the world does. Ive been a huge J Cole fan long enough to have had one or two conversations about who i think is doing the best from that class. Kendrick was more often than not peoples answer. Ive always had an affinity for risk takers and people who will do an entire project in such an artist fashion that people who don\031t love music can\031t enjoy it as much as someone like me. 4 Your Eyez Only comes to mind. You\031ve heard it a million times. If \\_\\_\\_\\_ did that, it would have been the biggest thing the world. It seems like a digression, but its not, because this is why I\031ve always wondered what does Kendrick think. Sure he did control. But does he really think he\031s the best out? Or does he hear bars like Big Krits verse on \0341 Train\035 and wish he could have did that? ***Crown*** kind of answers that. For better or worse, he acknowledges he wears a crown while saying things like \034**i can\031t please everybody**\035. One of the more shocking statements from an artist who, to me, seems to have done exactly that thus far. Sure, there are people who don\031t like this album or that album. But everybody has at least one Kendrick album they enjoy, unless they don\031t enjoy him at all. They are in the minority. \034**They idolize and praise your name across the nation/tap they feet and nod their head for confirmation/promise that you\031ll keep the music in rotation/thats what i call love**\035 could be taken as a brag, but to me it sounds like he\031s aware of the double standard. When you\031re of a certain caliber, people love you. But their love comes with the condition that you produce in the same fashion that you have. Don\031t believe me? Check out r/FrankOcean when you get a chance. Love without expectations isn\031t something most people have ever received, let alone given. \034**Heavy is the head that wears the crown/to whom its given, much is required now**\035 further cements my point. \034**One thing I\031ve learned, love can change with the seasons**\035 drills home that he overstands how love works, but he doesn\031t sound fazed. One thing is for sure from this record, Kendrick sees himself as a king of the game, even if its a reluctant title\n\n***Silent Hill*** reminds me of his earlier works. Mainly the hook reminds me of the love hate relationship i have with similar hooks (Hood Politics was corny at first, but then i came to love it after so many listens). This one, however, i jumped straight to loving. The meticulously placed hard hitting bass might have helped, and maybe its Kendrick finding his pocket constantly in such an esthetically pleasing way, but somehow \034**pushing them all off me like hhuhhhhh**\035 translates instantly even with the extreme animation. Honestly, even with how i initially receive records like this from, i still think its dope that he does it. This is the one glimpse of kendrick we have constantly gotten, and it goes further back than you\031d think, Cut You Off being one of the earliest examples i can think of. The animated type hooks should be a signature of his and thats dope because its something he enjoys, not something he\031s doing to make the most pop record ever. THIS Kodak verse did it for me, definitely one of my favorites. If it wasn\031t for Baby Keems performance, this would have been my favorite feature even over the Ghostface because it came from someone i didn\031t expect it. Speaking of Keen&\n\n***Savior (Interlude)*** is my favorite interlude. But its not fair at all, because *The Melodic Blue* was one of my favorite projects that year. So when i heard him going in, i was on board. Its lines like \034**the engineer dead if the drive don\031t back up**\035 and \034**my uncle had told me the shit in the movies could only be magic/this year i did 43 shows, and took it all home, to buy him a casket**\035 that show exactly what i mean when i say I\031m a fan of his. The former being a reference to the Atlanta episode (this isn\031t confirmed but i don\031t care lol it fits too well) when they were in the studio with the Clark county guy? His name escapes me but he made the yoohooo record. They sat as he recorded and as he did a freestyle type verse, the engineer said the computer crashed. Clark then proceeds to say \034don\031t crash it again\035 \034if it crashes again, imma crash my foot in your ass\035. Mind you, its definitely a technical issue, but thats the hilarity. The program ends up crashing again, where Clark takes a walk and his two goons say \034y\031all should go home\035 to Darius and PaperBoi which is the universal sign from one black person to another that some illegal shit is about to go down and y\031all not gonna want to witness it. The second line is what makes Keem great in his own right, he has a different type of awareness thats just as potent as the one Kendrick has found on this album. The idea that a family member would basically tell you that all the stuff in the movies is make believe, only for you to go out on a world tour that would have made him a believer had he lived to see it. Profound perspective delivered in an entertaining way is Keem.\n\n***Savior*** is another high high undeniably. \034**Are you happy for me**\035 became one of the questions everybody was asking post this album\031s release, and rightfully so. He said it best, \034**Kendrick made you think about it, but he is not you savior**\035 If you didn\031t notice the cover art, he has a crown of thorns with is a well known reference to the story of Jesus\031s crucifixion. \034**One protest for you, 365 for me**\035 is the perfect way to say that if he stands up for something, thats his view now. Whereas a person can protest two opposite ideas and nobody says anything. \034**Smile in my face, but are you happy for me? I\031m out the way, are you happy for me**\035 sounds like the response to people who said Kendrick should have been front and center when the world \034needed him\035. \034**see the christians say the vaccine mark of the beast/then he caught COVID and prayed to pfizer for relief/then i caught COVID and started to question Kyrie/will i stay organic or hurt in this bed for two weeks?**\035 is suchhhh a potent line. This is the duality of a lot of americans. I refuse to get political. So many people claimed it was this or that. But when their life was in need, their prayers are altered then and only then. Personally i don\031t believe in this \034opinion culture\035 where you\031re asking basketball players about health policies and rappers about socialism as if thats the source of the best info. 9 times out of 10 you get the same answer you\031d get if you asked a 40 year long teacher of math only about what happened april 1921 in oklahoma. A good guess, but its not his expertise. \034**The cat is out the bag, i am not your savior/i find it just as difficult to love thy neighbors**\035 needs no explanation. \034**the struggle for the right side of history/independent thought is like an eternal enemy/capitalist posing as compassions be offending me/yeah suck my dick with authenticity**\035 captures what i don\031t think a lot of people understand. We are in a herd mentality time, where if you don\031t agree with the herd, youre wrong. You\031re either good or bad, no in-between. The second half of that line directly made me think back to the pandemic year, when crimes by the cops against minorities were at a high, and every company came out and said \034we stand with you\035. Mind you, they\031re still supporting the people in power. Mind you, they aren\031t actually throwing their weight around to ensure that black lives matter. No, its easier to say these things because you\031re selling a product and you want to SEEM progressive. Seem being the operative word as most American companies either have blood or racism all over their hands, or both. Its just hard to trust statements from these companies knowing full well when things die down its back to business as usual. My favorite line from this song is \034**and they like to wonder where I\031ve been/protecting my soul, in the valley of silence**\035\n\n***Auntie Diaries*** is probably one of the most divisive records on the album, if not the most. In the times we are in, everybody is sensitive and nothing can be said. I don\031t really see how we can ever progress as humanity, if every time somebody does a singular thing we don\031t like, we shut down and ignore them completely. There was a Dave Chappelle joke about the LBGTQ movement on one of his specials where he compared it to people on a car ride. I won\031t butcher the joke, but it was brilliant. It even spoke to what some may see as tension with african americans and the LGBTQ movement, but again I\031m paraphrasing \034We aren\031t mad at your movement, we admire it because you\031ve gotten so much further than we have in a short time\035. It basically highlights that whatever movement white people are apart of, gets more traction while we are still fighting for equal treatment across the board. I brought this up to hopefully highlight, even with the best intentions, people can still get offended. This is what I\031ve found to be the case on this record, as i heard many people shaming him for \034mixing pronouns\035 or the past tense way he spoke, or even phrases like **\034my auntie is a man now**\035, while clearly rooted in him explaining he understands the problems the community faces, as well as taking accountability. While its not pleasing for everybody to hear, there was a period where saying somebody was \034gay\035 was an insult and people used \034f\\*\\*\\*\\*\\*\035 constantly for a number of different reasons. This happened. As much as american loves the \034my hands are clean now\035 approach, the fact still remains a lot of those things were not that long ago. I loved this record because it was an honest discussion on how we saw it from his point of view, and how he\031s changed. But the world sees records like this as tone deaf. Makes me think back to a coment I made recently. If we censor the Dave Chappelles, what can we expect from further generations but more Kevin Hart?\n\n***Mr. Morale*** contains my second favorite flow for the entire album. The records where Kendrick\031s rapping ability overshadows what he talked about are almost non existent on this album, which while thats refreshing, i also appreciate a silent hill or mr morale existing on such a heavy album. \034shit on my mind and its heavy/tell you in pieces cuz its way too heavy\035 captures that idea exactly. When you add in lines like \034transformation, i must had a thousand lives and like 3 thousand wives\035 the picture is clear. Honestly, that could have been the tag line for the entire album, because as this is still the Kendrick we have come to know from a distance over the years, this is the first up close view of his life. It would seem that in the years in-between he went through some hyperbolic time chamber to exponentially increase all his stats in a small amount of time. While he does touch on subjects that are lesser known, the next track is a much more concise journal of those\n\n***Mother I Sober*** is probably the one track that captures everything that Kendrick has been through both before fame, during, and after DAMN. I would basically have to paste every lyric to really capture the depth here, but lines like \034y**ou haven\031t felt grief until you felt it sober**\035 and then the flip \034**you haven\031t felt guilt until you felt it sober**\035 punctuate the verses they come from. Truly not a song to be spoiled until you hear the way Kendrick delivers it, it\031s the ultimate example of a song changing your life and perspective even if it\031s just heard once. I, on the other hand gravitate to this album because I\031ve had so many similar experiences, this song is like a silent last course for an album completely based in being food for thought. \034**I wish I was somebody, anybody but myself**\035 could not resonate more with me. Kendrick did what he hasn\031t before with this album to me, and after I discuss mirror I\031ll explain\n\n***Mirror*** is my favorite record. It\031s the first record I ever wanted to replay, from literally my first listen I doubled back a few times before hearing the album. \034I choose me\035 is undeniably one of the most powerful lyrics. I can only speak from experience, and after battling depression and dealing with never truly wanting to be who I was, I finally learned that I had to choose me. I had to make an effort to maintain my sanity above all else. You can be super giving, but you have to know when you\031re on 5% and don\031t have any battery to share. The hook is the most simple of the entire album outside of Silent Hill, yet for the purpose of a hook being something you\031d want to repeat, \034**I choose me I\031m sorry**\035 is the highest on the list for me. I found myself grooving in traffic multiple times like I was at a jazz concert with the green incense burning. With some of the final bars of the album, Kendrick doesn\031t miss saying \034**I can\031t live in the matrix/rather fall short of your graces/this time I won\031t trade places/not about who\031s right who\031s wrong/evolve the only known/ask me when I\031m coming home/blink twice and then I\031m gone**\035. This hits hard for me because to me Kendrick has always did music from someone else\031s perspective. He\031s aware of it, and saying this time I can\031t do that. I choose me. \034**You won\031t grow old waiting on me**\035 is another direct response to people demanding his time and presence. Not one to miss the moment, he even has choice words for hip hop herself when he says \034**She told me she need me the most, I didn't believe her/she even called me names on the post, the world can see it/jokes and gaslightin'/mad at me 'cause she didn't get my vote, she say I'm trifflin\031/disregardin' the way that I cope with my own vices/maybe, it's time to break it off/runaway from the culture to follow my heart**.\035 It\031s especially profound that someone would come off a hiatus, to put people back on notice that you just might not be as into it as you were. He\031s not saying flat out he\031s gonna stop, and most likely he has a couple more albums in him before he stops, but it\031s one of the most interesting things said on on the album. It makes you wonder was Kendrick on a break because of x, y or z, or was he testing the waters with stopping all together?\n\nSo one of my original points was that I related to this album more than any other Kendrick album. I touched on why randomly here and there, but the main point is that before, all of his albums felt like they were about something or for specific people. You never truly got a view into Kendrick\031s inner workings outside of a bar here or there, or songs like \034u\035, so he always had this mystique. If I met Kendrick then, I\031d probably have referred to him as k. dot or something informal. That\031s not to say I didn\031t like his albums, quite the contrary. TPAB is perfection. But, for me, when I hear an artist being vulnerable and baring all, that\031s what I relate to the most. Not Jay Z \034Dirt Off Your Shoulders\035, but \034Song Cry\035 or \034Regrets\035. I don\031t have to agree with your every take. For me it\031s just like when you meet somebody. I\031m not that invested. But once you tell me you\031re into Star Wars we can have a whole conversation before I realize it\031s been a couple hours. So to me, that makes this the number one album in his discography. If I saw him at the airport, I\031ll probably say yoooo Kendrick you killed that album! Lol because it feels like he let me into his Tuesday and Thursday therapy session. Which means Kendrick took the pressure, and still found a way to add more layers without disappointing. Which is honestly all we can ask after a long hiatus right? Growth\n\nQuestions\n\n1. **Where is this on Kendrick\031s discography for you?**\n2. **How much of the album do you still play at this point? All or select few? And why?**\n3. **Regardless of the quality of music, what did you want from Kendrick\031s return? Were you satisfied?**\n4. **Do you agree with people like Kendrick and Chappelle for trying to bridge the gap or do you think we should all just never mention anything that doesn\031t directly concern us?**\n5. **Who\031s the best rapper in the game right now?**\n6. **(Bonus) What are your 5 favorite albums from Kendrick\031s freshmen class (Kendrick, Meek, Mac Miller, Big KRIT, Yelawolf, Cyhi, Lil Twist, Lil B, Diggy Simmons, Fred The Godson, and YG were all in this photo but for the purposes of this question add in J Cole, Big Sean, Wale, and Drake)**
5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \nWill it be Collapse by a thousand cuts\024or one big gash?\n\nThis is *Last Week in Collapse*, a weekly self-post, bringing together some of the most important, timely, ironic, useful, demoralizing, stunning, or otherwise must-see moments in Collapse. \n\nThis is the 42nd edition. You can find the October 1-7 edition [here](https://old.reddit.com/r/collapse/comments/xyrca2/last_week_in_collapse_october_17_2022/) if you didn\031t catch it last week. If you don\031t want to miss an edition, you can [sign up for the SubStack](https://substack.com/profile/18092228-last-week-in-collapse), and upvote this so more people see it. \n\nThe soldier who took power in Burkina Faso\031s latest *coup d\031\xe9tat* was allegedly going to step down and [be replaced](https://www.france24.com/en/africa/20221008-burkina-faso-to-pick-a-transitional-president-ahead-of-elections) by a transitional president in advance of a 2023 election\024but then the Council [chose him for the job](https://globeecho.com/news/africa/burkina-faso-captain-ibrahim-traore-appointed-transitional-president-until-the-2024-election/), and pushed \034elections\035 to 2024. Each successive power struggle divides the military and the common people, eroding trust and legitimacy that is increasingly exploited by jihadist groups wresting power from the government.\n\nHaiti has become so unstable that [it is **asking for foreign military aid**](https://www.bbc.com/news/world-latin-america-63181481) to establish humanitarian corridors and intimidate gangsters into receding into the background. The last UN Peacekeepers left 5 years ago.\n\nThousands of [Pakistanis are protesting](https://www.aljazeera.com/news/2022/10/11/thousands-protest-rising-violence-in-pakistans-swat-valley) in the Swat Valley over rising violence in the region.\n\nThe Tigray War in Ethiopia, and its associated supply blockade, is [leading to the deaths](https://www.bbc.com/news/world-africa-63166044) of many medical patients who are not receiving medicine and hospital supplies. Not to mention the ~500,000 already estimated [dead from violence &amp; famine](https://en.wikipedia.org/wiki/Casualties_of_the_Tigray_War) (so far) in the 23-month old Civil War.\n\nAmerican officials are [trying to reduce **fears of nuclear \034armageddon\035**](https://www.theguardian.com/world/2022/oct/09/biden-armageddon-russia-nuclear-threat-pentagon) as a result of the Ukraine War. It must be a coincidence that [Putin appointed a new commander](https://www.bbc.com/news/world-europe-63217467) nicknamed \034General Armageddon.\035 [Power was restored](https://www.reuters.com/world/europe/power-supply-restored-zaporizhzhia-nuclear-plant-energoatom-2022-10-09) to the nuclear power plant (Europe\031s largest) at Zaporizhzhia, so a meltdown seems unlikely for the time being. The plant\031s emergency diesel generators (which are not currently active) have enough fuel now for 8 days.\n\nFrance is [running out of petrol](https://news.sky.com/story/third-of-petrol-stations-out-of-fuel-in-france-as-strikes-continue-12717708), too. **Fuel refinery worker strikes have entered their third week**, and lines have grown long across the nation. France\031s refined **oil supply has dropped by more than 60%**, and [more than 25%](https://www.thesun.co.uk/travel/20071679/france-holiday-warning-petrol-shortage/) of gas stations have closed. Some motorists are being turned away because they are judged to have enough petrol in their tanks already, and drivers with \034priority occupations\035 are given preference. Is this approach a model for how governments manage our future famine(s)? [This weekly observation](https://old.reddit.com/r/collapse/comments/y0arpk/weekly_observations_what_signs_of_collapse_do_you/is2wqoy/) explains a little more what\031s going on in the Paris area.\n\nThe [perpetually hungry](https://www.aspistrategist.org.au/north-korea-could-be-headed-back-towards-famine/) nation of North Korea, after testing several missiles two weeks ago, says [they were simulations for a nuclear attack on South Korea](https://www.bbc.com/news/world-asia-63196618), ahead of an **expected nuclear test** coming within a few weeks.\n\nRussia [struck Kyiv with a flurry of missiles](https://www.theguardian.com/world/2022/oct/10/explosions-kyiv-ukraine-war-russia-crimea-putin-bridge) on Monday morning, hitting Zelenskyy\031s office (he was not there), as well [as a German consulate](https://euroweeklynews.com/2022/10/10/breaking-german-embassy-in-kyiv-hit-by-russian-strikes/) building. The strike was reportedly retaliation for the Ukrainian ~~(?)~~ sabotage of the Crimean/Kerch bridge a few days earlier. President Putin [claims there will be no more \034massive strikes\035 in Ukraine](https://www.bbc.com/news/world-europe-63255617) for the time being, and claimed that he will have mobilized 300,000 soldiers by the month\031s end; 220,000 have allegedly been mobilized already.\n\n**Belarus** [**is positioning its troops** near the Ukraine border](https://www.telegraph.co.uk/world-news/2022/10/10/fears-new-invasion-putin-lukashenko-form-joint-task-force-ukraine/), a signal that they are likely to ~~join~~ be dragged into this conflict by Russia, deployed against Ukraine\031s northwest oblasts. [Moldova](https://www.aljazeera.com/news/2022/10/10/moldova-says-russian-missiles-that-hit-kyiv-crossed-its-airspace) may be drawn into this War in the not-too-distant future, too.\n\nSome people are talking about the potential for ~~worldwide annihilation~~ [nuclear war to counterbalance the effects of global warming](https://www.newsweek.com/fact-check-nuclear-war-climate-change-global-warming-1750274). Methinks maybe they misunderstood what made the Cold War cold.\n\nDr. Fauci [is warning](https://www.cnbc.com/2022/10/07/dr-fauci-new-more-dangerous-covid-variant-could-emerge-this-winter.html) that **an even more dangerous COVID variant could emerge this winter**. &gt;!Are people still reporting their positive cases to their governments, or have we all moved past that? I\024and [many others](https://old.reddit.com/r/collapse/comments/y2j1pa/the_data_is_clear_long_covid_is_devastating/is3ws1h/)\024can\031t tell who has Long COVID these days, and who just has late-stage [*Weltschmerz*](https://en.wikipedia.org/wiki/Weltschmerz). No doubt the two afflictions are often related.!&lt;\n\nIndia is at the forefront [of the growing **superbug crisis**](https://www.bbc.com/news/world-asia-india-63059585), wherein various bacteria have grown resistant to many conventional antibiotics. The antibiotic-resistance problem already leads to the deaths of over 60,000 newborn babies every year worldwide. Seems like we already slipped into dystopia.\n\nWildlife populations [are in freefall](https://archive.ph/l77fK), particularly in South America, where they have reportedly lost 94% of wildlife population in the last 50 years\024among 32,000 monitored species. Worldwide, a **loss of 69% of biodiversity** has been ~~lost~~ sacrificed, based on the 60-page [WWF 2022 Living Planet Report](https://wwflpr.awsassets.panda.org/downloads/lpr_2022_full_report.pdf). Freshwater populations are down 83% worldwide.\n\nTo highlight one example, penguins in Antarctica [are dying off](https://www.theguardian.com/world/2022/oct/12/australian-scientists-observe-rapid-decline-in-adelie-penguin-numbers-off-antarctic-coast); their population has dropped 43% in the last decade. Researchers blame overfishing (which depletes their food supply) and climate change.\n\nBrazil [broke a September record](https://www.rte.ie/news/world/2022/1007/1327812-brazil-deforestation/) for how much of the Amazon rainforest they deforested. 1,455 km\xb2 (562 mi\xb2) of the Amazon was lost last month; equivalent to slightly larger than the size of the Greek island of [Rhodes](https://en.wikipedia.org/wiki/Rhodes). \n\nFlooding in Venezuela [killed 39+](https://www.rte.ie/news/world/2022/1011/1328427-venezuela-landslide/) with dozens more missing. Rising floodwaters [are swallowing homes\024and humans\024in Nigeria](https://edition.cnn.com/2022/10/10/africa/casualties-latest-nigeria-flood-intl/index.html). Flooding in Nepal [killed at least 33](https://www.bbc.com/news/world-asia-63224454) last week.\n\nCanada [is talking about relocating people](https://www.cbc.ca/news/politics/fiona-climate-change-relocation-maritimes-1.6614604?cmp=rss) away from regions likely to be hit by natural disasters. Canada\031s [disaster budget is already overspent](https://www.cbc.ca/news/politics/disaster-adaptation-fund-money-1.6613203), and the government is reportedly releasing a revised plan later this year.\n\nAmerican rivers [are drying up](https://www.wbrz.com/news/drought-conditions-drop-mississippi-river-waters-exposing-19th-century-shipwreck/), revealing old sunken ships\024and new riverfront real estate. The mighty Mississippi River has gotten so low that [shipping barges, ferries, and recreational boats](https://eu.usatoday.com/story/news/nation/2022/10/09/mississippi-river-low-water-halt-key-commerce-travel/8229047001/) cannot safely traverse parts of the river. All the world\031s [crises are overlapping](https://www.smh.com.au/national/fires-floods-pandemic-the-age-of-overlapping-crises-20221011-p5bort.html) now, and sifting out cause-and-effect is an impossible task.\n\nEswatini [broke an October heat record](https://twitter.com/extremetemps/status/1579383733387304960) last week, at 46 \xb0C (115 \xb0F). T\xfcrkiye and Iran [also broke October records](https://twitter.com/extremetemps/status/1578827597961900033) for heat. The **Atlantic Ocean surface temperature** [**is 0.2 \xb0C warmer**](https://twitter.com/LeonSimons8/status/1579923715868934145) than it was in 2021&\n\nAll of Scotland [is now snow-free](https://www.bbc.com/news/uk-scotland-highlands-islands-63184780), an occasion thought to have happened just 9 times in the last 300 years. 7 of those snow-free times occurred within the last 30 years.\n\nRwanda [is trying to reforest land](https://allafrica.com/stories/202210100411.html) with drought-resistant trees, in preparation for a hotter, drier future. Forest cover dropped about 2/3rds in the last 80 years, something the Forestry Authority blames on \034different anthropogenic activities and resettlement of refugees.\035\n\n[Hungary is recruiting](https://archive.ph/bBCTe) a few thousand of its citizens to become **migrant \034hunters,\035** mostly on the Hungary-Serbia border, but also patrolling the fenced border to Croatia (which is in the EU but not yet the Schengen Area). Latvia has also [been accused](https://www.bbc.com/news/world-63237670) of mistreating migrants weaponized by neighboring Belarus.\n\nThe global economy continues [its slow plunge](https://archive.ph/SGybg). U.S. inflation \024 the CPI, which is &gt;!a price index for \034food and beverages, housing, apparel, transportation, medical care, recreation, education and communication, and other goods and services\035!&lt; \024 reached its [highest level in 40 years](https://www.straitstimes.com/business/economy/us-inflation-rises-to-40-year-high-paving-way-for-another-big-fed-hike). Growth possibilities are limited because [the global oil supply is strained](https://finance.yahoo.com/news/world-worried-saudi-aramco-world-160000039.html) and **energy prices have made operating many businesses unprofitable**. \n\nUnsurprisingly, [LNG tanker rates are are all-time highs](https://oilprice.com/Latest-Energy-News/World-News/LNG-Freight-Rates-Hit-Record-High-As-Europe-Races-To-Secure-Gas.html). The price to hire one of these massive, bulbous ships has increased more than 5x this year, and LNG demand is up 65% this year.\n\n**Food prices continue to soar** across the world; in [Tunisia](https://abcnews.go.com/International/wireStory/empty-shelves-unaffordable-food-tunisias-crisis-deepens-91320714), tremors of violence lurk behind economic unrest. [In India](https://www.channelnewsasia.com/asia/india-rising-inflation-hit-poor-fuel-food-prices-3001896), food &amp; fuel prices are pushing people back into poverty; [in Europe](https://euobserver.com/health-and-society/156260), pension funds are betting on food indices; [in Saudi Arabia](https://www.zawya.com/en/economy/gcc/saudi-inflation-up-31-in-september-as-food-prices-soar-xu2ix3bs); food price increases are slowly worsening inflation; [in Argentina](https://www.euronews.com/2022/10/14/argentina-inflation-poverty), people are scavenging landfills for things to trade for money/food& [In Australia](https://www.cnbc.com/2022/10/05/australia-inflation-rising-food-prices-are-hurting-restaurants-diners.html), produce costs have skyrocketed; [in Sudan and Somalia](https://finance.yahoo.com/news/global-survey-reveals-food-prices-202200209.html), food scarcity is triggering a slow famine. [Economic projections for next year](https://old.reddit.com/r/collapse/comments/y3fyo8/bofa_warns_that_the_us_economy_will_start_to_lose/) aren\031t exactly sanguine, either&\n\nIranian police [have been machine-gunning fleeing protestors](https://www.bbc.com/news/world-middle-east-63253724) at some of the protests, which have spread to more than 100 cities and could possibly result in the collapse of the regime. It is said that nations collapse slowly\024and then all at once; this is the opposite pattern for the collapse of an insurgency movement. Whether these [repressive measures](https://www.bbc.com/news/world-middle-east-63218963) are just another milestone in the slow-motion Collapse of Iran remains to be seen.\n\n&gt;"We used to be afraid of the {Iranian} regime, but now **the wall of fear has collapsed**. Nobody is frightened anymore&the regime won't collapse. It can't be changed. They are strong and they keep killing people. **We will never stop, and so they will keep killing us**&It's crazy, and it's corrupt. Nobody cares about us. The outside world says it supports Iran, but nobody does. We are being tortured and killed every day." -quotes from a questionable [BBC article](https://www.bbc.com/news/world-middle-east-63218963) \n\nMexico [is keeping its soldiers on the streets](https://www.bbc.com/news/world-latin-america-63241024) for the next 6 years, the government overwhelmingly decided. The military is a large part of Mexico\031s failing counterinsurgency practices to combat drug trafficking and the spread of gang violence which threatens the legitimacy/sovereignty of the state.\n\n[Tensions have been rising in Israel/Palestine](https://apnews.com/article/middle-east-jerusalem-israel-arrests-d518e1186fd89a7c4325ef8d5d696cf5) in the days after [a mystery shooter killed an IDF soldier](https://www.timesofisrael.com/palestinians-in-east-jerusalem-nablus-hold-strike-amid-manhunt-for-shuafat-shooter/) outside a refugee camp. The camp was put into lockdown so security forces could track the killer, but Palestinians began a general strike in East Jerusalem a couple days in. Another IDF soldier was killed days after the first, escalating violence in [an already violent year](https://www.middleeastmonitor.com/20221010-un-100-palestinians-killed-in-2022/). These **cycles of fear and vengeance** may prove impossible to escape.\n\nThe son of the 36-year-long (\034freedom fighter\035) President of Uganda bizarrely [claimed](https://www.capitalfm.co.ke/news/2022/10/musevenis-son-muhoozi-asks-president-ruto-to-forgive-him-following-kenya-invasion-tweets/) that the Ugandan military would invade and conquer Nairobi in two weeks. The boast was met with widespread ridicule and condemnation by the Ugandan government. Who will take over when the 78-year-old Museveni dies? \n\nThe [**Ebola outbreak in Uganda**](https://archive.ph/4JNmJ) is the country\031s worst in 20+ years; 39 are confirmed dead from it, and at least 54 infected. [**Cholera has reappeared in Syria**](https://reliefweb.int/report/syrian-arab-republic/cholera-spreads-across-syria-putting-vulnerable-people-serious-risk) for the first time since 2007, linked to contaminated Euphrates River water. Worldwide, [monkeypox numbers continue to decline](https://www.nature.com/articles/d41586-022-03204-7). The [risk of bird flu crossing over](https://www.theguardian.com/environment/2022/oct/06/bird-flu-an-urgent-warning-to-move-away-from-factory-farming) continues to hang over humanity&What would we do without our chicken?\n\n*Things to watch for next week include:*\n\n\xa0 China\031s President, Xi Jinping, is widely [expected to secure a third 5-year term](https://www.hrw.org/news/2022/10/10/china-third-term-xi-threatens-rights) at the CCP Congress next week. Nobody doubts that he will accomplish this; the question is rather: how will China\031s behavior change once this is achieved? China is still [maintaining its zero-COVID policy](https://www.aljazeera.com/economy/2022/10/10/xis-bid-to-extend-rule-dampens-hopes-for-zero-covid-exit), struggling with [a real estate/economic collapse](https://archive.ph/mCwb3), and alienating itself from other countries (even its old friend Russia).\n\nTo make matters more interesting, a daring, mystery protestor in Beijing [hung a large banner opposing Xi Jinping and China\031s zero-COVID policy](https://www.bbc.com/news/world-asia-china-63252559), and supposedly shared a manifesto online, calling for acts of civil disobedience. In an age of totalitarianism, can a single, dramatic act of defiance be considered an act of War?\n\n*Select comments/threads from the subreddit last week suggest:*\n\n-Looks like wood\031s back on the menu, according to [this post](https://old.reddit.com/r/collapse/comments/xz1zyi/firewood_demand_is_surging_as_europeans_return_to/) about the skyrocketing demand for firewood across Europe. One of my colleagues is from Germany, and he told me that his father is storing a tonne (1,000 kg, or 2,205 lbs) of coal in their basement, and another neighbor has already stockpiled a similar amount of firewood. Tell me again about our ~~soylent~~ green future?\n\n-Legendary r/collapse poster u/Myth_of_Progress linked [a small observation](https://old.reddit.com/r/collapse/comments/y0arpk/weekly_observations_what_signs_of_collapse_do_you/is5odpk/) citing the bonkers climate situation in Canada\031s southwest. Extreme heat, extreme cold, lengthy rain, and lengthy drought. The New Normal" is here.\n\n-There\031s a reason why so many reforestation programs are failing (many reasons, actually), and they are explored [in this thread](https://old.reddit.com/r/collapse/comments/xzlbju/phantom_forests_why_ambitious_tree_planting/) and its comments. \n\n-Healthcare systems around the world are overwhelmed by respiratory illnesses, [based on the link and comments to this thread](https://old.reddit.com/r/collapse/comments/y0xkcb/the_healthcare_system_is_under_stress_from/). The WHO [is still warning about **Long COVID**](https://www.theguardian.com/society/2022/oct/12/long-covid-who-tedros-adhanom-ghebreyesus), which they claim **1 in 3 women get; 1 in 5 men get Long COVID**, too, reportedly. This is not only a mass disabling event/process, but a mass psychologically destabilizing phenomenon, too. Can you feel your sense of reality under siege? Can you hold the line?\n\nHave any feedback, questions, comments, articles, news, death threats, home-heating advice, doomy prognostications, etc.? If you can\031t remember to check r/collapse every Saturday, you can join the [***Last Week in Collapse* SubStack**](https://substack.com/profile/18092228-last-week-in-collapse) and get this roundup sent to your email inbox every weekend. I always forget something; what did I miss this week this time?
6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n\nMy brother married this bridezilla last year.\n\nThis is a bit long but please bear with me. I will need to start the story with the events that happened 2 years ago.\n\nThe wedding should've been 2 years earlier and would have been held in our country but the pandemic happened so they had to reschedule it.\n\nTo make this easier let's call the bridezilla, "Anna".\n\nAnna was nice to us in the beginning. I only met her once and have been on video calls with her since I am always busy with my job.\n\nWhen she got comfortable with us, she started to ask us to do some errands for her (to be able to 'help' her and my brother).\n\nThe first thing she did was ask my sister to look for wedding decorations and produce wedding invitations and souvenirs according to her desired 'plans'. I am not that aware of what she wants but my sister told me that the materials she wants are very expensive. My sis gave her an estimate and she told her to look for 'cheaper' alternatives. This will go on for weeks and she\031s not even giving my sister some allowance for petrol. She also wants everything done as soon as possible (like we have no other stuff to do but serve her).\n\nMy sister informed me that Anna is kind of a slavedriver. After some time, she asked me to make blueprints for her \030dream house\031. I am a busy person that works for a minimum of 12 hrs a day (because of deadlines). I told her this and politely refused the offer to build plans for her. I recommended a trusted friend that can help her with it and was willing to give a discount on the plans. He quoted about $2000 for the plans which are cheaper by $200 (we are from a 3rd world country so it might be too cheap for others) compared to others. She said that it was too much and tried to look for someone else that can do it cheaper.\n\nAfter that, she rarely communicates with me. After a few months, I heard from my sister that Anna and her architect friend (who 'made' the plans) is talking shit behind my back. Saying that I am "worthless" and "wasted" my degree in engineering just because I wasn't able to help her with the plans. She probably thought I can give her plans for free too since she's my brother's fianc\xe9. \n\nThey ended up asking my cousin to work on the plans but my cousin discovered that the architectural plans Anna's friend provided were a copy of someone else's work (they literally just changed the title block) so my cousin ended up giving up on that job too.\n\nAfter a few months, the wedding was rescheduled and since my brother is busy with his work, he will only be able to have 2 weeks off for the wedding. Anna got livid! She forced my brother to go on an extended leave without the employer's approval (meaning he could lose his job). Since she wants to be able to go visit other countries for their honeymoon.\n\nThey decided to get married in South Africa. Since Anna isn't earning much, my brother probably ended up paying for everything. But since they recently bought properties, his money is not enough to bring the family with them. Instead, they will only be able to bring the parents and Anna's child. \n\nAnna wasn't able to provide the necessary documents to be able to get her child's passport though so only my mom and her mom were able to attend the wedding.\n\nMy brother provided my mom $2000 allowance so she can buy the stuff she wanted when they arrive. My mom was with them for 2-3 months (mostly at Anna's place). \n\nAfter a few weeks, I found out that my mom is going home by herself and that they took her allowance so Anna's sister and her child can go visit other countries too. \n\nI am okay with my mom going early though since we missed her and she did tell me once that she wants to go home early but she didn't spend much of the money since she was saving it for a few renovations in the house. \n\nEverything is fine until I found out from my sister that Anna was not feeding my mom properly during their stay at her place (my brother had to leave sometimes since he need to work to be able to recover from the wedding expenses). So while my mom was staying with Anna and her own mother, she would never ask my mom to eat. My mom is very shy and wouldn't get food on her own unless offered. This is a common thing in our country (Anna is aware of this). So then, my mom will only be able to eat when Anna\031s mother offers her food. \n\nMy mom wasn't able to buy her own food too since they took away her allowance as mentioned earlier. Also, Anna has a vibe that she doesn't like people going through her stuff so my mother doesn't look for food in the fridge or pantry.\n\nWhen they eat out, Anna would ask her mom what she wants and buys it for her. As for my mom, she would pick the cheapest food they have and would not even ask my mother's preference (my mom is not that adventurous when it comes to food and doesn\031t eat something that\031s spicy). It's a good thing that we have relatives in that country so there are times that my mother would visit them so she can eat properly.\n\nThe bitch also doesn\031t eat the same food twice a day. If there\031s leftover food (even if it\031s still good/edible) she would throw it in the trash. Not to mention, she always goes fine dining even if she doesn\031t earn that much.  \nShe has also a fear of missing out. Our family loves dogs so she got one too. Poor baby was kept inside a small cage for almost 24hrs a day. She lives alone with the dog so when she\031s at work and the dog is inside the cage, the dog got so stressed that it started to eat its own sh\\*t. Looking at her photos with the dog inside the cage in the background made me think she doesn\031t care about the poor thing at all. She then gave the dog away (which is a good thing since I heard the dog is getting along well with the new fur parents). It\031s sad that the dog had to go through a lot of stress though.\n\nMy brother and Anna have been married for a year now. I am surprised that he was able to survive. Anyways, I think he\031s at fault too since he spoiled her so much. He can\031t say \030no\031 to her demands. On top of the real estate properties they purchased, he even got her a ride. He told us that Anna \030paid for it\031 but I doubt that she can afford that on her own (considering the lavish lifestyle she currently has that doesn\031t match her income).  \nMy family treat her right every time she comes over to stay at our house for days. Mom cooks her whatever she likes. I really don\031t get why she had to treat my mom and sister like that.
        subreddit score upvotes downvotes up_ratio total_awards_received golds
1 entitledparents   528     528         0     0.99                     1     0
2            BBBY   527     527         0     0.97                     0     0
3       JUSTNOMIL   520     520         0     0.97                     0     0
4     hiphopheads   515     515         0     0.84                     0     0
5        collapse   515     515         0     0.98                    10     0
6     bridezillas   514     514         0     0.96                     0     0
  cross_posts comments
1           0       42
2           0       48
3           0       53
4           1      293
5           0       36
6           0       48
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{processed }\OtherTok{\textless{}{-}} \FunctionTok{textProcessor}\NormalTok{(pandemic\_threads}\SpecialCharTok{$}\NormalTok{text, }\AttributeTok{metadata =}\NormalTok{ pandemic\_threads)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Building corpus... 
Converting to Lower Case... 
Removing punctuation... 
Removing stopwords... 
Removing numbers... 
Stemming... 
Creating Output... 
\end{verbatim}

The \texttt{stm} package also requires us to store the documents, meta
data, and ``vocab'' in separate objects, essentially a list of words
described in the documents.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Eliminates both extremely common terms and extremely rare terms, since such terms make word{-}topic assignment more difficult.}
\NormalTok{out }\OtherTok{\textless{}{-}} \FunctionTok{prepDocuments}\NormalTok{(processed}\SpecialCharTok{$}\NormalTok{documents, processed}\SpecialCharTok{$}\NormalTok{vocab, processed}\SpecialCharTok{$}\NormalTok{meta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Removing 6469 of 11816 terms (6469 of 71023 tokens) due to frequency 
Your corpus now has 218 documents, 5347 terms and 64554 tokens.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{docs }\OtherTok{\textless{}{-}}\NormalTok{ out}\SpecialCharTok{$}\NormalTok{documents}
\NormalTok{vocab }\OtherTok{\textless{}{-}}\NormalTok{ out}\SpecialCharTok{$}\NormalTok{vocab}
\NormalTok{meta }\OtherTok{\textless{}{-}}\NormalTok{out}\SpecialCharTok{$}\NormalTok{meta}
\end{Highlighting}
\end{Shaded}

Then have to make another decision about the number of topics we might
expect to find in the corpus. Let's start out with 10. We also need to
specify how we want to use the meta data. This model uses the number of
``comments''. It's important to recognize that the variables selected in
this stage can have significant ramifications. If we make the wrong
choice, we could potentially miss identifying certain topics that are
discussed on both liberal and conservative blogs or mistakenly
categorize them as distinct subjects.

In addition, the \texttt{stm} package offers an argument that permits
the specification of the desired type of initialization or
randomization. For our purposes, we have chosen to use spectral
initialization. Please see
\href{https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html}{Bail,
C. 2020} for more.

This below code may take some time if you are running it on a large
body. You can read more about each function in the
\href{https://www.rdocumentation.org/packages/stm/versions/1.3.6/topics/stm}{package
documentation}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{First\_STM }\OtherTok{\textless{}{-}} \FunctionTok{stm}\NormalTok{(}\AttributeTok{documents =}\NormalTok{ out}\SpecialCharTok{$}\NormalTok{documents, }\AttributeTok{vocab =}\NormalTok{ out}\SpecialCharTok{$}\NormalTok{vocab,}
              \AttributeTok{K =} \DecValTok{10}\NormalTok{, }
              \AttributeTok{prevalence =}\SpecialCharTok{\textasciitilde{}}\NormalTok{ comments,}
              \AttributeTok{max.em.its =} \DecValTok{75}\NormalTok{, }\AttributeTok{data =}\NormalTok{ out}\SpecialCharTok{$}\NormalTok{meta,}
              \AttributeTok{init.type =} \StringTok{"Spectral"}\NormalTok{, }\AttributeTok{verbose =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We start by inspecting our results by browsing the top words associated
with each topic. The \texttt{stm} package has a useful function that
visualizes these results called plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(First\_STM)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{topic-modelling_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\end{figure}

The visualization provides information on both the occurrence rate of
the topic across the entire corpus and the top three words that are
linked to the topic. As you will notice, in a second iteration of the
model we may want to exclude words such as ``like''.

Some topics seem plausible, but many that do not seem very coherent or
meaningful. You may want to improve your topic classification with more
than one variable in the \texttt{prevalence} comments. Please see
\href{https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html}{Bail,
C. 2020} for more.

\hypertarget{limitations-of-topic-models}{%
\subsection{Limitations of Topic
Models}\label{limitations-of-topic-models}}

For various reasons, topic models have become a conventional tool for
quantitative text analysis. Depending on the application, they can be
more advantageous than simplistic word frequency or dictionary-based
methods. Generally, topic models yield optimal outcomes when utilized on
texts that are moderately lengthy and have a regular format.

On the other hand, topic models have a number of important limitations.
To start, the term ``topic'' is somewhat vague, and it is now evident
that topic models cannot generate extremely refined classifications of
texts. Furthermore, if topic models are incorrectly perceived as an
unbiased depiction of a text's meaning, they can be easily misused. Once
more, these instruments might be more accurately depicted as ``tools for
reading.'' It is not advisable to excessively interpret the outcomes of
topic models unless the researcher has solid theoretical prior knowledge
regarding the number of topics in a particular corpus, or if the
researcher has thoroughly verified the results of a topic model using
both quantitative and qualitative methodologies described earlier.

\hypertarget{questions-4}{%
\section{Questions}\label{questions-4}}

For the second assignment, we will focus on the United Kingdom as our
geographical area of analysis. As for
\href{https://www.population-science.net/sentiment-analysis.html}{Sentiment
Analysis chapter}, we will use a dataset of tweets about migration
posted by users in the United Kingdom during February 24th 2021 to July
1st 2022.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{twitter\_df }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"./data/sentiment{-}analysis/uk\_tweets\_24022021\_01072022.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Prepare the Twitter data so that it can be analyzed in the
  \texttt{topicmodels} package
\item
  Run three models and try to identify an appropriate value for k (the
  number of topics).
\item
  Create a chart of greatest differences between two relevant topics you
  have identified.
\item
  Use the \texttt{full\_place\_name} or \texttt{lat} and \texttt{long}
  variables as meta data to create classification between different
  types of places in the UK. For example: urban/rural, classified by
  population/density, or simply between different regions. There are
  plenty of ways you can divide the tweets geographically, see some easy
  examples
  \href{https://statistics.ukdataservice.ac.uk/dataset/2011-census-geography-boundaries-local-authorities/resource/928039eb-e75a-4648-814e-32498dcc5db6}{here}.
  Then explore whether there are differences in topics according to
  different locations in the UK using the \texttt{stm} package.
\item
  BONUS QUESTION: Discuss the limitations of topic models on short
  texts, such as Tweets. There have been a number of recent attempts to
  address this problem, and Graham Tierney has developed a very nice
  solution called
  \href{https://github.com/g-tierney/stLDA-C_public}{stLDA-C}.
\end{enumerate}

Analyse and discuss: a) whether there are different topics related to
migration that emerge, and what these are. b) how migration topics vary
spatially.

\textbf{The below code helps you geo-localise the Twitter data.} You
could then perform a spatial join to another sf objects that has
population density or classifies areas in the UK.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sf)}

\CommentTok{\# subset the data frame to remove rows with missing values in x and y columns}
\NormalTok{twitter\_df\_clean }\OtherTok{\textless{}{-}}\NormalTok{ twitter\_df[}\FunctionTok{complete.cases}\NormalTok{(twitter\_df[, }\FunctionTok{c}\NormalTok{(}\StringTok{"lat"}\NormalTok{, }\StringTok{"long"}\NormalTok{)]), ]}

\NormalTok{twitter\_df\_clean }\OtherTok{\textless{}{-}}\NormalTok{ twitter\_df\_clean }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  sf}\SpecialCharTok{::}\FunctionTok{st\_as\_sf}\NormalTok{(}\AttributeTok{coords =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{))  }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# create pts from coordinates}
  \FunctionTok{st\_set\_crs}\NormalTok{(}\DecValTok{4326}\NormalTok{)  }\CommentTok{\# set the original CRS}

\FunctionTok{plot}\NormalTok{(twitter\_df\_clean}\SpecialCharTok{$}\NormalTok{geometry)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{topic-modelling_files/figure-pdf/unnamed-chunk-11-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example spatial join. You can also specify other join types such as st\_contains, st\_within, and st\_touches depending on your analysis requirements.}
\CommentTok{\# cities \textless{}{-} st\_join(point, cities, join = st\_intersects)}
\end{Highlighting}
\end{Shaded}

\bookmarksetup{startatroot}

\hypertarget{sec-chp8}{%
\chapter{Modelling Time}\label{sec-chp8}}

Time is a critical variable in many areas of research, and its modeling
can help us gain insights into underlying phenomena. Timeseries data has
proven to be a valuable tool in understanding the COVID-19 pandemic. By
analyzing data over time, researchers have been able to track changes in
the spread of the virus, the effectiveness of interventions such as
social distancing and vaccination campaigns, and the impact of the
pandemic on various social and economic indicators. With the increasing
availability of digital footprint data, which includes information both
of offline behaviour such as visits to grocery shops and parks, as well
as online behavior such as internet searches and social media activity,
researchers have had new opportunities to understand the pandemic's
impact on individuals and communities (Ugolini et al. 2020; Schleicher
2020) and (Cinelli et al. 2020). These data sources provide insight into
people's attitudes, concerns, and behaviors during the pandemic, as well
as their response to interventions and policies. Together, timeseries
and digital footprint data offer powerful tools for understanding the
complex dynamics of the COVID-19 pandemic and developing effective
responses.

This chapter will introduce modelling time descriptively using linear
trends, quadratic trends, and splines. It will also provide and
introduction to modelling data over time and space.

\hypertarget{dependencies-3}{%
\section{Dependencies}\label{dependencies-3}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Data}
\FunctionTok{library}\NormalTok{(sf)}
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Dates and Times}
\FunctionTok{library}\NormalTok{(lubridate)}

\CommentTok{\# Regression Spline Functions and Classes}
\FunctionTok{library}\NormalTok{(splines)}

\CommentTok{\# graphs}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(dygraphs)}
\FunctionTok{library}\NormalTok{(plotly)}
\FunctionTok{library}\NormalTok{(ggthemes)}
\FunctionTok{library}\NormalTok{(ggpmisc)}
\FunctionTok{library}\NormalTok{(gridExtra)}
\FunctionTok{library}\NormalTok{(viridis)}
\FunctionTok{library}\NormalTok{(ggformula)}
\FunctionTok{library}\NormalTok{(ggimage)}
\FunctionTok{library}\NormalTok{(grid)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-3}{%
\section{Data}\label{data-3}}

We will use a sample of
\href{https://www.google.com/covid19/mobility/index.html?hl=en}{Google
Mobility data}. Google has made available Community Mobility data to
provide insights into what changed in response to policies aimed at
combating COVID-19. The data also has accompanying reports which analyse
movement trends over time by geography, across different categories of
places such as retail and recreation, groceries and pharmacies, parks,
transit stations, workplaces, and residential. Data is available from
2020 and 2022. Community Mobility data is no longer being updated as of
2022-10-15. All historical data will remain publicly available.

Location accuracy and the understanding of categorised places varies
from region to region, so it is not recommend the data is used to
compare changes between countries, or between regions with different
characteristics (e.g.~rural versus urban areas). Region that do not have
statistically significant levels of data have been left out of the
report.

Some important facts about the data:

\begin{itemize}
\item
  Changes for each day are compared to a baseline value for that day of
  the week. The baseline is the median value.
\item
  The data that is included in the calculation depends on user settings,
  connectivity and whether it meets our privacy threshold.
\item
  If the privacy threshold isn't met (when somewhere isn't busy enough
  to ensure anonymity) a change for the day is not shown.
\end{itemize}

For more about this data please see
\href{https://www.gstatic.com/covid19/mobility/2022-10-15_GB_Mobility_Report_en-GB.pdf}{here}.

\hypertarget{modelling-time}{%
\section{Modelling Time}\label{modelling-time}}

First we import the data we will be working with. We will be looking at
Google Mobility data for Italy.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mobility\_it }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/longitudinal{-}1/2022\_IT\_Region\_Mobility\_Report.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Check out variables in the dataframe

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check out variables in the dataframe }
\FunctionTok{colnames}\NormalTok{(mobility\_it)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "country_region_code"                               
 [2] "country_region"                                    
 [3] "sub_region_1"                                      
 [4] "sub_region_2"                                      
 [5] "metro_area"                                        
 [6] "iso_3166_2_code"                                   
 [7] "census_fips_code"                                  
 [8] "place_id"                                          
 [9] "date"                                              
[10] "retail_and_recreation_percent_change_from_baseline"
[11] "grocery_and_pharmacy_percent_change_from_baseline" 
[12] "parks_percent_change_from_baseline"                
[13] "transit_stations_percent_change_from_baseline"     
[14] "workplaces_percent_change_from_baseline"           
[15] "residential_percent_change_from_baseline"          
\end{verbatim}

\textbf{Long data vs.~Wide data}

Long data and wide data are two different formats used to store and
organize data in a tabular form. Wide data is a format where each
variable is stored in a separate column, and each observation or time
point is stored in a separate row. This format is often useful when the
number of variables is small compared to the number of observations, and
is often used for descriptive statistics and exploratory data analysis.

\begin{verbatim}
        dates           A          B
1  2022-01-01 -0.56047565  1.2240818
2  2022-01-02 -0.23017749  0.3598138
3  2022-01-03  1.55870831  0.4007715
4  2022-01-04  0.07050839  0.1106827
5  2022-01-05  0.12928774 -0.5558411
6  2022-01-06  1.71506499  1.7869131
7  2022-01-07  0.46091621  0.4978505
8  2022-01-08 -1.26506123 -1.9666172
9  2022-01-09 -0.68685285  0.7013559
10 2022-01-10 -0.44566197 -0.4727914
\end{verbatim}

Long data, on the other hand, is a format where each variable is
represented by two or more columns: one column for the variable name and
another for the variable values. This format is often useful when we
have many variables or when we want to perform statistical analysis.

\begin{verbatim}
        dates series       value
1  2022-01-01      A -0.56047565
2  2022-01-01      B  1.22408180
3  2022-01-02      A -0.23017749
4  2022-01-02      B  0.35981383
5  2022-01-03      A  1.55870831
6  2022-01-03      B  0.40077145
7  2022-01-04      A  0.07050839
8  2022-01-04      B  0.11068272
9  2022-01-05      A  0.12928774
10 2022-01-05      B -0.55584113
11 2022-01-06      A  1.71506499
12 2022-01-06      B  1.78691314
13 2022-01-07      A  0.46091621
14 2022-01-07      B  0.49785048
15 2022-01-08      A -1.26506123
16 2022-01-08      B -1.96661716
17 2022-01-09      A -0.68685285
18 2022-01-09      B  0.70135590
19 2022-01-10      A -0.44566197
20 2022-01-10      B -0.47279141
\end{verbatim}

Both wide and long data formats have their own advantages and
disadvantages, and the choice between them often depends on the specific
data and the analysis that is planned. Transforming data from one format
to another is a common task in data processing and analysis, and can be
accomplished using various data manipulation tools in R, such as
\texttt{tidyr} and \texttt{reshape2} packages. Look back at the Italian
google mobility data to check which format it is in.

\textbf{Date format}

Time series aim to study the evolution of one or several variables
through time. Several packages that are part of the \texttt{tidyverse}
family will help you analyse time series data in \texttt{R}. The
\texttt{lubridate}package is your best friend to deal with the date
format. \texttt{ggplot2} will allow you to plot it efficiently.
\texttt{dygraphs} will also help build attractive interactive charts.

Building time series requires the time variable to be at the date
format. The first step of your analysis must be to double check that R
read your data correctly, i.e.~at the date format. This is possible
thanks to the str() function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Checking date format}
\FunctionTok{str}\NormalTok{(mobility\_it)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   36576 obs. of  15 variables:
 $ country_region_code                               : chr  "IT" "IT" "IT" "IT" ...
 $ country_region                                    : chr  "Italy" "Italy" "Italy" "Italy" ...
 $ sub_region_1                                      : chr  "ALL" "ALL" "ALL" "ALL" ...
 $ sub_region_2                                      : chr  "" "" "" "" ...
 $ metro_area                                        : logi  NA NA NA NA NA NA ...
 $ iso_3166_2_code                                   : chr  "" "" "" "" ...
 $ census_fips_code                                  : logi  NA NA NA NA NA NA ...
 $ place_id                                          : chr  "ChIJA9KNRIL-1BIRb15jJFz1LOI" "ChIJA9KNRIL-1BIRb15jJFz1LOI" "ChIJA9KNRIL-1BIRb15jJFz1LOI" "ChIJA9KNRIL-1BIRb15jJFz1LOI" ...
 $ date                                              : chr  "01/01/2022" "02/01/2022" "03/01/2022" "04/01/2022" ...
 $ retail_and_recreation_percent_change_from_baseline: int  -65 -27 -13 -13 -10 -30 -20 -27 -39 -22 ...
 $ grocery_and_pharmacy_percent_change_from_baseline : int  -80 7 28 30 42 -24 25 6 -7 19 ...
 $ parks_percent_change_from_baseline                : int  21 12 19 17 10 42 13 3 -36 -18 ...
 $ transit_stations_percent_change_from_baseline     : int  -49 -18 -36 -37 -37 -50 -38 -30 -34 -35 ...
 $ workplaces_percent_change_from_baseline           : int  -69 -13 -42 -41 -42 -78 -48 -27 -13 -21 ...
 $ residential_percent_change_from_baseline          : int  13 6 13 13 12 23 16 9 10 10 ...
\end{verbatim}

This is already looking fine in the google mobility data, but in many
other cases the \texttt{lubridate} package is such a life saver. It
offers several function which name are composed by 3 letters: year
(\texttt{y}), month (\texttt{m}) and day (\texttt{d}). Have a look at
\texttt{lubridate}
\href{https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf}{cheat
sheet} to see more about converting time variables to useful formats.
There is also the anytime() function in the anytime package whose sole
goal is to automatically parse strings as dates regardless of the
format.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Convert the date column to a date format using the dmy() function }
\NormalTok{mobility\_it}\SpecialCharTok{$}\NormalTok{date\_fix }\OtherTok{\textless{}{-}} \FunctionTok{dmy}\NormalTok{(mobility\_it}\SpecialCharTok{$}\NormalTok{date)  }
\end{Highlighting}
\end{Shaded}

Tidy up some variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Rename variables}
\NormalTok{mobility\_it }\OtherTok{\textless{}{-}}\NormalTok{ mobility\_it }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{grocery\_pharmacy =}\NormalTok{ grocery\_and\_pharmacy\_percent\_change\_from\_baseline,}
         \AttributeTok{parks\_percent\_change\_from\_baseline =}\NormalTok{ parks\_percent\_change\_from\_baseline,}
         \AttributeTok{transit =}\NormalTok{ transit\_stations\_percent\_change\_from\_baseline)}
\end{Highlighting}
\end{Shaded}

\textbf{Initial time-series plotting with ggplot2}

Let's start easy by keeping only data for the whole of Italy.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter the dataframe to keep only the rows for all of Italy}
\NormalTok{mobility\_it\_nogeo }\OtherTok{\textless{}{-}}\FunctionTok{filter}\NormalTok{(mobility\_it, sub\_region\_1 }\SpecialCharTok{==} \StringTok{"ALL"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{ggplot2} offers great features when it comes to visualize time
series. The date format will be recognized automatically, resulting in a
neat x axis labels.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Most basic bubble plot {-} uncomment to visualise}
\CommentTok{\#timeseries1 \textless{}{-} ggplot(data = mobility\_it\_nogeo, aes(x=date\_fix, y=grocery\_pharmacy)) + geom\_point()}
\CommentTok{\#timeseries1 }

\CommentTok{\# Adding lines and starting to clean up graph}
\NormalTok{timeseries2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mobility\_it\_nogeo, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date\_fix, }\AttributeTok{y=}\NormalTok{grocery\_pharmacy)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"\#061685"}\NormalTok{, }\AttributeTok{shape=}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color=}\StringTok{"\#2c7bb6"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_tufte}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Grocery and Pharmacy \% change"}\NormalTok{) }

\NormalTok{timeseries2}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-1_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\end{figure}

The \texttt{ggplot2} package recognizes the date format and
automatically uses a specific type of X axis. If the time variable isn't
at the date format, this won't work. Always check with
\texttt{str(data)} how variables are understood by R. The
\texttt{scale\_x\_data()} makes it a breeze to customize those labels.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Use the limit option of the scale\_x\_date() function to select a time frame in the data:}
\NormalTok{timeseries3 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mobility\_it\_nogeo, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date\_fix, }\AttributeTok{y=}\NormalTok{grocery\_pharmacy)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"\#061685"}\NormalTok{, }\AttributeTok{shape=}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{( }\AttributeTok{color=}\StringTok{"\#2c7bb6"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Grocery and Pharmacy \% change visits"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_tufte}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_date}\NormalTok{(}\AttributeTok{limit=}\FunctionTok{c}\NormalTok{(}\FunctionTok{as.Date}\NormalTok{(}\StringTok{"2022{-}01{-}01"}\NormalTok{),}\FunctionTok{as.Date}\NormalTok{(}\StringTok{"2022{-}04{-}01"}\NormalTok{))) }\CommentTok{\# Limiting between two dates }

\NormalTok{timeseries3}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-1_files/figure-pdf/unnamed-chunk-10-1.pdf}

}

\end{figure}

\texttt{plotly} is also great to turn the resulting chart interactive in
one more line of code. With the \texttt{ggplotly()} function you can
hover circles to get a tooltip, or select an area of interest for
zooming. You can zoom by selecting an area of interest. Hover over the
line to get exact time and value. Export to a widget with
\texttt{htmlwidgets}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Usual chart}
\NormalTok{timeseries3bis }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mobility\_it\_nogeo, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date\_fix, }\AttributeTok{y=}\NormalTok{grocery\_pharmacy)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"\#061685"}\NormalTok{, }\AttributeTok{shape=}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color=}\StringTok{"\#2c7bb6"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_tufte}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Grocery and Pharmacy \% change"}\NormalTok{) }

\CommentTok{\# Turn it interactive with ggplotly}
\NormalTok{timeseries\_interactive }\OtherTok{\textless{}{-}} \FunctionTok{ggplotly}\NormalTok{(timeseries3bis)}
\NormalTok{timeseries\_interactive}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Of course this needs more cleaning to be a final output}

\CommentTok{\# To save the widget use the library(htmlwidgets)}
\CommentTok{\# saveWidget(p, file=paste0( getwd(), "/HtmlWidget/ggplotlyAreachart.html"))}
\end{Highlighting}
\end{Shaded}

\textbf{Linear trends}

Now let's try to model trends in the data. Let's first consider linear
trends. Linear trends are the simplest way to model time as a
quantitative variable. We can represent time as a series of evenly
spaced points on a graph, and then fit a straight line to those points.
The slope of the line tells us the rate at which the variable is
changing over time. For example, if we are measuring population counts
over time, a positive slope would indicate that a population increase,
and a negative slope would indicate a population decrease.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Estimate linear regression model}
\NormalTok{linear\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(grocery\_pharmacy }\SpecialCharTok{\textasciitilde{}}\NormalTok{ date\_fix, mobility\_it\_nogeo)}

\CommentTok{\# A simple plot line}
\FunctionTok{plot}\NormalTok{(mobility\_it\_nogeo}\SpecialCharTok{$}\NormalTok{date\_fix,                       }
\NormalTok{     mobility\_it\_nogeo}\SpecialCharTok{$}\NormalTok{grocery\_pharmacy,}
     \AttributeTok{type =} \StringTok{"l"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(mobility\_it\_nogeo}\SpecialCharTok{$}\NormalTok{date\_fix,}
      \FunctionTok{predict}\NormalTok{(linear\_model),}
      \AttributeTok{col =} \DecValTok{2}\NormalTok{,}
      \AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-1_files/figure-pdf/unnamed-chunk-12-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract coefficients of model and print}
\NormalTok{my\_coef }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(linear\_model)            }
\NormalTok{my\_coef                            }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  (Intercept)      date_fix 
-694.27809786    0.03706687 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract equation of model and print}
\NormalTok{my\_equation }\OtherTok{\textless{}{-}} \FunctionTok{paste}\NormalTok{(}\StringTok{"y ="}\NormalTok{,        }
                     \FunctionTok{coef}\NormalTok{(linear\_model)[[}\DecValTok{1}\NormalTok{]],}
                     \StringTok{"+"}\NormalTok{,}
                     \FunctionTok{coef}\NormalTok{(linear\_model)[[}\DecValTok{2}\NormalTok{]],}
                     \StringTok{"* x"}\NormalTok{)}
\NormalTok{my\_equation                        }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "y = -694.278097860568 + 0.037066871224827 * x"
\end{verbatim}

Let's plot this with ggplot to fit our line through our points. We can
use the package \texttt{ggpmisc} to add the equation and R2 with
\texttt{stat\_poly\_line()} and \texttt{stat\_poly\_eq}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Using geom\_smooth() for the linear fit}
\NormalTok{timeseries4a }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mobility\_it\_nogeo, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date\_fix, }\AttributeTok{y=}\NormalTok{grocery\_pharmacy)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"\#061685"}\NormalTok{, }\AttributeTok{shape=}\DecValTok{1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method=}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# linear regression }
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Grocery \& Pharmacy \%"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_poly\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_poly\_eq}\NormalTok{(}\FunctionTok{use\_label}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"eq"}\NormalTok{, }\StringTok{"R2"}\NormalTok{))) }\SpecialCharTok{+}
 \FunctionTok{theme\_tufte}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(Delta, }\StringTok{"\% Grocery \& Pharmacy"}\NormalTok{)))}
  
\NormalTok{timeseries4b }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mobility\_it\_nogeo, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date\_fix, }\AttributeTok{y=}\NormalTok{transit)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"\#061685"}\NormalTok{, }\AttributeTok{shape=}\DecValTok{1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method=}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\#This is where your linear regression is}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Transit stations \%"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_poly\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_poly\_eq}\NormalTok{(}\FunctionTok{use\_label}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"eq"}\NormalTok{, }\StringTok{"R2"}\NormalTok{))) }\SpecialCharTok{+}
 \FunctionTok{theme\_tufte}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(Delta, }\StringTok{"\% Transit stations"}\NormalTok{)))}

\NormalTok{timeseries4c }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mobility\_it\_nogeo, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date\_fix, }\AttributeTok{y=}\NormalTok{parks\_percent\_change\_from\_baseline)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"\#061685"}\NormalTok{, }\AttributeTok{shape=}\DecValTok{1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method=}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\#This is where your linear regression is}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Parks \%"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_poly\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{stat\_poly\_eq}\NormalTok{(}\FunctionTok{use\_label}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"eq"}\NormalTok{, }\StringTok{"R2"}\NormalTok{))) }\SpecialCharTok{+}
 \FunctionTok{theme\_tufte}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(Delta, }\StringTok{"\% Visit Parks"}\NormalTok{)))}
  
  
\FunctionTok{grid.arrange}\NormalTok{(timeseries4a, timeseries4b, timeseries4c, }\AttributeTok{ncol=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-1_files/figure-pdf/unnamed-chunk-13-1.pdf}

}

\end{figure}

Alternatively you can use the \texttt{ols()} function aswell followed by
the \texttt{stat\_function()}. See
\href{https://edrub.in/ARE212/Spring2017/section06.html}{here} for more
details.

Linear trends have some limitations. In many cases, the relationship
between time and the variable we are measuring is not linear, and
fitting a straight line may not capture the true nature of the
relationship. In such cases, we may need to consider a quadratic trend.

\textbf{Quadratic trends}

Quadratic trends model time as a second-degree polynomial, which allows
for a curved relationship between time and the variable we are
measuring. Quadratic trends can capture more complex patterns in the
data than linear trends, such as a gradual increase followed by a
gradual decrease. For example, if we are measuring the number of
COVID-19 cases over time, a quadratic trend may capture the initial
exponential growth, followed by a flattening out of the curve.

However, quadratic trends also have some limitations. They assume that
the relationship between time and the variable we are measuring is
symmetrical, which may not always be the case. In addition, they can be
difficult to interpret, as the coefficient for the quadratic term does
not have a straightforward interpretation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{timeseries5 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mobility\_it\_nogeo, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date\_fix, }\AttributeTok{y=}\NormalTok{transit)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"\#061685"}\NormalTok{, }\AttributeTok{shape=}\DecValTok{1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(x,}\DecValTok{2}\NormalTok{), }\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{se =}\NormalTok{ T, }\AttributeTok{level =} \FloatTok{0.99}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# 2nd order polynomial \& adjusting level of the confidence interval}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Transit stations \% change"}\NormalTok{) }\SpecialCharTok{+}
 \FunctionTok{theme\_tufte}\NormalTok{() }
  
\NormalTok{timeseries5}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-1_files/figure-pdf/unnamed-chunk-14-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{timeseries6 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mobility\_it\_nogeo, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date\_fix, }\AttributeTok{y=}\NormalTok{transit)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"\#061685"}\NormalTok{, }\AttributeTok{shape=}\DecValTok{1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(x,}\DecValTok{3}\NormalTok{), }\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{se =}\NormalTok{ T, }\AttributeTok{level =} \FloatTok{0.99}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# 3rd order polynomial \& adjusting level of the confidence interval}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Transit stations \% change"}\NormalTok{) }\SpecialCharTok{+}
 \FunctionTok{theme\_tufte}\NormalTok{() }
  
\NormalTok{timeseries6}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-1_files/figure-pdf/unnamed-chunk-14-2.pdf}

}

\end{figure}

\textbf{Splines}

Finally, we have splines. Splines are a more flexible way to model time
as a quantitative variable. Splines allow us to fit a piecewise function
to the data, where the function is a series of connected polynomial
segments. Each segment captures a different part of the relationship
between time and the variable we are measuring. Splines can capture more
complex patterns in the data than linear or quadratic trends, and they
can be customized to fit the specific shape of the relationship we are
trying to model.

However, splines also have some limitations. They can be computationally
intensive, and the choice of the number and location of the knots (i.e.,
the points where the polynomial segments connect) can have a big impact
on the results. Splines are useful when the underlying function is
complex or unknown, and can be used for a variety of applications,
including curve fitting, data smoothing, and prediction.

In R, you can plot x vs y using the plot() function. To plot a spline,
you can use the spline() function to generate the points for the curve,
and then plot the curve using the lines() function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{timeseries7 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mobility\_it\_nogeo, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date\_fix, }\AttributeTok{y=}\NormalTok{grocery\_pharmacy)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"\#061685"}\NormalTok{, }\AttributeTok{shape=}\DecValTok{1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  ggformula}\SpecialCharTok{::}\FunctionTok{stat\_spline}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# spline}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Grocery \& Pharmacy \%"}\NormalTok{) }\SpecialCharTok{+}
 \FunctionTok{theme\_tufte}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(Delta, }\StringTok{"\% Visit Parks"}\NormalTok{)))}
  
\NormalTok{timeseries7}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-1_files/figure-pdf/unnamed-chunk-15-1.pdf}

}

\end{figure}

Linear trends, quadratic trends, and splines are all ways to model time
as a quantitative variable, each with their own strengths and
weaknesses. The choice of method will depend on the specific research
question and the shape of the relationship between time and the variable
we are measuring. You can find more on descriptive timeseries analysis
\href{http://r-statistics.co/Time-Series-Analysis-With-R.html?utm_content=cmp-true}{here}.

\hypertarget{modelling-time-and-space}{%
\section{Modelling Time and Space}\label{modelling-time-and-space}}

We can also examine the heterogeneity in the data by region. Some
regions in Italy were had many more COVID-19 cases than others.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Mobility data by region over time}
\NormalTok{timeseries\_all\_1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mobility\_it, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date\_fix, }\AttributeTok{y=}\NormalTok{transit, }\AttributeTok{color =}\NormalTok{ sub\_region\_1)) }\SpecialCharTok{+}
  \CommentTok{\# geom\_point(alpha = 0.8) + }
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{,}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(x,}\DecValTok{2}\NormalTok{), }\AttributeTok{se=}\NormalTok{F) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
  \AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{,}
  \AttributeTok{panel.background =} \FunctionTok{element\_rect}\NormalTok{(}\AttributeTok{fill =} \ConstantTok{NA}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Change in use of transit stations"}\NormalTok{) }

\CommentTok{\# Initial Graph}
\NormalTok{timeseries\_all\_1  }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-1_files/figure-pdf/unnamed-chunk-16-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mobility\_it\_filtered }\OtherTok{\textless{}{-}}\NormalTok{ mobility\_it }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(sub\_region\_1 }\SpecialCharTok{!=} \StringTok{"ALL"}\NormalTok{)}

\CommentTok{\# Mobility data by region over time with some edits}
\NormalTok{timeseries\_all\_2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mobility\_it\_filtered, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date\_fix, }\AttributeTok{y =}\NormalTok{ transit, }\AttributeTok{color =}\NormalTok{ sub\_region\_1)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(x, }\DecValTok{2}\NormalTok{), }\AttributeTok{se =}\NormalTok{ F, }\AttributeTok{size =} \FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_viridis}\NormalTok{(}\AttributeTok{discrete =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{option=}\StringTok{"mako"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{,}
    \AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{14}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{),}
    \AttributeTok{axis.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{axis.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{12}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{),}
    \AttributeTok{legend.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{legend.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{12}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{""}\NormalTok{,}
    \AttributeTok{y =} \StringTok{""}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"Change in use of transit stations"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"Region"}
\NormalTok{  )}

\NormalTok{timeseries\_all\_2}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-1_files/figure-pdf/unnamed-chunk-16-2.pdf}

}

\end{figure}

These graphs would need further work. We could for example divide the
data between North, Centre and Southern Italy. Let's load the geojson of
italian regions and plot the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add the polygons of Italy to the environment}
\NormalTok{italy\_iso3166 }\OtherTok{\textless{}{-}} \FunctionTok{st\_read}\NormalTok{(}\StringTok{"data/longitudinal{-}1/italy\_projected\_simplified.geojson"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Reading layer `italy_projected_simplified' from data source 
  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/longitudinal-1/italy_projected_simplified.geojson' 
  using driver `GeoJSON'
Simple feature collection with 124 features and 4 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 1313364 ymin: 3933695 xmax: 2312062 ymax: 5220353
Projected CRS: Monte Mario / Italy zone 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a simple plot}
\FunctionTok{plot}\NormalTok{(italy\_iso3166}\SpecialCharTok{$}\NormalTok{geometry)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-1_files/figure-pdf/unnamed-chunk-17-1.pdf}

}

\end{figure}

Now let's join the data by regional code.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Join by regional code}
\NormalTok{italy\_iso3166\_tidy }\OtherTok{\textless{}{-}}\NormalTok{ italy\_iso3166 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(mobility\_it, }\AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\StringTok{"ISO3166.2"}\OtherTok{=}\StringTok{"iso\_3166\_2\_code"}\NormalTok{))}

\CommentTok{\# check the first rows of the merged data table}
\CommentTok{\# head(italy\_iso3166\_tidy)}
\end{Highlighting}
\end{Shaded}

To start with, we can plot the data statically using ggplot.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# One point in time}
\NormalTok{italy\_sf\_subset\_1 }\OtherTok{\textless{}{-}}\NormalTok{ italy\_iso3166\_tidy }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(date\_fix }\SpecialCharTok{==} \StringTok{"2022{-}01{-}01"}\NormalTok{)}
\NormalTok{italy\_sf\_subset\_2 }\OtherTok{\textless{}{-}}\NormalTok{ italy\_iso3166\_tidy }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(date\_fix }\SpecialCharTok{==} \StringTok{"2022{-}02{-}01"}\NormalTok{)}
\NormalTok{italy\_sf\_subset\_3 }\OtherTok{\textless{}{-}}\NormalTok{ italy\_iso3166\_tidy }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(date\_fix }\SpecialCharTok{==} \StringTok{"2022{-}03{-}01"}\NormalTok{)}
\NormalTok{italy\_sf\_subset\_4 }\OtherTok{\textless{}{-}}\NormalTok{ italy\_iso3166\_tidy }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(date\_fix }\SpecialCharTok{==} \StringTok{"2022{-}04{-}01"}\NormalTok{)}
\NormalTok{italy\_sf\_subset\_5 }\OtherTok{\textless{}{-}}\NormalTok{ italy\_iso3166\_tidy }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(date\_fix }\SpecialCharTok{==} \StringTok{"2022{-}05{-}01"}\NormalTok{)}
\NormalTok{italy\_sf\_subset\_6 }\OtherTok{\textless{}{-}}\NormalTok{ italy\_iso3166\_tidy }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(date\_fix }\SpecialCharTok{==} \StringTok{"2022{-}06{-}01"}\NormalTok{)}

\CommentTok{\#This may take some time to load}
\NormalTok{g1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ italy\_sf\_subset\_1, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ transit, }\AttributeTok{geometry =}\NormalTok{ geometry)) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_gradient2}\NormalTok{(}\AttributeTok{low =} \StringTok{"\#d7191c"}\NormalTok{, }\AttributeTok{mid =} \StringTok{"white"}\NormalTok{, }\AttributeTok{high =} \StringTok{"darkblue"}\NormalTok{, }
                      \AttributeTok{midpoint =} \DecValTok{0}\NormalTok{, }\AttributeTok{guide =} \StringTok{"colourbar"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}

 
\NormalTok{g2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ italy\_sf\_subset\_2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ transit, }\AttributeTok{geometry =}\NormalTok{ geometry)) }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_gradient2}\NormalTok{(}\AttributeTok{low =} \StringTok{"\#d7191c"}\NormalTok{, }\AttributeTok{mid =} \StringTok{"white"}\NormalTok{, }\AttributeTok{high =} \StringTok{"darkblue"}\NormalTok{, }
                      \AttributeTok{midpoint =} \DecValTok{0}\NormalTok{, }\AttributeTok{guide =} \StringTok{"colourbar"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}

\NormalTok{g3 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ italy\_sf\_subset\_3, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ transit, }\AttributeTok{geometry =}\NormalTok{ geometry)) }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_gradient2}\NormalTok{(}\AttributeTok{low =} \StringTok{"\#d7191c"}\NormalTok{, }\AttributeTok{mid =} \StringTok{"white"}\NormalTok{, }\AttributeTok{high =} \StringTok{"darkblue"}\NormalTok{, }
                      \AttributeTok{midpoint =} \DecValTok{0}\NormalTok{, }\AttributeTok{guide =} \StringTok{"colourbar"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}

\NormalTok{g4 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ italy\_sf\_subset\_4, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ transit, }\AttributeTok{geometry =}\NormalTok{ geometry)) }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_gradient2}\NormalTok{(}\AttributeTok{low =} \StringTok{"\#d7191c"}\NormalTok{, }\AttributeTok{mid =} \StringTok{"white"}\NormalTok{, }\AttributeTok{high =} \StringTok{"darkblue"}\NormalTok{, }
                      \AttributeTok{midpoint =} \DecValTok{0}\NormalTok{, }\AttributeTok{guide =} \StringTok{"colourbar"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}

\NormalTok{g5 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ italy\_sf\_subset\_5, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ transit, }\AttributeTok{geometry =}\NormalTok{ geometry)) }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_gradient2}\NormalTok{(}\AttributeTok{low =} \StringTok{"\#d7191c"}\NormalTok{, }\AttributeTok{mid =} \StringTok{"white"}\NormalTok{, }\AttributeTok{high =} \StringTok{"darkblue"}\NormalTok{, }
                      \AttributeTok{midpoint =} \DecValTok{0}\NormalTok{, }\AttributeTok{guide =} \StringTok{"colourbar"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}

\NormalTok{g6 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ italy\_sf\_subset\_6, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ transit, }\AttributeTok{geometry =}\NormalTok{ geometry)) }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_gradient2}\NormalTok{(}\AttributeTok{low =} \StringTok{"\#d7191c"}\NormalTok{, }\AttributeTok{mid =} \StringTok{"white"}\NormalTok{, }\AttributeTok{high =} \StringTok{"darkblue"}\NormalTok{, }
                      \AttributeTok{midpoint =} \DecValTok{0}\NormalTok{, }\AttributeTok{guide =} \StringTok{"colourbar"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}

\FunctionTok{grid.arrange}\NormalTok{(g1, g2, g3, g4, g5, g6, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }
             \AttributeTok{top =} \FunctionTok{textGrob}\NormalTok{(}\StringTok{"Change in use of transit by Italian Region"}\NormalTok{, }\AttributeTok{gp=}\FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize=}\DecValTok{18}\NormalTok{,}\AttributeTok{font=}\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-1_files/figure-pdf/join plots-1.pdf}

}

\end{figure}

Plotting time and space interactively is more complex. There are quite a
few examples of how to visualise data over space and time
\href{https://rstudio-pubs-static.s3.amazonaws.com/595502_57d6130e00384d99a6b0cea0e5212d44.html}{here}.

\hypertarget{questions-5}{%
\section{Questions}\label{questions-5}}

For the assignment, we will continue to focus on the United Kingdom as
our geographical area of analysis. For this section, we will use Google
Mobility data for the UK for 2021. It has the same format as the Google
Mobility data for Italy used in this chapter. During this year the UK
underwent a third national lockdown, limitations of gatherings and more.
For details on the timeline you can have a look
\href{https://www.instituteforgovernment.org.uk/sites/default/files/2022-12/timeline-coronavirus-lockdown-december-2021.pdf}{here}.

Start by loading both the csv and geojsons.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mobility\_gb }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/longitudinal{-}1/2021\_GB\_Region\_Mobility\_Report.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{uk\_iso3166 }\OtherTok{\textless{}{-}} \FunctionTok{st\_read}\NormalTok{(}\StringTok{"data/longitudinal{-}1/uk\_projected\_simplified.geojson"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Reading layer `uk_projected_simplified' from data source 
  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/longitudinal-1/uk_projected_simplified.geojson' 
  using driver `GeoJSON'
Simple feature collection with 245 features and 4 fields (with 16 geometries empty)
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: -50462.93 ymin: 5270.466 xmax: 655975.3 ymax: 1219809
Projected CRS: OSGB36 / British National Grid
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Chose three variables to focus on and plot them together using
  \texttt{ggplot}. Discuss what format the data and if you can see any
  shocks over time by referring to the UK COVID-19 timeline.
\item
  Chose one variable and fit a linear, quadratic or spline model.
  Justify why you think this is the most appropriate and how you would
  interpret the resulting equation.
\item
  Model both time and space: use \texttt{ggplot} to create a graph of
  the data by region or other geographical area. You can chose what to
  focus on here and do not necessarily need to use all the geographical
  classifications available to you. Analyse the trends that appear in
  your plot.
\item
  Create a static or interactive map (bonus points) of your choice from
  the data.
\end{enumerate}

Analyse and discuss what insights you obtain into people's attitudes,
concerns, and behaviors during the pandemic, as well as their response
to interventions and policies.

\bookmarksetup{startatroot}

\hypertarget{sec-chp9}{%
\chapter{Assessing Interventions}\label{sec-chp9}}

Since its outbreak in late 2019, COVID-19 affected millions of people
worldwide. One of the ways to track the spread of the disease is through
the collection and analysis of data on daily cases, which has become a
crucial component of new digital footprint trends. Contact tracing apps
and tools were developed and implemented in many countries to help track
the spread of COVID-19.

Data on daily cases can help us evaluate the effectiveness of policies
implemented during the pandemic, such as lockdowns and other
restrictions. By analyzing these data, policymakers and public health
experts have gained insights into the impact of these policies on the
spread of the disease Brodeur et al. (2021) and (Zhou and Kan 2021). For
instance, if daily cases decrease significantly during a lockdown, it
could indicate that the policy was successful in controlling the spread
of the disease.

Data on daily cases can also help policymakers to make informed
decisions about when to implement or lift certain restrictions. For
example, if daily cases start to rise again after a lifting of
restrictions, it may indicate that it is too early to do so, and that
more caution is necessary. In summary, data on daily cases was an
essential tool for evaluating the effectiveness of policies implemented
during the COVID-19 pandemic and making informed decisions about future
policies. As we will see in this chapter, evaluating COVI9-19 policies
based onn cases is, unfortunately not so simple.

To assess government interventions during the pandemic, we will be using
\textbf{difference-in-differences} (diff-n-diff), a widely-adopted
quasi-experimental design, used to evaluate the effect of an
intervention or exposure on an outcome of interest. This strategy
involves comparing the change in the outcome before and after the
intervention or exposure in the treated group, relative to the change in
the outcome over the same time period in a control group. By doing so,
diff-n-diff can help isolate the effect of the intervention or exposure
from other factors that may be driving the outcome.

This chapter is based on :

\begin{itemize}
\item
  Goodman-Bacon, Andrew, and Jan Marcus.
  \href{https://d-nb.info/1212037294/34}{``Using
  difference-in-differences to identify causal effects of COVID-19
  policies.''} (2020)
\item
  Andrew Heiss' chapter on
  \href{https://evalf20.classes.andrewheiss.com/example/diff-in-diff/}{Difference-in-differences}
\item
  Brodeur, Abel, et
  al.~\href{https://www.sciencedirect.com/science/article/pii/S0047272720302103?casa_token=fuC4W9x8SJwAAAAA:QgZY6w7qPwTRAR0xdLlkSDKNpbivMdzwKIyIA3l3FfqOZRXwok2hqliaBwDOttESv-AtrUXjgrE}{``COVID-19,
  lockdowns and well-being: Evidence from Google Trends.''} Journal of
  public economics 193 (2021): 104346.
\end{itemize}

\hypertarget{dependencies-4}{%
\section{Dependencies}\label{dependencies-4}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Data}
\FunctionTok{library}\NormalTok{(sf)}
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Dates and Times}
\FunctionTok{library}\NormalTok{(lubridate)}

\CommentTok{\# graphs}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(ggthemes)}
\FunctionTok{library}\NormalTok{(gridExtra)}
\FunctionTok{library}\NormalTok{(viridis)}
\FunctionTok{library}\NormalTok{(ggimage)}
\FunctionTok{library}\NormalTok{(grid)}
\FunctionTok{library}\NormalTok{(plotly)}

\CommentTok{\# Models}
\FunctionTok{library}\NormalTok{(modelsummary)}
\FunctionTok{library}\NormalTok{(broom)}
\FunctionTok{library}\NormalTok{(gtools)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-4}{%
\section{Data}\label{data-4}}

First let's import the Greater London COVID-19 data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# import csv }
\NormalTok{covid\_cases\_london }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/longitudinal{-}2/covid\_cases\_london.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# check out the variables}
\FunctionTok{colnames}\NormalTok{(covid\_cases\_london)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "areaType"                       "areaName"                      
 [3] "areaCode"                       "date"                          
 [5] "newCasesBySpecimenDate"         "cumCasesBySpecimenDate"        
 [7] "newFirstEpisodesBySpecimenDate" "cumFirstEpisodesBySpecimenDate"
 [9] "newReinfectionsBySpecimenDate"  "cumReinfectionsBySpecimenDate" 
\end{verbatim}

Then we do the same for the Lazio area data, which is the Region of the
capital of Italy, Rome. We are choosing this region because it did not
see sharp peaks in COVID-19 cases during the winter of 2020/2021.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# import csv }
\NormalTok{covid\_cases\_lazio }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/longitudinal{-}2/covid\_cases\_lazio.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# check out the variables}
\FunctionTok{colnames}\NormalTok{(covid\_cases\_lazio)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "data"                    "stato"                  
 [3] "codice_regione"          "denominazione_regione"  
 [5] "codice_provincia"        "denominazione_provincia"
 [7] "sigla_provincia"         "lat"                    
 [9] "long"                    "totale_casi"            
\end{verbatim}

First we need to clean up the data somewhat and rename some variables in
both dataframes to have 4 variables:

\begin{itemize}
\item
  \texttt{date}: year-month-day
\item
  \texttt{geo}: geographical region
\item
  \texttt{cases}: number of COVID-19 cases that day
\item
  \texttt{area} : Lazio (Rome) or London
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Rename the variables in the Lombardia data frame}
\NormalTok{covid\_cases\_lazio\_ren }\OtherTok{\textless{}{-}}\NormalTok{ covid\_cases\_lazio }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{date =}\NormalTok{ data , }\AttributeTok{geo =}\NormalTok{ denominazione\_provincia, }\AttributeTok{totalcases =}\NormalTok{ totale\_casi) }

\CommentTok{\# Group the data by region and calculate total cases for each day (not cumulative cases)}
\NormalTok{covid\_cases\_lazio\_daily }\OtherTok{\textless{}{-}}\NormalTok{ covid\_cases\_lazio\_ren }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(geo) }\SpecialCharTok{\%\textgreater{}\%}
   \FunctionTok{mutate}\NormalTok{(}\AttributeTok{cases =}\NormalTok{ totalcases }\SpecialCharTok{{-}} \FunctionTok{lag}\NormalTok{(totalcases, }\AttributeTok{default =} \DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(date, geo, cases) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{area =} \StringTok{"Rome (Lazio)"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(cases }\SpecialCharTok{\textgreater{}=} \DecValTok{0}\NormalTok{)}

\CommentTok{\#df\_milan \textless{}{-} covid\_cases\_lombardia\_new \%\textgreater{}\%}
\CommentTok{\#  filter(geo == "Milano", new\_cases \textgreater{}= 0)}

\CommentTok{\# Rename the variables in the first data frame}
\NormalTok{covid\_cases\_london\_ren }\OtherTok{\textless{}{-}}\NormalTok{ covid\_cases\_london }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{date =}\NormalTok{ date , }\AttributeTok{geo =}\NormalTok{ areaName, }\AttributeTok{cases =}\NormalTok{ newCasesBySpecimenDate) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(date, geo, cases) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{area =} \StringTok{"London"}\NormalTok{)}

\CommentTok{\# Correct date format}
\NormalTok{covid\_cases\_london\_ren}\SpecialCharTok{$}\NormalTok{date }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(covid\_cases\_london\_ren}\SpecialCharTok{$}\NormalTok{date)}
\NormalTok{covid\_cases\_lazio\_daily}\SpecialCharTok{$}\NormalTok{date }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(covid\_cases\_lazio\_daily}\SpecialCharTok{$}\NormalTok{date)}

\CommentTok{\# Append the renamed data frame to the second data frame}
\NormalTok{covid\_combined }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(covid\_cases\_london\_ren, covid\_cases\_lazio\_daily)}

\CommentTok{\# Add a variable of log of cases}
\NormalTok{covid\_combined }\OtherTok{\textless{}{-}}\NormalTok{ covid\_combined }\SpecialCharTok{\%\textgreater{}\%} 
          \FunctionTok{mutate}\NormalTok{(}\AttributeTok{log\_cases =} \FunctionTok{log}\NormalTok{(cases))}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-exploration}{%
\section{Data Exploration}\label{data-exploration}}

Similarly to the previous chapter. Let's start by eyeballing the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualizing Cases in London}
\NormalTok{covid\_cases\_1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ covid\_cases\_london\_ren, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date, }\AttributeTok{y =}\NormalTok{ cases, }\AttributeTok{color=}\NormalTok{geo)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_viridis}\NormalTok{(}\AttributeTok{discrete =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{option=}\StringTok{"magma"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{""}\NormalTok{,}
    \AttributeTok{y =} \StringTok{""}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"Evolution in Covid{-}19 cases"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"Region"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_date}\NormalTok{(}\AttributeTok{date\_breaks =} \StringTok{"6 months"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
NULL
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  covid\_cases\_1}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-2_files/figure-pdf/unnamed-chunk-7-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualizing Cases in Lazio (Rome)}
\NormalTok{covid\_cases\_2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ covid\_cases\_lazio\_daily, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date, }\AttributeTok{y =}\NormalTok{ cases, }\AttributeTok{color=}\NormalTok{geo)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_viridis}\NormalTok{(}\AttributeTok{discrete =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{option=}\StringTok{"magma"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{""}\NormalTok{,}
    \AttributeTok{y =} \StringTok{""}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"Evolution in Covid{-}19 cases"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"Region"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_date}\NormalTok{(}\AttributeTok{date\_breaks =} \StringTok{"6 months"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
NULL
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{covid\_cases\_2}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-2_files/figure-pdf/unnamed-chunk-7-2.pdf}

}

\end{figure}

To identify whether there is a timeperiod where a lockdown was
implemented in one location but not the other, and how cases evolved, we
can plot aggregates of both locations in one plot.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Aggregate the data by region for each day}
\NormalTok{covid\_combined\_agg }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{(cases }\SpecialCharTok{\textasciitilde{}}\NormalTok{ area }\SpecialCharTok{+}\NormalTok{ date, }\AttributeTok{data =}\NormalTok{ covid\_combined, }\AttributeTok{FUN =}\NormalTok{ sum)}

\CommentTok{\# Visualizing aggregated}
\NormalTok{covid\_cases\_3 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ covid\_combined\_agg, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date, }\AttributeTok{y =}\NormalTok{ cases, }\AttributeTok{color=}\NormalTok{area)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\StringTok{"darkblue"}\NormalTok{, }\StringTok{"darkred"}\NormalTok{)) }\SpecialCharTok{+} \CommentTok{\# set individual colors for the areas}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{legend.position =} \StringTok{"bottom"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{""}\NormalTok{,}
    \AttributeTok{y =} \StringTok{""}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"Evolution in Covid{-}19 cases"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"Region"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_date}\NormalTok{(}\AttributeTok{date\_breaks =} \StringTok{"6 months"}\NormalTok{)}

\NormalTok{covid\_cases\_3}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-2_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\end{figure}

From an initial look at the data, the 2020/2021 winter period seems
interesting as there is a high increase in London cases but not as much
as a peak in Lazio cases. In fact, after a quick review of COVID-19
lockdowns, we found that:

\begin{itemize}
\tightlist
\item
  On the 5th of November 2020, the UK Prime Minister announced a second
  national lockdown, coming into force in England
\item
  On 4 November 2020, Italian Prime Minister Conte announced a new
  lockdown as well, however this lockdown divided the country into three
  zones depending on the severity of the pandemic, corresponding to red,
  orange and yellow zones. The Lazio region, was a yellow zone for the
  duration of this second lockdown. In yellow zones, the only
  restrictions included compulsory closing for restaurant and bar
  activities at 6 PM, and online education for high schools only.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Usual chart}
\NormalTok{covid\_cases\_4 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ covid\_combined\_agg, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date, }\AttributeTok{y =}\NormalTok{ cases, }\AttributeTok{color=}\NormalTok{area)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\StringTok{"darkblue"}\NormalTok{, }\StringTok{"darkred"}\NormalTok{)) }\SpecialCharTok{+} \CommentTok{\# set individual colors for the areas}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{legend.position =} \StringTok{"bottom"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{""}\NormalTok{,}
    \AttributeTok{y =} \StringTok{""}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"Evolution in Covid{-}19 cases"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"Region"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_date}\NormalTok{(}\AttributeTok{limit=}\FunctionTok{c}\NormalTok{(}\FunctionTok{as.Date}\NormalTok{(}\StringTok{"2020{-}08{-}01"}\NormalTok{), }\FunctionTok{as.Date}\NormalTok{(}\StringTok{"2021{-}01{-}15"}\NormalTok{))) }\SpecialCharTok{+}
 \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept=}\FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.Date}\NormalTok{(}\StringTok{"2020{-}11{-}05"}\NormalTok{)), }\AttributeTok{linetype=}\StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\AttributeTok{x=}\FunctionTok{as.Date}\NormalTok{(}\StringTok{"2020{-}11{-}06"}\NormalTok{), }\AttributeTok{y=}\DecValTok{25000}\NormalTok{, }\AttributeTok{label=}\StringTok{"Lockdown"}\NormalTok{, }
           \AttributeTok{color=}\StringTok{"black"}\NormalTok{, }\AttributeTok{fontface=}\StringTok{"bold"}\NormalTok{, }\AttributeTok{angle=}\DecValTok{0}\NormalTok{, }\AttributeTok{hjust=}\DecValTok{0}\NormalTok{, }\AttributeTok{vjust=}\DecValTok{0}\NormalTok{)}

\NormalTok{covid\_cases\_4}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-2_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\end{figure}

We could make some assumptions and set this up as a quasi experiment. In
social science, researchers are often using natural or quasi
experimental setting as randomized experiments can rarely be conducted.
This involves splitting the population at hand into a treatment and
control group.

\hypertarget{difference-in-difference}{%
\section{Difference in Difference}\label{difference-in-difference}}

\textbf{Plotting Means}

For a \texttt{diff-in-diff} analysis using COVID data, possible shocks
that would make this type of quasi-experiment possible could be the
following:

\begin{itemize}
\item
  \textbf{National lockdown}: The first national lockdown in the UK was
  announced on March 23, 2020. This sudden shock to the economy and
  society could be used as a treatment group for the diff-in-diff
  analysis, with the pre-lockdown period as the control group.
\item
  \textbf{Regional lockdowns}: The UK also implemented regional
  lockdowns throughout the pandemic, with different regions experiencing
  restrictions at different times. These regional lockdowns could be
  used as treatment groups, with regions that did not experience
  lockdowns as the control group.
\item
  \textbf{School closures}: In response to the pandemic, schools in the
  UK were closed from March 20, 2020, until June 1, 2020, and then again
  from January 5, 2021, until March 8, 2021. The impact of school
  closures on education outcomes could be studied using a diff-in-diff
  approach, with the period before school closures as the control group.
\item
  \textbf{Travel restrictions}: The UK implemented various travel
  restrictions throughout the pandemic, including quarantine
  requirements for travelers from certain countries. The impact of these
  travel restrictions on the tourism industry or the spread of the virus
  could be studied using a diff-in-diff approach.
\item
  \textbf{Vaccine rollout}: The UK began its COVID-19 vaccination
  program in December 2020. The impact of the vaccine rollout on various
  health and economic outcomes could be studied using a diff-in-diff
  approach, with the period before the rollout as the control group.
\end{itemize}

These are just a few examples of shocks that could be used for a
diff-in-diff analysis using COVID data. The choice of shock will depend
on the research question and the data available.

The DiD approach includes a before-after comparison for a treatment and
control group. In our example:

\begin{itemize}
\item
  A \texttt{cross-sectional\ comparison} (= compare a sample that was
  treated (London) to an non-treated control group (Rome))
\item
  A \texttt{before-after\ comparison} (= compare treatment group with
  itself, before and after the treatment (5th of November))
\end{itemize}

The \textbf{main assumption} is that without the change in the natural
environment the outcome variable would have remained constant!

First, we create a dummy variable to indicate the time when the
treatment started. In our case this will be the 5th of November 2020. We
will also limit the time-span of our data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# keep data from 2020{-}09{-}01 to 2021{-}01{-}01}
\NormalTok{covid\_combined\_filtered }\OtherTok{\textless{}{-}}\NormalTok{ covid\_combined }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(date }\SpecialCharTok{\textgreater{}=} \StringTok{"2020{-}09{-}01"} \SpecialCharTok{\&}\NormalTok{ date }\SpecialCharTok{\textless{}=} \StringTok{"2021{-}01{-}01"}\NormalTok{)}

\CommentTok{\# create a dummy variable to indicate the time when the treatment started (5 Nov 2020)}
\NormalTok{covid\_combined\_filtered }\OtherTok{\textless{}{-}}\NormalTok{ covid\_combined\_filtered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{after\_5nov =} \FunctionTok{ifelse}\NormalTok{(date }\SpecialCharTok{\textgreater{}=} \StringTok{"2020{-}11{-}05"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\CommentTok{\#changed to 05 Nov}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a frequency table of area and treatment}
\NormalTok{freq\_table }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(covid\_combined\_filtered}\SpecialCharTok{$}\NormalTok{area, covid\_combined\_filtered}\SpecialCharTok{$}\NormalTok{after\_5nov)}

\CommentTok{\# Print the frequency table}
\FunctionTok{print}\NormalTok{(freq\_table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              
                  0    1
  London       3150  540
  Rome (Lazio) 1436  240
\end{verbatim}

We then want to plot averages to see differences between
treatment/control groups and before/after. But we can also calculate the
mean and 95\% confidence interval. We can also use group\_by() and
summarize() to figure out group means before sending the data to ggplot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_data }\OtherTok{\textless{}{-}}\NormalTok{ covid\_combined\_filtered }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Make these categories instead of 0/1 numbers so they look nicer in the plot}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{after\_5nov =} \FunctionTok{factor}\NormalTok{(after\_5nov, }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Before 5 November 2020"}\NormalTok{, }\StringTok{"After 5 November 2020"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(area, after\_5nov) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean\_cases =} \FunctionTok{mean}\NormalTok{(cases),}
            \AttributeTok{se\_cases =} \FunctionTok{sd}\NormalTok{(cases) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{n}\NormalTok{()),}
            \AttributeTok{upper =}\NormalTok{ mean\_cases }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_cases),}
            \AttributeTok{lower =}\NormalTok{ mean\_cases }\SpecialCharTok{+}\NormalTok{ (}\SpecialCharTok{{-}}\FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_cases)) }

\FunctionTok{ggplot}\NormalTok{(plot\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ area, }\AttributeTok{y =}\NormalTok{ mean\_cases)) }\SpecialCharTok{+}
  \FunctionTok{geom\_pointrange}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ lower, }\AttributeTok{ymax =}\NormalTok{ upper), }
                  \AttributeTok{color =} \StringTok{"darkred"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(after\_5nov))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-2_files/figure-pdf/unnamed-chunk-13-1.pdf}

}

\end{figure}

Here, we can start to see a diff-in-diff plot, where there is little to
no difference in means with our control city (Rome-Lazio) and a
substancial jump in means in our treatment city (London). It looks there
were many more cases of COVID-19 after the 5th of march in London,
indicating the lockdown did not have an effect, at least in this
time-frame. Why could that be?

We can also plot a more standard diff-in-diff format:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(plot\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ after\_5nov, }\AttributeTok{y =}\NormalTok{ mean\_cases, }\AttributeTok{color =}\NormalTok{ area)) }\SpecialCharTok{+}
  \FunctionTok{geom\_pointrange}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ lower, }\AttributeTok{ymax =}\NormalTok{ upper), }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{group =}\NormalTok{ area)) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"darkblue"}\NormalTok{, }\StringTok{"darkred"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-2_files/figure-pdf/unnamed-chunk-14-1.pdf}

}

\end{figure}

This second plot shows us it is probable that our diff-n-diff set up
will not work. A clean classic diff-n-diff would look more like the
following. Please note the following plot is theoretical.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# import csv }
\NormalTok{covid\_perfect\_example }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/longitudinal{-}2/example\_covid.csv"}\NormalTok{)}

\CommentTok{\# label pre/post labels}
\NormalTok{covid\_perfect\_example }\OtherTok{\textless{}{-}}\NormalTok{ covid\_perfect\_example }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{after\_5nov =} \FunctionTok{factor}\NormalTok{(after\_5nov, }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Before 5 November 2020"}\NormalTok{, }\StringTok{"After 5 November 2020"}\NormalTok{)))}

\CommentTok{\# plot    }
\FunctionTok{ggplot}\NormalTok{(covid\_perfect\_example, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ after\_5nov, }\AttributeTok{y =}\NormalTok{ mean\_cases, }\AttributeTok{color =}\NormalTok{ area)) }\SpecialCharTok{+}
  \FunctionTok{geom\_pointrange}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ lower, }\AttributeTok{ymax =}\NormalTok{ upper), }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{group =}\NormalTok{ area)) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"darkblue"}\NormalTok{, }\StringTok{"darkred"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-2_files/figure-pdf/unnamed-chunk-15-1.pdf}

}

\end{figure}

\textbf{Difference in Difference by hand}

We can find the exact difference by filling out the 2x2 before/after
treatment/control table:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
& Before & After & Difference \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Treatment & A & B & B - A \\
Control & C & D & D - C \\
Difference & C - A & D - B & (D − C) − (B − A) \\
\end{longtable}

A combination of group\_by() and summarize() makes this really easy. We
can pull each of these numbers out of the table with some filter()s and
pull():

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{before\_treatment }\OtherTok{\textless{}{-}}\NormalTok{ covid\_perfect\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(after\_5nov }\SpecialCharTok{==} \StringTok{"Before 5 November 2020"}\NormalTok{, area }\SpecialCharTok{==} \StringTok{"London"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{(mean\_cases)}

\NormalTok{before\_control }\OtherTok{\textless{}{-}}\NormalTok{ covid\_perfect\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(after\_5nov }\SpecialCharTok{==} \StringTok{"Before 5 November 2020"}\NormalTok{, area }\SpecialCharTok{==} \StringTok{"Lazio"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{(mean\_cases)}

\NormalTok{after\_treatment }\OtherTok{\textless{}{-}}\NormalTok{ covid\_perfect\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(after\_5nov }\SpecialCharTok{==} \StringTok{"After 5 November 2020"}\NormalTok{, area }\SpecialCharTok{==} \StringTok{"London"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{(mean\_cases)}

\NormalTok{after\_control }\OtherTok{\textless{}{-}}\NormalTok{ covid\_perfect\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(after\_5nov }\SpecialCharTok{==} \StringTok{"After 5 November 2020"}\NormalTok{, area }\SpecialCharTok{==} \StringTok{"Lazio"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{(mean\_cases)}

\NormalTok{diff\_treatment\_before\_after }\OtherTok{\textless{}{-}}\NormalTok{ after\_treatment }\SpecialCharTok{{-}}\NormalTok{ before\_treatment}
\NormalTok{diff\_treatment\_before\_after}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 156.35
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diff\_control\_before\_after }\OtherTok{\textless{}{-}}\NormalTok{ after\_control }\SpecialCharTok{{-}}\NormalTok{ before\_control}
\NormalTok{diff\_control\_before\_after}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 24.35387
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diff\_diff }\OtherTok{\textless{}{-}}\NormalTok{ diff\_treatment\_before\_after }\SpecialCharTok{{-}}\NormalTok{ diff\_control\_before\_after}
\NormalTok{diff\_diff}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 131.9961
\end{verbatim}

The diff-in-diff estimate is 131.99, which means that the lockdown here
caused an increase in cases in the time-window we are analysing. Not
it's intended effect!

We can visualise this really well with a bit of extra code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(covid\_perfect\_example, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ after\_5nov, }\AttributeTok{y =}\NormalTok{ mean\_cases, }\AttributeTok{color =}\NormalTok{ area)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \CommentTok{\#geom\_pointrange(aes(ymin = lower, ymax = upper), size = 1) + }
  \CommentTok{\#geom\_line(aes(group = area)) +}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{group =} \FunctionTok{as.factor}\NormalTok{(area))) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"darkblue"}\NormalTok{, }\StringTok{"darkred"}\NormalTok{)) }\SpecialCharTok{+}
  \CommentTok{\# If you use these lines you\textquotesingle{}ll get some extra annotation lines and}
  \CommentTok{\# labels. The annotate() function lets you put stuff on a ggplot that\textquotesingle{}s not}
  \CommentTok{\# part of a dataset. Normally with geom\_line, geom\_point, etc., you have to}
  \CommentTok{\# plot data that is in columns. With annotate() you can specify your own x and}
  \CommentTok{\# y values.}
  \FunctionTok{annotate}\NormalTok{(}\AttributeTok{geom =} \StringTok{"segment"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Before 5 November 2020"}\NormalTok{, }\AttributeTok{xend =} \StringTok{"After 5 November 2020"}\NormalTok{,}
           \AttributeTok{y =}\NormalTok{ before\_treatment, }\AttributeTok{yend =}\NormalTok{ after\_treatment }\SpecialCharTok{{-}}\NormalTok{ diff\_diff,}
           \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"grey50"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{annotate}\NormalTok{(}\AttributeTok{geom =} \StringTok{"segment"}\NormalTok{, }\AttributeTok{x =} \StringTok{"After 5 November 2020"}\NormalTok{, }\AttributeTok{xend =} \StringTok{"After 5 November 2020"}\NormalTok{,}
           \AttributeTok{y =}\NormalTok{ after\_treatment, }\AttributeTok{yend =}\NormalTok{ after\_treatment }\SpecialCharTok{{-}}\NormalTok{ diff\_diff,}
           \AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{, }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{annotate}\NormalTok{(}\AttributeTok{geom =} \StringTok{"label"}\NormalTok{, }\AttributeTok{x =} \StringTok{"After 5 November 2020"}\NormalTok{, }\AttributeTok{y =}\NormalTok{ after\_treatment }\SpecialCharTok{{-}}\NormalTok{ (diff\_diff }\SpecialCharTok{/} \DecValTok{2}\NormalTok{), }
           \AttributeTok{label =} \StringTok{"Program effect"}\NormalTok{, }\AttributeTok{size =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{longitudinal-2_files/figure-pdf/unnamed-chunk-17-1.pdf}

}

\end{figure}

It is important for all diff-in-diff analyses to give careful attention
to possible violations of the common trends assumption, especially
considering the COVID-19 situation where many of these violations are
likely to occur. Furthermore, due to the unique dynamics of COVID-19
such as lags between exposure and recorded infections, nonlinearities
from person-to-person transmission, and the possibility of policies
having differential effects over time, it further complicates the
potential risks to the diff-in-diff research design.

(Goodman-Bacon and Marcus 2020) comment on the following problems which
can be consulted in their paper:

\begin{itemize}
\item
  Packaged Policies
\item
  Reverse Causality
\item
  Voluntary Precautions
\item
  Difference Data collection
\item
  Anticipation Spillovers
\item
  Variation in Policy Timing
\end{itemize}

(Goodman-Bacon and Marcus 2020) also give great recommendations on how
to address these problems, but this is far beyond the objective of this
chapter.

\textbf{Difference-in-Difference with regression}

Calculating all the pieces by hand like that is tedious, so we can use
regression to do it instead! Remember that we need to include indicator
variables for treatment/control and for before/after, as well as the
interaction of the two.

This is the equation:

\(\Delta Y_{gt} = \beta_0 + \beta_1 London_{g} + \beta_2 Post5Nov_{t} + \beta_3 London_{g} \times Post5Nov_{t} + \beta_4 Rome_{g} + \epsilon_{gt}\)

The output will show the diff-in-diff coefficient estimate, standard
error, t-value, and p-value, which can be used to determine whether
there was a significant effect of the second lockdown 4 on Covid cases
in November 2020.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_small }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(cases }\SpecialCharTok{\textasciitilde{}}\NormalTok{ area }\SpecialCharTok{+}\NormalTok{ after\_5nov }\SpecialCharTok{+}\NormalTok{ area }\SpecialCharTok{*}\NormalTok{ after\_5nov,}
                  \AttributeTok{data =}\NormalTok{ covid\_combined\_filtered)}

\CommentTok{\# Tidy the model output}
\NormalTok{diffndiff1 }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{(model\_small) }

\CommentTok{\# Add significance stars using stars.pval from gtools}
\NormalTok{diffndiff1}\SpecialCharTok{$}\NormalTok{stars }\OtherTok{\textless{}{-}} \FunctionTok{stars.pval}\NormalTok{(diffndiff1}\SpecialCharTok{$}\NormalTok{p.value)}

\CommentTok{\# View the results}
\NormalTok{diffndiff1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 6
  term                        estimate std.error statistic   p.value stars
  <chr>                          <dbl>     <dbl>     <dbl>     <dbl> <chr>
1 (Intercept)                     55.7      4.47      12.5 3.62e- 35 ***  
2 areaRome (Lazio)               126.       7.99      15.8 8.81e- 55 ***  
3 after_5nov                     301.      11.7       25.7 7.63e-138 ***  
4 areaRome (Lazio):after_5nov   -269.      21.0      -12.8 5.37e- 37 ***  
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a model summary table for the model}
\NormalTok{summary\_table }\OtherTok{\textless{}{-}} \FunctionTok{modelsummary}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\StringTok{"Simple"} \OtherTok{=}\NormalTok{ model\_small), }\AttributeTok{estimate =}\FunctionTok{c}\NormalTok{(}\StringTok{"\{estimate\}\{stars\}"}\NormalTok{))}

\CommentTok{\# View the results}
\NormalTok{summary\_table}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lc}
\toprule
  & Simple\\
\midrule
(Intercept) & \num{55.694}***\\
 & (\num{4.469})\\
areaRome (Lazio) & \num{125.910}***\\
 & (\num{7.986})\\
after\_5nov & \num{300.656}***\\
 & (\num{11.681})\\
areaRome (Lazio) × after\_5nov & \num{-269.302}***\\
 & (\num{21.032})\\
\midrule
Num.Obs. & \num{5366}\\
R2 & \num{0.130}\\
R2 Adj. & \num{0.130}\\
AIC & \num{74524.8}\\
BIC & \num{74557.7}\\
Log.Lik. & \num{-37257.375}\\
F & \num{267.482}\\
RMSE & \num{250.71}\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{questions-6}{%
\section{Questions}\label{questions-6}}

For the assignment, you will continue to use Google Mobility data for
the UK for 2021. For details on the timeline you can have a look
\href{https://www.instituteforgovernment.org.uk/sites/default/files/2022-12/timeline-coronavirus-lockdown-december-2021.pdf}{here}.
You will need to do a bit of digging on when lockdowns or other COVID-19
related shock happened in 2021 to set up a diff-in-diff strategy. Have a
look at Brodeur et al. (2021) to get some inspiration. They used Google
Trends data to test whether COVID-19 and the associated lockdowns
implemented in Europe and America led to changes in well-being.

Start by loading both the csv

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mobility\_gb }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/longitudinal{-}1/2021\_GB\_Region\_Mobility\_Report.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Visualize the data with \texttt{ggplot} and identify what section of
  the data could be used to evaluate a COVID-19 intervention. Examples
  of these interventions could be a regional lockdown, school closures,
  travel restrictions or vaccine rollouts. Generate a clean
  \texttt{ggplot} which indicates which intervention you are going to
  examine.
\item
  Explore differences in means through a \texttt{frequency\ table} and a
  graph of these averages. Chose whichever suits your purposes best.
\item
  Define and estimage a diff-in-diff regression. What do the results
  suggest? Was the intervention you chose effective? Discuss the reasons
  why it was or was not.
\item
  Discuss how the unique dynamics of COVID-19 and the possibility of
  policies having differential effects over time complicate the
  interpretation of your results.
\end{enumerate}

Analyse and discuss what insights you obtain into people's changes in
behaviors during the pandemic in responde to an intervention.

\bookmarksetup{startatroot}

\hypertarget{sec-chp10}{%
\chapter{Machine Learning}\label{sec-chp10}}

Machine learning (ML) is one of the most wanted skills in today's job
market. It has very wide applications in many areas, from developing
software for self-driving cars to early detection of deadly diseases
such as cancer, but also in the studies of populations and human
geography. Although the term \textbf{machine learning} has become a lot
more popular in the last few years thanks to the increased capabilities
to analyse larger amounts of data, many ML techniques have existed for
decades in other fields such as statistics and computer science.

Just like in regression problems, ML algorithms build models based on
sample data in order to make predictions or decisions about other unseen
data. In fact, linear regression can be regarded as a very simple case
of a neural network. There are different approaches to ML according to
the type of feedback available to the ML algorithms:

\begin{itemize}
\item
  Supervised learning: we give the algorithm data where the inputs and
  outputs are already mapped, then the algorithm learns patterns of this
  mapping to be applied on new unseen data sets. Linear regression is an
  example of supervised learning.
\item
  Unsupervised learning: we give the algorithm data where the inputs
  have no matched outputs. The task of the algorithm is to learn
  patterns in the data or processes to achieve a certain goal. An
  example of this is the k-means clustering algorithm that you explored
  in previous chapters.
\item
  Reinforcement learning: the algorithm must achieve a goal while
  interacting with a dynamic environment. While navigating the
  environment, the algorithm is provided with feedback in the form of
  rewards which it tries to maximise (Bishop 2006). An example of this
  would be some of the software involved in self-driving cars.
\end{itemize}

In this chapter we will focus only on two ML techniques which are,
however, very versatile since they can be used for classification,
regression and other tasks. The first and most basic of the techniques
that we will explore is \textbf{decision trees}. Building on these, we
will then explore \textbf{random forests}. Both methods belong to the
supervised learning approach. To illustrate the usage of these
techniques within the context of population science, we will look at
predicting the annual household income across the UK geography based on
demographic data obtained from the census.

This chapter is based, among others, on the following references:

\begin{itemize}
\item
  Hands-on Machine Learning with R. Chapter 9: Decision trees (Boehmke
  2019).
\item
  UC Business Analytics R Programming Guide, developed by Brad Boehmke.
  Specifically, the chapters on Regression Trees \& Bagging and Random
  Forests, which can be found
  \href{http://uc-r.github.io/predictive}{here}.
\end{itemize}

\hypertarget{dependencies-5}{%
\section{Dependencies}\label{dependencies-5}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Import the dplyr package for data manipulation}
\FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(dplyr))}
\CommentTok{\# Import the rpart package for decision tree modeling}
\FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(rpart))}
\CommentTok{\# Import the rpart.plot package for visualization of decision trees}
\FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(rpart.plot))}
\CommentTok{\# Import ggplot 2 to make plots}
\FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(ggplot2))}
\CommentTok{\# Import Metrics for performance metrics calculation}
\FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(Metrics))}
\CommentTok{\# Import caret for machine learning modeling and evaluation}
\FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(caret))}
\CommentTok{\# Import randomForest for the random forest algorithm}
\FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(randomForest))}
\CommentTok{\# Import ranger for the ranger implementation of random forest, which is optimised for performance}
\FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(ranger))}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-5}{%
\section{Data}\label{data-5}}

As mentioned above, we will learn about decision trees and random
forests through a practical example where the goal is to predict the net
annual household income across different regions of the UK. Given that
UK censuses take place separately in Northern Ireland, Scotland and
England \& Wales, we will focus only on England \& Wales for simplicity.

In the code chunk below, we load a data set that has already been
prepared for this notebook. It includes a variety of variables related
to demographic characteristics of the population, aggregated at the
Middle-Layer Super Output Area (MSOA) level. All the variables, except
for the average annual household income, are derived from the 2021
census and the raw data can be downloaded from
\href{https://www.nomisweb.co.uk/census/2021/bulk}{here}. The annual
household income data is from the Annual Survey of Hours and Earnings
and can be downloaded from this
\href{https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/earningsandworkinghours/datasets/smallareaincomeestimatesformiddlelayersuperoutputareasenglandandwales}{link}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the data}
\NormalTok{df\_MSOA }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"./data/machine{-}learning/census2021{-}msoa{-}income.csv"}\NormalTok{)}
\CommentTok{\# Data cleaning, remove the X field}
\NormalTok{df\_MSOA}\SpecialCharTok{$}\NormalTok{X }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\CommentTok{\# Data cleaning, the fields "date", "geography" and "geography.code" are not needed}
\NormalTok{df\_MSOA }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(df\_MSOA, }\AttributeTok{select =} \SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(date, geography, geography.code))}
\CommentTok{\# More data cleaning, remove comma from income and turn it into a numeric value}
\NormalTok{df\_MSOA}\SpecialCharTok{$}\NormalTok{INCOME }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{gsub}\NormalTok{(}\StringTok{","}\NormalTok{, }\StringTok{""}\NormalTok{, df\_MSOA}\SpecialCharTok{$}\NormalTok{INCOME))}
\end{Highlighting}
\end{Shaded}

For a description of the variables in the columns of \texttt{df\_MSOA},
we can load a dictionary for these variables:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load variable dictionary}
\NormalTok{df\_dictionary }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"./data/machine{-}learning/Dictionary.csv"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(df\_dictionary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                                      Dictionary       X
1                                                       
2                                           Name     Key
3                 Lives in household (% persons)    inHH
4    Lives in communal establishment (% persons)    inCE
5 Never married or civil partnership (% persons)    SING
6    Married or in civil partnership (% persons) MARRIED
\end{verbatim}

\hypertarget{splitting-the-data}{%
\section{Splitting the data}\label{splitting-the-data}}

For most supervised learning problems, the goal is to find an algorithm
or model that not only fits well known data, but also accurately
predicts unknown values of the average annual household income based on
a set of inputs. In other words, we want the algorithm to be
generalisable. In order to measure the generalisability of the optimal
algorithm, we can split the data into a training set containing input
and output data and a test set. The training set is used to teach the
algorithm how to map inputs to outputs, and the test set is used to
estimate the prediction error of the final algorithm, which quantifies
the generalisability of the model. The test set should never be used
during the training stage.

As a rule of thumb, the data should be split so that 70\% of the samples
are in the training set and 30\% in the test set, although this
percentages might vary slightly according to the size of the original
data set. Furthermore, to ensure generalisability, the data split should
be done so that the distribution of outputs in both the training and
test set is approximately the same.

The function \texttt{create\_train\_test} below (borrowed from
\href{https://www.guru99.com/r-decision-trees.html}{here}) allows us to
select samples from the data to create the training set (when the train
parameter is set to \texttt{TRUE}) and the test set (when the train
parameter is set to \texttt{FALSE}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{create\_train\_test }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, }\AttributeTok{size =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{train =} \ConstantTok{TRUE}\NormalTok{) \{}
\NormalTok{ n\_row }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(data)}
\NormalTok{ total\_row }\OtherTok{=}\NormalTok{ size }\SpecialCharTok{*}\NormalTok{ n\_row}
\NormalTok{ train\_sample }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\NormalTok{ total\_row}
 \ControlFlowTok{if}\NormalTok{ (train }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{) \{}
 \FunctionTok{return}\NormalTok{ (data[train\_sample, ])}
\NormalTok{ \} }\ControlFlowTok{else}\NormalTok{ \{}
 \FunctionTok{return}\NormalTok{ (data[}\SpecialCharTok{{-}}\NormalTok{train\_sample, ])}
\NormalTok{ \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_train }\OtherTok{\textless{}{-}} \FunctionTok{create\_train\_test}\NormalTok{(df\_MSOA, }\FloatTok{0.7}\NormalTok{, }\AttributeTok{train =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{df\_test }\OtherTok{\textless{}{-}} \FunctionTok{create\_train\_test}\NormalTok{(df\_MSOA, }\FloatTok{0.7}\NormalTok{, }\AttributeTok{train =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If the data is naively split into training and test sets as we did
above, the distribution of outputs in the training and test set will not
be the same, as the following plot shows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d1 }\OtherTok{\textless{}{-}} \FunctionTok{density}\NormalTok{(df\_test}\SpecialCharTok{$}\NormalTok{INCOME)}
\FunctionTok{plot}\NormalTok{(d1, }\AttributeTok{col=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Income"}\NormalTok{, }\AttributeTok{main=}\StringTok{"Distribution of income"}\NormalTok{)}

\NormalTok{d2 }\OtherTok{\textless{}{-}} \FunctionTok{density}\NormalTok{(df\_train}\SpecialCharTok{$}\NormalTok{INCOME)}
\FunctionTok{lines}\NormalTok{(d2, }\AttributeTok{col=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}

\FunctionTok{legend}\NormalTok{(}\DecValTok{52000}\NormalTok{, }\FloatTok{0.00006}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"test set"}\NormalTok{,}\StringTok{"training set"}\NormalTok{), }\AttributeTok{lwd=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{machine-learning_files/figure-pdf/unnamed-chunk-6-1.pdf}

}

\end{figure}

This is due to the fact that the dataset that we loaded at the beginning
of this workbook is sorted so that some entries corresponding to MSOAs
that are geographically close to each other are in consecutive rows.
Therefore, to ensure that the distribution of outputs in training and
test sets is the same, the data needs to be randomly shuffled. The code
below achieves this goal, as it can be seen in the kernel density plot
with the new data split.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Shuffle the entries of the original dataset}
\NormalTok{shuffle\_index }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(df\_MSOA))}
\NormalTok{df\_MSOA }\OtherTok{\textless{}{-}}\NormalTok{ df\_MSOA[shuffle\_index, ]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Perform the data split with the new shuffled dataset}
\NormalTok{df\_train }\OtherTok{\textless{}{-}} \FunctionTok{create\_train\_test}\NormalTok{(df\_MSOA, }\FloatTok{0.7}\NormalTok{, }\AttributeTok{train =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{df\_test }\OtherTok{\textless{}{-}} \FunctionTok{create\_train\_test}\NormalTok{(df\_MSOA, }\FloatTok{0.7}\NormalTok{, }\AttributeTok{train =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Plot the kernel density for both training and test data sets}
\NormalTok{d1 }\OtherTok{\textless{}{-}} \FunctionTok{density}\NormalTok{(df\_test}\SpecialCharTok{$}\NormalTok{INCOME)}
\FunctionTok{plot}\NormalTok{(d1, }\AttributeTok{col=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Income"}\NormalTok{, }\AttributeTok{main=}\StringTok{"Distribution of income"}\NormalTok{)}

\NormalTok{d2 }\OtherTok{\textless{}{-}} \FunctionTok{density}\NormalTok{(df\_train}\SpecialCharTok{$}\NormalTok{INCOME)}
\FunctionTok{lines}\NormalTok{(d2, }\AttributeTok{col=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}

\FunctionTok{legend}\NormalTok{(}\DecValTok{52000}\NormalTok{, }\FloatTok{0.00006}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"test set"}\NormalTok{,}\StringTok{"training set"}\NormalTok{), }\AttributeTok{lwd=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{machine-learning_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\end{figure}

Before proceeding any further we should note here that one of the
advantages of decision trees and random forests is that they are not
sensitive to the magnitude of the input variables, so standardisation is
not needed before fitting these models. However, there are other ML
algorithms such as k-means, where standardisation is a crucial step to
ensure the success of learning process and it should always take place
before training the algorithm. Similarly, random forests are not
sensitive to correlations between independent variables, so there is no
need to check for correlations before training the models.

\hypertarget{decision-trees}{%
\section{Decision trees}\label{decision-trees}}

\hypertarget{fitting-the-training-data}{%
\subsection{Fitting the training data}\label{fitting-the-training-data}}

Decision trees are an ML algorithm capable of performing both
classification and regression tasks, although in this workbook we will
focus only on regression. One of their most notable advantages is that
they are very interpretable, although their predictions are not always
accurate. However, by aggregating decision trees through a method called
bagging, the algorithm can become much more powerful.

In essence, a decision tree is like a flowchart that helps us make a
decision based on a number of questions or conditions, which are
represented by internal nodes. It starts with a root node and branches
out into different paths, with each branch representing a decision. The
final nodes represent the outcome and are known as leaf nodes. For
example, imagine you are trying to decide what to wear to an event. You
could create a decision tree with the root node being ``Formal event?'',
like in the diagram below. If the answer is ``yes'' you would proceed to
the left and if the answer if ``no'', you would proceed to the right. On
the right, you would have another node for ``Black tie?'', and again two
``yes'' and ``no'' branches emerging from it. On the left, you would
have a node for ``Everyday wear?'' with its two branches. Each branch
would eventually lead to a decision or action represented by the
so-called leaf nodes, such as ``wear suit''.

\begin{figure}

{\centering \includegraphics[width=4.89583in,height=\textheight]{figs/chp10/Decision tree.jpg}

}

\end{figure}

The decision tree to predict the annual household income of an MSOA
based on its demographic features will be a lot more complex than the
one depicted above. The branches will lead to leaf nodes representing
the predicted values of annual household income. The internal nodes will
represent conditions for the demographic variables (e.g.~is the
percentage of people aged 65 or over in this MSOA greater than \emph{X}
\%?). Not all the demographic variables will be equally relevant to
predict the annual household income at the MSOA level. To optimise the
prediction process, conditions on the most relevant variables should be
near the root of the tree so the most determining questions can be asked
at the beginning of the decision-making process. The internal nodes that
are further from the tree will help fine-tune the final predictions.
But, how can we choose which are the most relevant variables? And, what
are the right questions to ask in each internal node (e.g.~if we are
asking `is the percentage of people aged 65 or over in this MSOA greater
than \emph{X} \%?', what should be the value of \emph{X} ?).

Luckily, nowadays there are many off-the-shelf software packages
available that can help us build decision trees with just a line of
code. Here, we use the R package \texttt{rpart}, which is based on the
Classification And Regression Tree (CART) algorithm proposed by Breiman
(Breiman 1984). In particular, we can use the function \texttt{rpart()}
to find the decision tree that best fits the training set. This function
requires to use the formula method for expressing the model. In this
case, the formula is \texttt{INCOME\ \textasciitilde{}.}, which means
that we regard the variable \texttt{INCOME} as a function of all the
other variables in the training data set. For more information on the
formula method, you can check the
\href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/formula}{R
documentation}. The function \texttt{rpart()} also requires to specify
the method for fitting. Since we are performing a regression task (as
opposed to classification), we need to set method to
\texttt{\textquotesingle{}anova\textquotesingle{}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(INCOME }\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ df\_train, }\AttributeTok{method =} \StringTok{\textquotesingle{}anova\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can visualise the fitted decision tree with the \texttt{rpart.plot()}
function from the library with the same name. Interestingly, the
condition for the root node is associated with the variable representing
the percentage of people born in Antarctica, Oceania and Other, so if an
MSOA has less than 0.25\% people born in those territories, the model
predicts it to have lower annual household income. Can you think of why
this might be the case?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rpart.plot}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{machine-learning_files/figure-pdf/unnamed-chunk-11-1.pdf}

}

\end{figure}

As we can see, \texttt{rpart()} produces a decision tree with 11 leaf
nodes and the conditions to reach each leaf are associated with only 8
demographic variables. However, in the original training set, there were
many more demographic variables that have not been included in the
model. The reason for this is that, behind the scenes, \texttt{rpart()}
is trying to find a balance between the complexity of the tree (i.e.~its
depth) and the generalisability of the tree to predict new unseen data.
If it is too deep, the tree runs the risk of overfitting the training
data and failing to predict the annual household income for other MSOAs
that are not included in the training set.

To illustrate the point of selecting a tree with 11 leaves, we can
manually control the level of complexity allowed when fitting of a
decision tree model. The lower we set the value of the parameter
\texttt{cp}, the more complex the resulting tree will be, so \texttt{cp}
can be regarded as penalty for the level of complexity or a cost
complexity parameter. Below, we fit a decision tree with no penalty for
generating a complex tree model, i.e.~\texttt{cp=0}, and then we use the
function \texttt{plotcp()} to plot the prediction error
(\href{https://en.wikipedia.org/wiki/PRESS_statistic}{PRESS statistic})
that would be achieved with decision trees of different levels of
complexity.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{\# train model}
\NormalTok{fit2 }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(INCOME }\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ df\_train, }\AttributeTok{method =} \StringTok{\textquotesingle{}anova\textquotesingle{}}\NormalTok{, }\AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{cp =} \DecValTok{0}\NormalTok{))}
\CommentTok{\# plot error vs tree complexity }
\FunctionTok{plotcp}\NormalTok{(fit2)}
\CommentTok{\# draw vertical line at 11 trees}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\DecValTok{11}\NormalTok{, }\AttributeTok{lty=}\StringTok{\textquotesingle{}dashed\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{machine-learning_files/figure-pdf/unnamed-chunk-12-1.pdf}

}

\end{figure}

As we can see from the plot above, with more than 11 leaves, little
reduction in the prediction error is achieved as the model becomes more
and more complex. In other words, we start seeing diminishing returns in
error reduction as the tree grows deeper. Hence \texttt{rpart()} is
doing some behind-the-scenes tuning by pruning the tree so it only has
11 leaf nodes.

There are other model parameters that can be tuned in order to improve
the model performance via the control argument of the \texttt{rpart()}
function, just like we did above for \texttt{cp}. While we do not
experiment with these additional parameters in the workbook, we provide
brief descriptions below so you can explore them on your own time:

\begin{itemize}
\item
  \texttt{minsplit} controls the minimum number of data points required
  in each leaf node. The default is 20. Setting this lower will result
  in more leaves with very few data points belonging to the
  corresponding branch.
\item
  \texttt{maxdepth} controls the maximum number of internal nodes
  between the root node and the terminal nodes. By default, it is set to
  30. Setting it higher allows to create deeper trees.
\end{itemize}

\hypertarget{measuring-the-performance-of-regression-models}{%
\subsection{Measuring the performance of regression
models}\label{measuring-the-performance-of-regression-models}}

To measure the performance of the regression tree that we fitted above,
we can use the test set. We firstly use the \texttt{predict()} function
from the \texttt{rpart} library to compute some predictions on the MSOA
annual household income for the test set data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predict\_unseen }\OtherTok{\textless{}{-}}\FunctionTok{predict}\NormalTok{(fit, df\_test)}
\end{Highlighting}
\end{Shaded}

Then, we compare the predictions with the actual values and measure the
discrepancy with a regression error metric.

Note that, \textbf{to measure the performance of classification trees,
the procedure would be slightly different} and it would involve the
computation of a confusion matrix and the accuracy metric.

Different error metrics exist to measure the performance of regression
models such as the Mean Squared Error (MSE), the Mean Absolute Error
(MAE) or the Root Mean Squared Error (RMSE). The MSE is more sensitive
to outliers than the MAE, however, the units of MSE are squared units.
The RMSE solves the problem of the squared units associated with MSE by
taking its squared root. The library \texttt{Metrics} provides the
in-built function \texttt{rmse()} which makes the computation of RMSE
straightforward:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rmse}\NormalTok{(predict\_unseen, df\_test}\SpecialCharTok{$}\NormalTok{INCOME)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4680.266
\end{verbatim}

This value does not have much meaning in isolation. A good or bad RMSE
is always relative to the specific data set. For this reason, we need to
establish a baseline RMSE that we can compare it with. Here we establish
this baseline as the RMSE that would be obtained from a naive tree that
merely predicts the mean annual household income value across all the
data entries in the training set. If the fitted model achieves an RSME
lower than the naive model, we say that the fitted model ``has skill''.
The following line of code confirms that our fitted model is better than
the naive model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rmse}\NormalTok{(predict\_unseen, }\FunctionTok{mean}\NormalTok{(df\_train}\SpecialCharTok{$}\NormalTok{INCOME))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5424.923
\end{verbatim}

\hypertarget{bagging}{%
\subsection{Bagging}\label{bagging}}

Even though single decision trees have many advantages such as being
very simple and interpretable, their predictions are not always accurate
due to their high variance. This results in unstable predictions that
may be dependent on the chosen training data.

A method called bagging can help solve this issue by combining and
averaging the predictions of multiple decision tree models. The method
can actually be applied to any regression or classification model,
however, it is most effective when applied to models that have high
variance. Bagging works by following three steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create \emph{m} bootstrap samples from the training data
  (i.e.~\emph{m} random samples with replacement).
\item
  For each bootstrap sample, train an unpruned single tree (i.e.~with
  \texttt{cp=0}).
\item
  To create a prediction for a new data point, input the data in the
  single trees fitted with each bootstrap sample. The prediction will be
  the average of all the individual predictions output by each tree.
\end{enumerate}

Each bootstrap sample typically contains about two-thirds of the
training data, leaving one-third out. This left-out third is known as
the out-of-bag (OOB) sample and it provides a natural opportunity to
cross-validate the predictive performance of the model. By
cross-validating the model, we can estimate how well our model will
perform on new data without necessarily having to use the test set to
test it. In other words, we can use the whole of the original data set
for training and still quantify the performance of the model. However,
for simplicity, we will train the model on the training set only here.

Bagging can be easily done with a library called \texttt{caret}. Here we
fit a 10-fold cross-validated model, meaning that the bagging is applied
so that there are 10 different OOB samples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# specify 10{-}fold cross validation}
\NormalTok{ctrl }\OtherTok{\textless{}{-}} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{,  }\AttributeTok{number =} \DecValTok{10}\NormalTok{) }
\CommentTok{\# set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{\# train the cross{-}validated bagged model}
\NormalTok{bagged }\OtherTok{\textless{}{-}}\NormalTok{ caret}\SpecialCharTok{::}\FunctionTok{train}\NormalTok{(INCOME }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ df\_train, }\AttributeTok{method =} \StringTok{"treebag"}\NormalTok{, }\AttributeTok{trControl =}\NormalTok{ ctrl, }\AttributeTok{importance =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{print}\NormalTok{(bagged)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Bagged CART 

4956 samples
  48 predictor

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 4460, 4460, 4461, 4461, 4461, 4461, ... 
Resampling results:

  RMSE  Rsquared   MAE     
  4130  0.6603757  3099.709
\end{verbatim}

We see that the cross-validated value of RMSE with bagging is lower than
that associated with the single decision tree that we trained with
\texttt{rcart}. This indicates that the predictive performance is
estimated to be better. We can compare the cross-validated value of the
RMSE with the RMSE from the test set. These two quantities should be
close:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predict\_bagged\_unseen }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(bagged, df\_test)}
\FunctionTok{rmse}\NormalTok{(predict\_bagged\_unseen, df\_test}\SpecialCharTok{$}\NormalTok{INCOME)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4251.69
\end{verbatim}

The library \texttt{caret} has an additional function \texttt{varImp()}
that helps us understand the variable importance across the bagged
trees, i.e.~the variables that are most relevant to determine the
predictions of MSOA net annual household income. You are welcome to
check the \texttt{caret} documentation to learn more about how the
variable importance is determined. We can plot a rank of variable
importance by running the code below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{varImp}\NormalTok{(bagged), }\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{machine-learning_files/figure-pdf/unnamed-chunk-18-1.pdf}

}

\end{figure}

As noted before, given the variables included in our original data set,
the percentage of people born in Antarctica, Oceania and Other is,
almost invariably, the best predictor of annual household income,
although due to the randomness introduced in bagging, this could
sometimes change.

\hypertarget{random-forests}{%
\section{Random forests}\label{random-forests}}

While bagging considerably improves the performance of decision trees,
the resulting models are still subject to some issues. Mainly, the
multiple trees that are fitted through the bagging process are not
completely independent of each other since all the original variables
are considered at every split in every tree. As a consequence, trees in
different bootstrap samples have similar structure (with almost always
the same variables near the root) and the variance in the predictions
cannot be reduced optimally. This issue is known as tree correlation.

Random forests optimally reduce the variance of the predicted values by
minimising the tree correlation. This is achieved in two steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Like in bagging, different trees are fitted from bootstrap samples.
\item
  However, when an internal node is to be created in a given tree, the
  search for the optimal variable in that node is limited to only a
  random subset of the explanatory variables. By default, the number of
  variables in these subsets is one-third of the total number of
  variables, although this proportion is considered a tuning parameter
  for the model.
\end{enumerate}

\hypertarget{basic-implementation}{%
\subsection{Basic implementation}\label{basic-implementation}}

Several R implementations for random forest fitting exist, however, the
most well known is provided by the \texttt{randomForest} library. By
default it performs 500 trees (i.e.~500 bootstrap samples) and randomly
selects one-third of the explanatory variables for each split, although
these parameters can be manually tuned. The random forest model can be
trained by executing just a line of code:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{\# train model}
\NormalTok{fit\_rf }\OtherTok{\textless{}{-}} \FunctionTok{randomForest}\NormalTok{(}\AttributeTok{formula=}\NormalTok{ INCOME }\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ df\_train)}
\CommentTok{\# print summary of fit}
\NormalTok{fit\_rf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
 randomForest(formula = INCOME ~ ., data = df_train) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 16

          Mean of squared residuals: 9853155
                    % Var explained: 79.83
\end{verbatim}

As we can see from above, the mean of squared residuals, which is the
same as the MSE, is 9903736 for 500 trees and therefore, RMSE = 3147.
This metric is computed by averaging residuals from the OOB samples. To
illustrate how the MSE varies as more bootstrap samples are added to the
model, we can plot the fitted model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(fit\_rf, }\AttributeTok{main =} \StringTok{"Errors vs no. of trees"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{machine-learning_files/figure-pdf/unnamed-chunk-20-1.pdf}

}

\end{figure}

We see that the MSE becomes stable with approximately 100 trees, but it
continues to decrease slowly. To find the number of trees that lead to
the minimum error, we can run the following line:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{which.min}\NormalTok{(fit\_rf}\SpecialCharTok{$}\NormalTok{mse)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 499
\end{verbatim}

By computing the RMSE, we can compare the performance of this model with
performance of the models in the previous sections:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(fit\_rf}\SpecialCharTok{$}\NormalTok{mse[}\FunctionTok{which.min}\NormalTok{(fit\_rf}\SpecialCharTok{$}\NormalTok{mse)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3138.608
\end{verbatim}

This is a much lower value than what we obtained with just a single tree
and even after applying bagging! Remember, this RMSE is based on the OOB
samples, but we could also obtain it from the test set.
\texttt{randomForest()} allows us to easily compare the RMSE obtained
from OOB data and from the test set.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# format test set for comparison of errors with randomForest}
\NormalTok{x\_test }\OtherTok{\textless{}{-}}\NormalTok{ df\_test[}\FunctionTok{setdiff}\NormalTok{(}\FunctionTok{names}\NormalTok{(df\_test), }\StringTok{"INCOME"}\NormalTok{)]}
\NormalTok{y\_test }\OtherTok{\textless{}{-}}\NormalTok{ df\_test}\SpecialCharTok{$}\NormalTok{INCOME}

\CommentTok{\# set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{\# include test data in training}
\NormalTok{rf\_oob\_compare }\OtherTok{\textless{}{-}} \FunctionTok{randomForest}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ INCOME }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data  =}\NormalTok{ df\_train, }\AttributeTok{xtest =}\NormalTok{ x\_test, }\AttributeTok{ytest =}\NormalTok{ y\_test)}

\CommentTok{\# extract OOB \& test errors}
\NormalTok{oob\_rmse }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(rf\_oob\_compare}\SpecialCharTok{$}\NormalTok{mse)}
\NormalTok{test\_rmse }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(rf\_oob\_compare}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{mse)}

\CommentTok{\# plot error vs no. of trees}
\NormalTok{x}\OtherTok{=}\DecValTok{1}\SpecialCharTok{:}\NormalTok{rf\_oob\_compare}\SpecialCharTok{$}\NormalTok{ntree}
\FunctionTok{plot}\NormalTok{(x, oob\_rmse, }\AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{col=}\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"No. of trees"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"RMSE"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Comparison of RMSE"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(x, test\_rmse, }\AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{col=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\DecValTok{350}\NormalTok{, }\DecValTok{4500}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"OOB"}\NormalTok{,}\StringTok{"test"}\NormalTok{), }\AttributeTok{lwd=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{), }\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"red"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{machine-learning_files/figure-pdf/unnamed-chunk-23-1.pdf}

}

\end{figure}

\hypertarget{tuning}{%
\subsection{Tuning}\label{tuning}}

You may have noticed that the number of trees is not the only parameter
we can tune in a random forest model. Below we list the model
parameters, a.k.a. hyperparameters that can be tuned to improve the
performance of the random tree models:

\begin{itemize}
\item
  \texttt{num.trees} is the number of trees. It should be large enough
  to make sure the MSE (or the RMSE) stabilises, but not too large that
  it creates unnecessary work.
\item
  \texttt{mtry} is the number of variables that are randomly sampled at
  each split. The default is one-third of the number of variables in the
  original data set. If \texttt{mtry} was equal to the total number of
  variables, the random forest model would be equivalent to bagging.
  Similarly, if \texttt{mtry} was equal to 1, it would mean that only
  one variable is chosen, but then the results can become too biased. To
  find the optimal value of \texttt{mtry}, it is common to attempt 5
  values evenly spread between 2 and the total number of variables in
  the original data set.
\item
  \texttt{sample.fraction} controls the number of data points in each
  bootstrap sample, i.e.~the number of samples chosen to create each
  tree. By default, it is 63.25\% (about two-thirds) of the training set
  since on average, this guarantees unique data points in a sample. If
  the sample size is smaller, it could reduce the training time but it
  could also introduce some bias in the model. If the sample size is
  larger, it could lead to overfitting. When tuning the model, this
  parameter is frequently kept between 60 and 80\% of the total size of
  the training set.
\item
  \texttt{min.node.size} is the minimal node size to split at. Default
  is 5 for regression.
\item
  \texttt{max.depth} is the maximum depth of the trees.
\end{itemize}

In order to find the combination of hyperparameters that leads to the
best performing model, we need to try them all and select the one with
the lowest MSE or RMSE. This is usually a computationally heavy task, so
as the models and the training data become larger, the process of tuning
can become very slow. The library \texttt{ranger} provides a C++
implementation of the random forest algorithm and allows to perform
hyperparameter search faster than \texttt{randomForest}.

As mentioned, to find the best performing model, we need to find the
right combination of hyperparameters. So the first step in the tuning
process is to generate a ``grid'' of possible combinations of
hyperparameters. If we only wanted to tune \texttt{ntree} (as we did in
the previous subsection when we found that the number of trees leading
to the lowest value of MSE is 495), the grid would be simply a list of
possible \texttt{ntree} values. To illustrate more complex tuning, here
we generate a grid that considers \texttt{mtry},
\texttt{sample.fraction} and \texttt{min.node.size}. The grid is created
as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Considering that there are 48 explanatory variables in the original dataset, we will try values of mtry between 10 and 30. The sample size will go from 60\% and 80\% of the total size of the training set. We will try minimal node size splits between 5 and 20.}
\NormalTok{hyper\_grid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{mtry =} \FunctionTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\AttributeTok{by=}\DecValTok{2}\NormalTok{), }
                          \AttributeTok{sample.fraction =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.60}\NormalTok{, }\FloatTok{0.65}\NormalTok{, }\FloatTok{0.70}\NormalTok{, }\FloatTok{0.75}\NormalTok{, }\FloatTok{0.80}\NormalTok{),}
                          \AttributeTok{min.node.size =} \FunctionTok{seq}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{9}\NormalTok{, }\AttributeTok{by=}\DecValTok{2}\NormalTok{))}

\CommentTok{\# total number of hyperparameter combinations}
\FunctionTok{nrow}\NormalTok{(hyper\_grid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 120
\end{verbatim}

Next, we can loop through the grid and generate, for each hyperparameter
combination, random forest models based on 500 trees. For each random
forest model, we will add the OOB RMSE error to the grid so we can find
what hyperparameter combination minimises this error. Note that we set
the value of seed for code reproducibility purposes.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(hyper\_grid)) \{}
  
  \CommentTok{\# train model}
\NormalTok{  fit\_rf\_tuning }\OtherTok{\textless{}{-}} \FunctionTok{ranger}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ INCOME }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
    \AttributeTok{data =}\NormalTok{ df\_train, }
    \AttributeTok{num.trees =} \DecValTok{500}\NormalTok{,}
    \AttributeTok{mtry =}\NormalTok{ hyper\_grid}\SpecialCharTok{$}\NormalTok{mtry[i],}
    \AttributeTok{sample.fraction =}\NormalTok{ hyper\_grid}\SpecialCharTok{$}\NormalTok{sample.fraction[i],}
    \AttributeTok{min.node.size =}\NormalTok{ hyper\_grid}\SpecialCharTok{$}\NormalTok{min.node.size[i],}
    \AttributeTok{seed =} \DecValTok{123}\NormalTok{)}
  
  \CommentTok{\# add OOB error to grid}
\NormalTok{  hyper\_grid}\SpecialCharTok{$}\NormalTok{OOB\_RMSE[i] }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(fit\_rf\_tuning}\SpecialCharTok{$}\NormalTok{prediction.error)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

From the fitted models, the one that produces the minimum OOB RMSE and
hence, the best-performing one, is given by the combination of
parameters printed below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hyper\_grid[}\FunctionTok{which.min}\NormalTok{(hyper\_grid}\SpecialCharTok{$}\NormalTok{OOB\_RMSE),]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   mtry sample.fraction min.node.size OOB_RMSE
58   16             0.8             5 3142.358
\end{verbatim}

\texttt{mtry}= 14, \texttt{sample.fraction} =0.8 and
\texttt{min.node.size}=3. The OOB RMSE is 3141.089, slightly lower than
the error we obtained with the default model for random forest with no
tuning, yay!

\hypertarget{questions-7}{%
\section{Questions}\label{questions-7}}

For this set of questions, you will use a data set very similar to the
one used in the examples above. However, instead of focusing on
predicting the net annual household income, you will focus in the median
house price paid in each MSOA in 2021. The raw data for median house
price can be downloaded
\href{https://www.ons.gov.uk/peoplepopulationandcommunity/housing/datasets/hpssadataset2medianhousepricebymsoaquarterlyrollingyear}{here},
but we have created a clean dataset for you. You can load the relevant
data set by running the code below:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the data}
\NormalTok{df\_housing }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"./data/machine{-}learning/census2021{-}msoa{-}houseprice.csv"}\NormalTok{)}
\CommentTok{\# Data cleaning, remove the X field}
\NormalTok{df\_housing}\SpecialCharTok{$}\NormalTok{X }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\CommentTok{\# Data cleaning, the fields "date", "geography" and "geography.code" are not needed}
\NormalTok{df\_housing }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(df\_housing, }\AttributeTok{select =} \SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(date, geography, geography.code))}
\CommentTok{\# More data cleaning, remove comma from income and turn it into a numeric value}
\NormalTok{df\_housing}\SpecialCharTok{$}\NormalTok{HOUSEPRICE }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{gsub}\NormalTok{(}\StringTok{","}\NormalTok{, }\StringTok{""}\NormalTok{, df\_housing}\SpecialCharTok{$}\NormalTok{HOUSEPRICE))}
\end{Highlighting}
\end{Shaded}

For the following questions, you will use the whole dataset for training
and evaluate the performance of the ML models via the OOB error.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train a model of decision trees with bagging using \texttt{caret}. Use
  10-fold cross-validation and the default number of trees in the
  training process. Set the seed to 123 so that your results are
  reproducible. Report the OOB RMSE and the first three most important
  variables for the decision process in the fitted model using the
  function \texttt{varImp()}. Comment on why you think these three
  demographic variables are relatively important to determine the median
  house price for the MSOAs in England \& Wales. What is special about
  these demographic characteristics? Do not include any plots.
\item
  Train a random forest model to predict mean house price at the MSOA
  level using \texttt{randomForest} with the default settings. Without
  including any plots, report the number of trees that produces the
  minimum OOB MSE. What would be the associated minimum RMSE? Like
  before, set the seed to 123 to ensure your results are reproducible.
  Do you observe any improvements in model performance with respect to
  the model you fitted in question 1?
\item
  Use \texttt{ranger} to tune the random forest model. Perform the
  hyperparameter search through a grid that considers only the number of
  trees and the number of variables to be sampled at each split. For the
  number of trees, try values from 490 to 500, separated by 1 unit. For
  the number of variables to be sampled at each split, try values from
  10 to 20, also separated by 1 unit. Set the seed to 123. Without
  including any plots, report the combination of parameter values that
  leads to the model with the lowest OOB RMSE. Report the value of the
  OOB RMSE. Do you observe any further improvements? In the context of
  predicting mean house price, do you think this RMSE is acceptable?
  Justify your answer.
\item
  Using your own words, explain what are the advantages of using random
  forests to predict median house prices instead of multilinear
  regression. You should include references to the ML literature to
  support your arguments.
\end{enumerate}

\bookmarksetup{startatroot}

\hypertarget{sec-chp11}{%
\chapter{Data sets}\label{sec-chp11}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sf)}
\FunctionTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\hypertarget{greater-machester-land-use-data}{%
\section{Greater Machester land use
data}\label{greater-machester-land-use-data}}

\hypertarget{availability}{%
\subsection*{Availability}\label{availability}}
\addcontentsline{toc}{subsection}{Availability}

The dataset is stored on a gpkg file that can be found, within the
structure of this project, under:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{st\_LSOA }\OtherTok{\textless{}{-}} \FunctionTok{st\_read}\NormalTok{(}\StringTok{"./data/geodemographics/manchester\_land\_cover\_2011.gpkg"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Reading layer `manchester_land_cover_2011' from data source 
  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/geodemographics/manchester_land_cover_2011.gpkg' 
  using driver `GPKG'
Simple feature collection with 1673 features and 44 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 351662.3 ymin: 381166 xmax: 406087.2 ymax: 421037.7
Projected CRS: OSGB36 / British National Grid
\end{verbatim}

\hypertarget{variables}{%
\subsection*{Variables}\label{variables}}
\addcontentsline{toc}{subsection}{Variables}

The variables included in this dataset follow the land use
classification of the CORINE Land Cover dataset.

\hypertarget{source-pre-processing}{%
\subsection*{Source \& Pre-processing}\label{source-pre-processing}}
\addcontentsline{toc}{subsection}{Source \& Pre-processing}

The data was sourced from
\href{https://github.com/GDSL-UL/APPG-LBA/blob/main/README.md}{What do
`left behind' areas look like over time?} and cleaned on Python.

\hypertarget{british-administrative-boundaries-lsoas-and-las}{%
\section{British administrative boundaries (LSOAs and
LAs)}\label{british-administrative-boundaries-lsoas-and-las}}

\hypertarget{availability-1}{%
\subsection*{Availability}\label{availability-1}}
\addcontentsline{toc}{subsection}{Availability}

The dataset for the boundaries of the lower-layer super-output areas
(LSOAs) within London is stored as a shapefile that can be found under:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{st\_LSOA }\OtherTok{\textless{}{-}} \FunctionTok{st\_read}\NormalTok{(}\StringTok{"data/geodemographics{-}old/LSOA\_2011\_London\_gen\_MHW/LSOA\_2011\_London\_gen\_MHW.shp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Reading layer `LSOA_2011_London_gen_MHW' from data source 
  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/geodemographics-old/LSOA_2011_London_gen_MHW/LSOA_2011_London_gen_MHW.shp' 
  using driver `ESRI Shapefile'
Simple feature collection with 4835 features and 14 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6
Projected CRS: OSGB36 / British National Grid
\end{verbatim}

The dataset for the boundaries of the local authority distrits (LADs)
for the UK is stored as a shapefile that can be found under:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LA\_UK }\OtherTok{\textless{}{-}} \FunctionTok{st\_read}\NormalTok{(}\StringTok{"./data/networks/Local\_Authority\_Districts\_(December\_2022)\_Boundaries\_UK\_BFC/LAD\_DEC\_2022\_UK\_BFC.shp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Reading layer `LAD_DEC_2022_UK_BFC' from data source 
  `/Users/franciscorowe/Dropbox/Francisco/uol/teaching/envs418/202324/r4ps/data/networks/Local_Authority_Districts_(December_2022)_Boundaries_UK_BFC/LAD_DEC_2022_UK_BFC.shp' 
  using driver `ESRI Shapefile'
Simple feature collection with 374 features and 10 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: -116.1928 ymin: 5336.966 xmax: 655653.8 ymax: 1220302
Projected CRS: OSGB36 / British National Grid
\end{verbatim}

\hypertarget{variables-1}{%
\subsection*{Variables}\label{variables-1}}
\addcontentsline{toc}{subsection}{Variables}

For each of the 4,835 LSOAs, the following characteristics are
available:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(st\_LSOA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "LSOA11CD"  "LSOA11NM"  "MSOA11CD"  "MSOA11NM"  "LAD11CD"   "LAD11NM"  
 [7] "RGN11CD"   "RGN11NM"   "USUALRES"  "HHOLDRES"  "COMESTRES" "POPDEN"   
[13] "HHOLDS"    "AVHHOLDSZ" "geometry" 
\end{verbatim}

where:

\begin{itemize}
\tightlist
\item
  \texttt{LSOA11CD}: Lower-Layer Super-Output Area code
\item
  \texttt{LSOA11NM}: Lower-Layer Super-Output Area code
\item
  \texttt{MSOA11CD}: Medium-Layer Super-Output Area code
\item
  \texttt{MSOA11NM}: Medium-Layer Super-Output Area code
\item
  \texttt{LAD11CD}: Local Authority District code
\item
  \texttt{LAD11NM}: Local Authority District name
\item
  \texttt{RGN11CD}: Region code
\item
  \texttt{RGN11NM}: Region name
\item
  \texttt{USUALRES}: Usual residents
\item
  \texttt{HHOLDRES}: Household residents
\item
  \texttt{COMESTRES}: Communal Establishment residents
\item
  \texttt{POPDEN}: Population density
\item
  \texttt{HHOLDS}: Number of households
\item
  \texttt{AVHHOLDSZ}: Average household size
\item
  \texttt{geometry}: Polygon of LSOA
\end{itemize}

For each of the 374 LADs, the following characteristics are available:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(LA\_UK)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "OBJECTID"   "LAD22CD"    "LAD22NM"    "BNG_E"      "BNG_N"     
 [6] "LONG"       "LAT"        "GlobalID"   "SHAPE_Leng" "SHAPE_Area"
[11] "geometry"  
\end{verbatim}

where:

\begin{itemize}
\tightlist
\item
  \texttt{OBJECTID}: object identifier
\item
  \texttt{LAD22CD}: Local Authority District code
\item
  \texttt{LAD22NM}: Local Authority District name
\item
  \texttt{BNG\_E}: Location Easting
\item
  \texttt{BNG\_N}: Location Northing
\item
  \texttt{LONG}: Location Longitude
\item
  \texttt{LAT}: Location Latitude
\item
  \texttt{GlobalID}: Global Identifier
\item
  \texttt{SHAPE\_Leng}: Boundary length
\item
  \texttt{SHAPE\_Area}: Area within boundary
\item
  \texttt{geometry}: Polygon of LAD
\end{itemize}

\hypertarget{projection}{%
\subsection*{Projection}\label{projection}}
\addcontentsline{toc}{subsection}{Projection}

The shapes of each LSOA are stored as polygons an expressed in the
OSGB36 projection:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{st\_crs}\NormalTok{(st\_LSOA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Coordinate Reference System:
  User input: OSGB36 / British National Grid 
  wkt:
PROJCRS["OSGB36 / British National Grid",
    BASEGEOGCRS["OSGB36",
        DATUM["Ordnance Survey of Great Britain 1936",
            ELLIPSOID["Airy 1830",6377563.396,299.3249646,
                LENGTHUNIT["metre",1]],
            ID["EPSG",6277]],
        PRIMEM["Greenwich",0,
            ANGLEUNIT["Degree",0.0174532925199433]]],
    CONVERSION["unnamed",
        METHOD["Transverse Mercator",
            ID["EPSG",9807]],
        PARAMETER["Latitude of natural origin",49,
            ANGLEUNIT["Degree",0.0174532925199433],
            ID["EPSG",8801]],
        PARAMETER["Longitude of natural origin",-2,
            ANGLEUNIT["Degree",0.0174532925199433],
            ID["EPSG",8802]],
        PARAMETER["Scale factor at natural origin",0.999601272,
            SCALEUNIT["unity",1],
            ID["EPSG",8805]],
        PARAMETER["False easting",400000,
            LENGTHUNIT["metre",1],
            ID["EPSG",8806]],
        PARAMETER["False northing",-100000,
            LENGTHUNIT["metre",1],
            ID["EPSG",8807]]],
    CS[Cartesian,2],
        AXIS["(E)",east,
            ORDER[1],
            LENGTHUNIT["metre",1,
                ID["EPSG",9001]]],
        AXIS["(N)",north,
            ORDER[2],
            LENGTHUNIT["metre",1,
                ID["EPSG",9001]]]]
\end{verbatim}

Similarly, the shapes of each LAD are stored as polygons an expressed in
the OSGB36 projection:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{st\_crs}\NormalTok{(LA\_UK)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Coordinate Reference System:
  User input: OSGB36 / British National Grid 
  wkt:
PROJCRS["OSGB36 / British National Grid",
    BASEGEOGCRS["OSGB36",
        DATUM["Ordnance Survey of Great Britain 1936",
            ELLIPSOID["Airy 1830",6377563.396,299.3249646,
                LENGTHUNIT["metre",1]]],
        PRIMEM["Greenwich",0,
            ANGLEUNIT["degree",0.0174532925199433]],
        ID["EPSG",4277]],
    CONVERSION["British National Grid",
        METHOD["Transverse Mercator",
            ID["EPSG",9807]],
        PARAMETER["Latitude of natural origin",49,
            ANGLEUNIT["degree",0.0174532925199433],
            ID["EPSG",8801]],
        PARAMETER["Longitude of natural origin",-2,
            ANGLEUNIT["degree",0.0174532925199433],
            ID["EPSG",8802]],
        PARAMETER["Scale factor at natural origin",0.9996012717,
            SCALEUNIT["unity",1],
            ID["EPSG",8805]],
        PARAMETER["False easting",400000,
            LENGTHUNIT["metre",1],
            ID["EPSG",8806]],
        PARAMETER["False northing",-100000,
            LENGTHUNIT["metre",1],
            ID["EPSG",8807]]],
    CS[Cartesian,2],
        AXIS["(E)",east,
            ORDER[1],
            LENGTHUNIT["metre",1]],
        AXIS["(N)",north,
            ORDER[2],
            LENGTHUNIT["metre",1]],
    USAGE[
        SCOPE["Engineering survey, topographic mapping."],
        AREA["United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore."],
        BBOX[49.75,-9,61.01,2.01]],
    ID["EPSG",27700]]
\end{verbatim}

\hypertarget{source-pre-processing-1}{%
\subsection*{Source \& Pre-processing}\label{source-pre-processing-1}}
\addcontentsline{toc}{subsection}{Source \& Pre-processing}

The boundaries for the LSOAs within London can be found directly from
the
\href{https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london}{London
Datastore} website.

The boundaries for the LADs for the UK can be found on the ONS Open
Geography Portal
\href{https://geoportal.statistics.gov.uk/search?collection=Dataset\&sort=name\&tags=all(BDY_LAD\%2CDEC_2022)}{website}.
To filter for the London LADs, i.e.~the London boroughs, we run the
following line of code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LND\_boroughs }\OtherTok{\textless{}{-}}\NormalTok{ LA\_UK }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(}\FunctionTok{grepl}\NormalTok{(}\StringTok{\textquotesingle{}E09\textquotesingle{}}\NormalTok{, LAD22CD)) }
\end{Highlighting}
\end{Shaded}

\hypertarget{worldpop-population-count-data-for-ukraine}{%
\section{Worldpop population count data for
Ukraine}\label{worldpop-population-count-data-for-ukraine}}

\hypertarget{census-population-count-data-for-uk}{%
\section{Census population count data for
UK}\label{census-population-count-data-for-uk}}

\hypertarget{ukraines-administrative-boundaries}{%
\section{Ukraine's administrative
boundaries}\label{ukraines-administrative-boundaries}}

\hypertarget{internal-migration-flows-between-us-metropolitan-areas-and-between-london-boroughs}{%
\section{Internal migration flows between US metropolitan areas and
between London
boroughs}\label{internal-migration-flows-between-us-metropolitan-areas-and-between-london-boroughs}}

\hypertarget{availability-2}{%
\subsection*{Availability}\label{availability-2}}
\addcontentsline{toc}{subsection}{Availability}

The dataset for the migration flows between US metropolitan areas can be
found as a csv file under:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_metro }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"./data/networks/metro\_to\_metro\_2015\_2019\_US\_migration.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The dataset for the migration flows between London boroughs can be found
as a csv file under:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_borough }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"./data/networks/LA\_to\_LA\_2019\_London\_clean.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{variables-2}{%
\subsection*{Variables}\label{variables-2}}
\addcontentsline{toc}{subsection}{Variables}

For each of the 52,930 movements recorded on the dataset for the
migration flows between US metropolitan areas, the following fields are
available:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(df\_metro)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "MSA_Current_Code"                                
 [2] "MSA_Current_Name"                                
 [3] "MSA_Current_State"                               
 [4] "MSA_Current_Population_1_Year_and_Over_Estimate" 
 [5] "MSA_Current_Population_1_Year_and_Over_MOE"      
 [6] "MSA_Previous_Code"                               
 [7] "MSA_Previous_Name"                               
 [8] "MSA_Previous_State"                              
 [9] "MSA_Previous_Population_1_Year_and_Over_Estimate"
[10] "MSA_Previous_Population_1_Year_and_Over_MOE"     
[11] "Movers_Metro_to_Metro_Flow_Estimate"             
[12] "Movers_Metro_to_Metro_Flow_MOE"                  
\end{verbatim}

All the fields that start with \texttt{MSA\_Current\_} or
\texttt{MSA\_Previous\_} refer to the characteristics of the origin and
destination metropolican areas. The relevant fields for the analysis in
this book are:

\begin{itemize}
\tightlist
\item
  \texttt{Movers\_Metro\_to\_Metro\_Flow\_Estimate}: Estimate of number
  of people moving between origin and destination
\item
  \texttt{Movers\_Metro\_to\_Metro\_Flow\_MOE}: Margin of error for the
  above estimate
\end{itemize}

More details on the methodology to obtain the estimates and the margin
of error for each population movement can be found on the
\href{https://www.census.gov/programs-surveys/acs/methodology.html}{US
Census Bureau website}.

For each of the 1,053 movements recorded on the dataset for the
migration flows between London boroughs, the following fields are
available:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(df\_borough)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "OutLA" "InLA"  "Moves"
\end{verbatim}

where:

\begin{itemize}
\tightlist
\item
  \texttt{OutLA} is the code corresponding to the origin borough
\item
  \texttt{InLA} is the code corresponding to the destination borough
\item
  \texttt{Moves} is the number of internal migration moves within each
  flow. Note that the numbers are not integers. This is because of the
  various scaling processes used to produce the dataset, which are
  described in more detail in the latest methodology document, which can
  be found
  \href{https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/migrationwithintheuk/datasets/internalmigrationbyoriginanddestinationlocalauthoritiessexandsingleyearofagedetailedestimatesdataset}{here}.
\end{itemize}

\hypertarget{source-pre-processing-2}{%
\subsection*{Source \& pre-processing}\label{source-pre-processing-2}}
\addcontentsline{toc}{subsection}{Source \& pre-processing}

The dataset for the migration flows between US metropolitan areas can be
downloaded from the
\href{https://www.census.gov/data/tables/2019/demo/geographic-mobility/metro-to-metro-migration.html}{US
Census Bureau website}. The data was cleaned on Microsoft Excel.

The dataset for the migration flows between London boroughs can be
downloaded from the
\href{https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/migrationwithintheuk/datasets/internalmigrationbyoriginanddestinationlocalauthoritiessexandsingleyearofagedetailedestimatesdataset}{ONS
website}. The data was cleaned on Microsoft Excel.

\hypertarget{twitter-data-on-public-opinion-originated-in-the-us-and-in-the-uk}{%
\section{Twitter data on public opinion originated in the US and in the
UK}\label{twitter-data-on-public-opinion-originated-in-the-us-and-in-the-uk}}

\hypertarget{reddit-data}{%
\section{Reddit data}\label{reddit-data}}

\hypertarget{google-mobility-data-for-italy-and-the-uk}{%
\section{Google mobility data for Italy and the
UK}\label{google-mobility-data-for-italy-and-the-uk}}

\hypertarget{covid-19-cases-data-for-london-and-rome}{%
\section{COVID-19 cases data for London and
Rome}\label{covid-19-cases-data-for-london-and-rome}}

\hypertarget{census-msoa-data-for-england-and-wales}{%
\section{Census MSOA data for England and
Wales}\label{census-msoa-data-for-england-and-wales}}

\hypertarget{availability-3}{%
\subsection*{Availability}\label{availability-3}}
\addcontentsline{toc}{subsection}{Availability}

The dataset for the demographic census data of each MSOA in England and
Wales can be loaded as a csv file from:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_MSOA }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"./data/machine{-}learning/census2021{-}msoa{-}income.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A very similar dataset for the demographic census data of each MSOA in
England and Wales which also contains data on the median house price can
be loaded as a csv file from:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_housing }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"./data/machine{-}learning/census2021{-}msoa{-}houseprice.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{variables-3}{%
\subsection*{Variables}\label{variables-3}}
\addcontentsline{toc}{subsection}{Variables}

For each of the 7,080 MSOAs recorded in England and Wales, the following
fields are available:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(df\_MSOA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "X"                "date"             "geography"        "geography.code"  
 [5] "inHH"             "inCE"             "SING"             "MARRIED"         
 [9] "SEP"              "DIV"              "WIDOW"            "UK"              
[13] "EU"               "AFR"              "AS"               "AM"              
[17] "OC"               "BO"               "DENSITY"          "Y14orUNDER"      
[21] "Y15to19"          "Y20to24"          "Y25to29"          "Y30to34"         
[25] "Y35to49"          "Y40to44"          "Y45to49"          "Y50to54"         
[29] "Y55to59"          "Y60to64"          "Y65orOVER"        "F"               
[33] "M"                "HH1"              "HH2"              "HH3"             
[37] "HH4"              "HH5"              "HH6"              "ADD1YagoSAME"    
[41] "ADD1YagoSTUDENT"  "ADD1YagoUK"       "ADD1YagoNONUK"    "NHH"             
[45] "OWN"              "MORTGAGE"         "SHAREDOWN"        "RENTfromCOUNCIL" 
[49] "RENTotherSOCIAL"  "RENTprivate"      "RENTprivateOTHER" "RENTfree"        
[53] "INCOME"          
\end{verbatim}

For a description of the variables in the columns of df\_MSOA, we can
load a dictionary for these variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_dictionary }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"./data/machine{-}learning/Dictionary.csv"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(df\_dictionary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                                      Dictionary       X
1                                                       
2                                           Name     Key
3                 Lives in household (% persons)    inHH
4    Lives in communal establishment (% persons)    inCE
5 Never married or civil partnership (% persons)    SING
6    Married or in civil partnership (% persons) MARRIED
\end{verbatim}

\hypertarget{source-pre-processing-3}{%
\subsection*{Source \& pre-processing}\label{source-pre-processing-3}}
\addcontentsline{toc}{subsection}{Source \& pre-processing}

Data on the the census characteristics for different MSOAs can be
downloaded from the
\href{https://www.nomisweb.co.uk/census/2021/bulk}{Nomis website}. Data
on the average net household income can be obtained from the
\href{https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/earningsandworkinghours/datasets/smallareaincomeestimatesformiddlelayersuperoutputareasenglandandwales}{ONS
website}.

Data on the median houseprice for different MSOAs can be downloaded from
the
\href{https://www.ons.gov.uk/peoplepopulationandcommunity/housing/datasets/hpssadataset2medianhousepricebymsoaquarterlyrollingyear}{ONS
website}.

All the data has been pre-processed on Microsoft Excel.

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-abbott2000sequence}{}}%
Abbott, Andrew, and Angela Tsay. 2000. {``Sequence Analysis and Optimal
Matching Methods in Sociology: Review and Prospect.''}
\emph{Sociological Methods \& Research} 29 (1): 3--33.

\leavevmode\vadjust pre{\hypertarget{ref-arribas-bel2021}{}}%
Arribas-Bel, Dani, Mark Green, Francisco Rowe, and Alex Singleton. 2021.
{``Open Data Products-A Framework for Creating Valuable Analysis Ready
Data.''} \emph{Journal of Geographical Systems} 23 (4): 497--514.
\url{https://doi.org/10.1007/s10109-021-00363-5}.

\leavevmode\vadjust pre{\hypertarget{ref-backman2020}{}}%
Backman, Mikaela, Esteban Lopez, and Francisco Rowe. 2020. {``The
Occupational Trajectories and Outcomes of Forced Migrants in Sweden.
Entrepreneurship, Employment or Persistent Inactivity?''} \emph{Small
Business Economics} 56 (3): 963--83.
\url{https://doi.org/10.1007/s11187-019-00312-z}.

\leavevmode\vadjust pre{\hypertarget{ref-bail2018}{}}%
Bail, Christopher A., Lisa P. Argyle, Taylor W. Brown, John P. Bumpus,
Haohan Chen, M. B. Fallin Hunzaker, Jaemin Lee, Marcus Mann, Friedolin
Merhout, and Alexander Volfovsky. 2018. {``Exposure to Opposing Views on
Social Media Can Increase Political Polarization.''} \emph{Proceedings
of the National Academy of Sciences} 115 (37): 9216--21.
\url{https://doi.org/10.1073/pnas.1804840115}.

\leavevmode\vadjust pre{\hypertarget{ref-bar2018did}{}}%
Bar, Michael, Moshe Hazan, Oksana Leukhina, David Weiss, and Hosny
Zoabi. 2018. {``Why Did Rich Families Increase Their Fertility?
Inequality and Marketization of Child Care.''} \emph{Journal of Economic
Growth} 23: 427--63.

\leavevmode\vadjust pre{\hypertarget{ref-ChristopherBishop06}{}}%
Bishop, Christopher M. 2006. \emph{Pattern Recognition and Machine
Learning}. Information Science and Statistics. New York: Springer.

\leavevmode\vadjust pre{\hypertarget{ref-blei2003latent}{}}%
Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. {``Latent
Dirichlet Allocation.''} \emph{Journal of Machine Learning Research} 3
(Jan): 993--1022.

\leavevmode\vadjust pre{\hypertarget{ref-boemkhe2019}{}}%
Boehmke, Brad. 2019. {``Hands-on Machine Learning with r. Chapter 9:
Decision Trees.''} \url{https://bradleyboehmke.github.io/HOML/DT.html}.

\leavevmode\vadjust pre{\hypertarget{ref-breiman1984}{}}%
Breiman, L. 1984. \emph{Classification and {Regression} {Trees} (1st
Ed.)}. Routledge.

\leavevmode\vadjust pre{\hypertarget{ref-brodeur2021covid}{}}%
Brodeur, Abel, Andrew E Clark, Sarah Fleche, and Nattavudh Powdthavee.
2021. {``COVID-19, Lockdowns and Well-Being: Evidence from Google
Trends.''} \emph{Journal of Public Economics} 193: 104346.

\leavevmode\vadjust pre{\hypertarget{ref-Cabrera-Arnau22}{}}%
Cabrera-Arnau, Carmen, Chen Zhong, Michael Batty, Ricardo Silva, and
Soong Moon Kang. 2022. {``Inferring Urban Polycentricity from the
Variability in Human Mobility Patterns.''} arXiv.
\url{https://doi.org/10.48550/ARXIV.2212.03973}.

\leavevmode\vadjust pre{\hypertarget{ref-cadwalladr2018revealed}{}}%
Cadwalladr, Carole, and Emma Graham-Harrison. 2018. {``Revealed: 50
Million Facebook Profiles Harvested for Cambridge Analytica in Major
Data Breach.''} \emph{The Guardian} 17 (1): 22.

\leavevmode\vadjust pre{\hypertarget{ref-cesare2018}{}}%
Cesare, Nina, Hedwig Lee, Tyler McCormick, Emma Spiro, and Emilio
Zagheni. 2018. {``Promises and Pitfalls of Using Digital Traces for
Demographic Research.''} \emph{Demography} 55 (5): 1979--99.
\url{https://doi.org/10.1007/s13524-018-0715-2}.

\leavevmode\vadjust pre{\hypertarget{ref-cheong2007}{}}%
Cheong, Pauline Hope, Rosalind Edwards, Harry Goulbourne, and John
Solomos. 2007. {``Immigration, Social Cohesion and Social Capital: A
Critical Review.''} \emph{Critical Social Policy} 27 (1): 24--49.
\url{https://doi.org/10.1177/0261018307072206}.

\leavevmode\vadjust pre{\hypertarget{ref-cinelli2020covid}{}}%
Cinelli, Matteo, Walter Quattrociocchi, Alessandro Galeazzi, Carlo
Michele Valensise, Emanuele Brugnoli, Ana Lucia Schmidt, Paola Zola,
Fabiana Zollo, and Antonio Scala. 2020. {``The COVID-19 Social Media
Infodemic.''} \emph{Scientific Reports} 10 (1): 1--10.

\leavevmode\vadjust pre{\hypertarget{ref-coates2020}{}}%
Coates, Melanie. 2020. {``Covid-19 and the Rise of Racism.''}
\emph{BMJ}, April, m1384. \url{https://doi.org/10.1136/bmj.m1384}.

\leavevmode\vadjust pre{\hypertarget{ref-cowper2020}{}}%
Cowper, Andy. 2020. {``Covid-19: Are We Getting the Communications
Right?''} \emph{BMJ}, March, m919.
\url{https://doi.org/10.1136/bmj.m919}.

\leavevmode\vadjust pre{\hypertarget{ref-dolega2021}{}}%
Dolega, Les, Francisco Rowe, and Emma Branagan. 2021. {``Going Digital?
The Impact of Social Media Marketing on Retail Website Traffic, Orders
and Sales.''} \emph{Journal of Retailing and Consumer Services} 60
(May): 102501. \url{https://doi.org/10.1016/j.jretconser.2021.102501}.

\leavevmode\vadjust pre{\hypertarget{ref-elbagir2020}{}}%
Elbagir, Shihab, and Jing Yang. 2020. {``Sentiment Analysis on Twitter
with Python{'}s Natural Language Toolkit and VADER Sentiment
Analyzer.''} \emph{IAENG Transactions on Engineering Sciences}, January.
\url{https://doi.org/10.1142/9789811215094_0005}.

\leavevmode\vadjust pre{\hypertarget{ref-EuropeanCommision2019}{}}%
European Commision. 2019. {``{10 Trends Shaping Migration}.''}
\url{https://op.europa.eu/en/publication-detail/-/publication/aa25fb8f-10cc-11ea-8c1f-01aa75ed71a1}.

\leavevmode\vadjust pre{\hypertarget{ref-fielding1992}{}}%
Fielding, A. J. 1992. {``Migration and Social Mobility: South East
England as an Escalator Region.''} \emph{Regional Studies} 26 (1):
1--15. \url{https://doi.org/10.1080/00343409212331346741}.

\leavevmode\vadjust pre{\hypertarget{ref-franklin2022}{}}%
Franklin, Rachel. 2022. {``Quantitative Methods II: Big Theory.''}
\emph{Progress in Human Geography} 47 (1): 178--86.
\url{https://doi.org/10.1177/03091325221137334}.

\leavevmode\vadjust pre{\hypertarget{ref-gabadinho2011}{}}%
Gabadinho, Alexis, Gilbert Ritschard, Nicolas S. Müller, and Matthias
Studer. 2011. {``Analyzing and Visualizing State Sequences
in{\emph{R}}with{\textbf{TraMineR}}.''} \emph{Journal of Statistical
Software} 40 (4). \url{https://doi.org/10.18637/jss.v040.i04}.

\leavevmode\vadjust pre{\hypertarget{ref-gabadinho2009mining}{}}%
Gabadinho, Alexis, Gilbert Ritschard, Matthias Studer, and Nicolas S
Müller. 2009. {``Mining Sequence Data in r with the TraMineR Package: A
User's Guide.''} \emph{Geneva: Department of Econometrics and Laboratory
of Demography, University of Geneva}.

\leavevmode\vadjust pre{\hypertarget{ref-ghani2019}{}}%
Ghani, Norjihan Abdul, Suraya Hamid, Ibrahim Abaker Targio Hashem, and
Ejaz Ahmed. 2019. {``Social Media Big Data Analytics: A Survey.''}
\emph{Computers in Human Behavior} 101 (December): 417--28.
\url{https://doi.org/10.1016/j.chb.2018.08.039}.

\leavevmode\vadjust pre{\hypertarget{ref-gonzuxe1lez-leonardo2023}{}}%
González-Leonardo, Miguel, Niall Newsham, and Francisco Rowe. 2023.
{``Understanding Population Decline Trajectories in Spain Using Sequence
Analysis.''} \emph{Geographical Analysis}, January.
\url{https://doi.org/10.1111/gean.12357}.

\leavevmode\vadjust pre{\hypertarget{ref-goodman2020using}{}}%
Goodman-Bacon, Andrew, and Jan Marcus. 2020. {``Using
Difference-in-Differences to Identify Causal Effects of COVID-19
Policies.''}

\leavevmode\vadjust pre{\hypertarget{ref-green2021}{}}%
Green, Mark, Frances Darlington Pollock, and Francisco Rowe. 2021.
{``New Forms of Data and New Forms of Opportunities to Monitor and
Tackle a Pandemic.''} In, 423--29. Springer International Publishing.
\url{https://doi.org/10.1007/978-3-030-70179-6_56}.

\leavevmode\vadjust pre{\hypertarget{ref-hilbert2011}{}}%
Hilbert, Martin, and Priscila López. 2011. {``The World{'}s
Technological Capacity to Store, Communicate, and Compute
Information.''} \emph{Science} 332 (6025): 60--65.
\url{https://doi.org/10.1126/science.1200970}.

\leavevmode\vadjust pre{\hypertarget{ref-HomeAffairsCommittee2020}{}}%
Home Affairs Committee. 2020. {``{Oral evidence: Home Office
preparedness for Covid-19 (Coronavirus), HC 232}.''} London: House of
Commons.
\url{https://committees.parliament.uk/oralevidence/359/default/}.

\leavevmode\vadjust pre{\hypertarget{ref-hutto2014}{}}%
Hutto, C., and Eric Gilbert. 2014. {``VADER: A Parsimonious Rule-Based
Model for Sentiment Analysis of Social Media Text.''} \emph{Proceedings
of the International AAAI Conference on Web and Social Media} 8 (1):
216--25. \url{https://doi.org/10.1609/icwsm.v8i1.14550}.

\leavevmode\vadjust pre{\hypertarget{ref-europeancommission.jointresearchcentre.2022}{}}%
Joint Research Centre. 2022. \emph{Data innovation in demography,
migration and human mobility.} LU: European Commission. Publications
Office. \url{https://doi.org/10.2760/027157}.

\leavevmode\vadjust pre{\hypertarget{ref-kashyap2022}{}}%
Kashyap, Ridhi, R. Gordon Rinderknecht, Aliakbar Akbaritabar, Diego
Alburez-Gutierrez, Sofia Gil-Clavel, André Grow, Jisu Kim, et al. 2022.
{``Digital and Computational Demography.''}
\url{http://dx.doi.org/10.31235/osf.io/7bvpt}.

\leavevmode\vadjust pre{\hypertarget{ref-kaufman2009finding}{}}%
Kaufman, Leonard, and Peter J Rousseeuw. 2009. \emph{Finding Groups in
Data: An Introduction to Cluster Analysis}. John Wiley \& Sons.

\leavevmode\vadjust pre{\hypertarget{ref-kitchin2014}{}}%
Kitchin, Rob. 2014. {``Big Data, New Epistemologies and Paradigm
Shifts.''} \emph{Big Data \& Society} 1 (1): 205395171452848.
\url{https://doi.org/10.1177/2053951714528481}.

\leavevmode\vadjust pre{\hypertarget{ref-lazer2009}{}}%
Lazer, David, Alex Pentland, Lada Adamic, Sinan Aral, Albert-László
Barabási, Devon Brewer, Nicholas Christakis, et al. 2009.
{``Computational Social Science.''} \emph{Science} 323 (5915): 721--23.
\url{https://doi.org/10.1126/science.1167742}.

\leavevmode\vadjust pre{\hypertarget{ref-lazer2020}{}}%
Lazer, David, Alex Pentland, Duncan J. Watts, Sinan Aral, Susan Athey,
Noshir Contractor, Deen Freelon, et al. 2020. {``Computational Social
Science: Obstacles and Opportunities.''} \emph{Science} 369 (6507):
1060--62. \url{https://doi.org/10.1126/science.aaz8170}.

\leavevmode\vadjust pre{\hypertarget{ref-liang2015}{}}%
Liang, Hai, and King-wa Fu. 2015. {``Testing Propositions Derived from
Twitter Studies: Generalization and Replication in Computational Social
Science.''} Edited by Zi-Ke Zhang. \emph{PLOS ONE} 10 (8): e0134270.
\url{https://doi.org/10.1371/journal.pone.0134270}.

\leavevmode\vadjust pre{\hypertarget{ref-marshall2013defining}{}}%
Marshall, Emily A. 2013. {``Defining Population Problems: Using Topic
Models for Cross-National Comparison of Disciplinary Development.''}
\emph{Poetics} 41 (6): 701--24.

\leavevmode\vadjust pre{\hypertarget{ref-Newman18}{}}%
Newman, Mark. 2018. \emph{Networks / Mark Newman.} Second edition.
Oxford: Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-newsham2022}{}}%
Newsham, Niall, and Francisco Rowe. 2022a. {``Understanding the
Trajectories of Population Decline Across Rural and Urban Europe: A
Sequence Analysis.''} \url{https://doi.org/10.48550/ARXIV.2203.09798}.

\leavevmode\vadjust pre{\hypertarget{ref-newsham2022a}{}}%
---------. 2022b. {``Understanding Trajectories of Population Decline
Across Rural and Urban Europe: A Sequence Analysis.''} \emph{Population,
Space and Place}, December. \url{https://doi.org/10.1002/psp.2630}.

\leavevmode\vadjust pre{\hypertarget{ref-Ognyanova16}{}}%
Ognyanova, K. 2016. {``Network Analysis with r and Igraph: NetSci x
Tutorial.''}
\href{https://www.kateto.net/networks-r-igraph}{www.kateto.net/networks-r-igraph}.

\leavevmode\vadjust pre{\hypertarget{ref-patias2021}{}}%
Patias, Nikos, Francisco Rowe, and Dani Arribas-Bel. 2021.
{``Trajectories of Neighbourhood Inequality in Britain: Unpacking
Inter{-}Regional Socioeconomic Imbalances, 1971{-}2011.''} \emph{The
Geographical Journal} 188 (2): 150--65.
\url{https://doi.org/10.1111/geoj.12420}.

\leavevmode\vadjust pre{\hypertarget{ref-patias2021a}{}}%
Patias, Nikos, Francisco Rowe, Stefano Cavazzi, and Dani Arribas-Bel.
2021. {``Sustainable Urban Development Indicators in Great Britain from
2001 to 2016.''} \emph{Landscape and Urban Planning} 214 (October):
104148. \url{https://doi.org/10.1016/j.landurbplan.2021.104148}.

\leavevmode\vadjust pre{\hypertarget{ref-petti2020}{}}%
Petti, Samantha, and Abraham Flaxman. 2020. {``Differential Privacy in
the 2020 US Census: What Will It Do? Quantifying the Accuracy/Privacy
Tradeoff.''} \emph{Gates Open Research} 3 (April): 1722.
\url{https://doi.org/10.12688/gatesopenres.13089.2}.

\leavevmode\vadjust pre{\hypertarget{ref-Prieto22}{}}%
Prieto Curiel, Rafael, Carmen Cabrera-Arnau, and Steven Richard Bishop.
2022. {``Scaling Beyond Cities.''} \emph{Frontiers in Physics} 10.
\url{https://doi.org/10.3389/fphy.2022.858307}.

\leavevmode\vadjust pre{\hypertarget{ref-ribeiro2020}{}}%
Ribeiro, Filipe N., Fabrício Benevenuto, and Emilio Zagheni. 2020.
{``How Biased Is the Population of Facebook Users? Comparing the
Demographics of Facebook Users with Census Data to Generate Correction
Factors.''} \emph{12th ACM Conference on Web Science}, July.
\url{https://doi.org/10.1145/3394231.3397923}.

\leavevmode\vadjust pre{\hypertarget{ref-rosa2019}{}}%
Rosa, H., N. Pereira, R. Ribeiro, P. C. Ferreira, J. P. Carvalho, S.
Oliveira, L. Coheur, P. Paulino, A. M. Veiga Simão, and I. Trancoso.
2019. {``Automatic Cyberbullying Detection: A Systematic Review.''}
\emph{Computers in Human Behavior} 93 (April): 333--45.
\url{https://doi.org/10.1016/j.chb.2018.12.021}.

\leavevmode\vadjust pre{\hypertarget{ref-rowe2021}{}}%
Rowe, Francisco. 2021a. {``Using Twitter Data to Monitor Immigration
Sentiment.''} \url{http://dx.doi.org/10.31219/osf.io/sf7u4}.

\leavevmode\vadjust pre{\hypertarget{ref-rowe2021b}{}}%
---------. 2021b. {``Big Data and Human Geography.''}
\url{http://dx.doi.org/10.31235/osf.io/phz3e}.

\leavevmode\vadjust pre{\hypertarget{ref-rowe2022f}{}}%
---------. 2022a. {``Introduction to Geographic Data Science.''}
\emph{Open Science Framework}, August.
\url{https://doi.org/10.17605/OSF.IO/VHY2P}.

\leavevmode\vadjust pre{\hypertarget{ref-rowe2022c}{}}%
---------. 2022b. {``Using Digital Footprint Data to Monitor Human
Mobility and Support Rapid Humanitarian Responses.''} \emph{Regional
Studies, Regional Science} 9 (1): 665--68.
\url{https://doi.org/10.1080/21681376.2022.2135458}.

\leavevmode\vadjust pre{\hypertarget{ref-rowe2022e}{}}%
Rowe, Francisco, and Dani Arribas-Bel. 2022. {``Spatial Modelling for
Data Scientists.''} \emph{Open Science Framework}, August.
\url{https://doi.org/10.17605/OSF.IO/8F6XR}.

\leavevmode\vadjust pre{\hypertarget{ref-rowe2016}{}}%
Rowe, Francisco, Jonathan Corcoran, and Martin Bell. 2016. {``The
Returns to Migration and Human Capital Accumulation Pathways:
Non-Metropolitan Youth in the School-to-Work Transition.''} \emph{The
Annals of Regional Science} 59 (3): 819--45.
\url{https://doi.org/10.1007/s00168-016-0771-8}.

\leavevmode\vadjust pre{\hypertarget{ref-rowe2021a}{}}%
Rowe, Francisco, Michael Mahony, Eduardo Graells-Garrido, Marzia Rango,
and Niklas Sievers. 2021. {``Using Twitter to Track Immigration
Sentiment During Early Stages of the COVID-19 Pandemic.''} \emph{Data \&
Policy} 3. \url{https://doi.org/10.1017/dap.2021.38}.

\leavevmode\vadjust pre{\hypertarget{ref-Rowe2021b}{}}%
Rowe, Francisco, Michael Mahony, Niklas Sievers, Marzia Rango, and
Eduardo Graells-Garrido. 2021. {``{Sentiment towards Migration during
COVID-19. What Twitter Data Can Tell Us}.''} \emph{IOM Publications}.

\leavevmode\vadjust pre{\hypertarget{ref-rowe2022a}{}}%
Rowe, Francisco, Ruth Neville, and Miguel González-Leonardo. 2022.
{``Sensing Population Displacement from Ukraine Using Facebook Data:
Potential Impacts and Settlement Areas.''}
\url{http://dx.doi.org/10.31219/osf.io/7n6wm}.

\leavevmode\vadjust pre{\hypertarget{ref-salmela-aro2011}{}}%
Salmela-Aro, Katariina, Noona Kiuru, Jari-Erik Nurmi, and Mervi Eerola.
2011. {``Mapping Pathways to Adulthood Among Finnish University
Students: Sequences, Patterns, Variations in Family- and Work-Related
Roles.''} \emph{Advances in Life Course Research} 16 (1): 25--41.
\url{https://doi.org/10.1016/j.alcr.2011.01.003}.

\leavevmode\vadjust pre{\hypertarget{ref-schleicher2020impact}{}}%
Schleicher, Andreas. 2020. {``The Impact of COVID-19 on Education:
Insights from" Education at a Glance 2020".''} \emph{OECD Publishing}.

\leavevmode\vadjust pre{\hypertarget{ref-schlosser2021}{}}%
Schlosser, Frank, Vedran Sekara, Dirk Brockmann, and Manuel
Garcia-Herranz. 2021. {``Biases in Human Mobility Data Impact Epidemic
Modeling.''} \url{https://doi.org/10.48550/ARXIV.2112.12521}.

\leavevmode\vadjust pre{\hypertarget{ref-silgerobison2022}{}}%
Sielge, Jullia, and David Robinson. 2022. \emph{Welcome to Text Mining
with r}. {O'Reilly}. \url{https://www.tidytextmining.com}.

\leavevmode\vadjust pre{\hypertarget{ref-singleton2019}{}}%
Singleton, Alex, and Daniel Arribas-Bel. 2019. {``Geographic Data
Science.''} \emph{Geographical Analysis} 53 (1): 61--75.
\url{https://doi.org/10.1111/gean.12194}.

\leavevmode\vadjust pre{\hypertarget{ref-stopthe2020}{}}%
{``Stop the Coronavirus Stigma Now.''} 2020. \emph{Nature} 580 (7802):
165--65. \url{https://doi.org/10.1038/d41586-020-01009-0}.

\leavevmode\vadjust pre{\hypertarget{ref-tatem2017}{}}%
Tatem, Andrew J. 2017. {``WorldPop, Open Data for Spatial Demography.''}
\emph{Scientific Data} 4 (1).
\url{https://doi.org/10.1038/sdata.2017.4}.

\leavevmode\vadjust pre{\hypertarget{ref-turok2007}{}}%
Turok, Ivan, and Vlad Mykhnenko. 2007. {``The Trajectories of European
Cities, 1960{\textendash}2005.''} \emph{Cities} 24 (3): 165--82.
\url{https://doi.org/10.1016/j.cities.2007.01.007}.

\leavevmode\vadjust pre{\hypertarget{ref-ugolini2020effects}{}}%
Ugolini, Francesca, Luciano Massetti, Pedro Calaza-Martı́nez, Paloma
Cariñanos, Cynnamon Dobbs, Silvija Krajter Ostoić, Ana Marija Marin, et
al. 2020. {``Effects of the COVID-19 Pandemic on the Use and Perceptions
of Urban Green Space: An International Exploratory Study.''} \emph{Urban
Forestry \& Urban Greening} 56: 126888.

\leavevmode\vadjust pre{\hypertarget{ref-zagheni2015}{}}%
Zagheni, Emilio, and Ingmar Weber. 2015. {``Demographic Research with
Non-Representative Internet Data.''} Edited by Nikolaos Askitas and
Professor Professor Klaus F. Zimmermann. \emph{International Journal of
Manpower} 36 (1): 13--25.
\url{https://doi.org/10.1108/ijm-12-2014-0261}.

\leavevmode\vadjust pre{\hypertarget{ref-zhou2021varying}{}}%
Zhou, Muzhi, and Man-Yee Kan. 2021. {``The Varying Impacts of COVID-19
and Its Related Measures in the UK: A Year in Review.''} \emph{PLoS One}
16 (9): e0257286.

\end{CSLReferences}



\end{document}
